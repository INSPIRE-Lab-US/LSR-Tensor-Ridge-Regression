{"cells":[{"cell_type":"markdown","metadata":{"id":"KzKHJMnbfVlm"},"source":["# Tensor Pipeline"]},{"cell_type":"markdown","metadata":{"id":"uGRufj_xfVlo"},"source":["## Setting Path"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import sys\n","sys.path.append('../Code Files')\n","sys.path.append('../Data')"]},{"cell_type":"markdown","metadata":{"id":"BMZmvyrBfVlq"},"source":["## Import Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import datetime\n","import numpy as np\n","import dill\n","from KFoldCV_PyTorch import KFoldCV\n","from train_test_PyTorch import train_test\n","from DataGenerationB_PyTorch import *\n","from sklearn.preprocessing import StandardScaler"]},{"cell_type":"markdown","metadata":{"id":"RraGuUhGfVls"},"source":["## Import Data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3277,"status":"ok","timestamp":1712171307195,"user":{"displayName":"Lakshitha Ramanayake Mudiyanselage","userId":"09058794058790427589"},"user_tz":240},"id":"YUFTld9DfVls","outputId":"c7820ce2-d7a9-4ca1-86d4-8ea8b3fad406"},"outputs":[],"source":["import pickle\n","pkl_file = \"../Data/Bounded_Var_Time_2024-03-31 20_19_12, intercept_5,n_train_5000, n_test_1000, tensor_dimensions_[32 32], tensor_mode_ranks_[4 4], separation_rank_2.pkl\"\n","file= open(pkl_file, 'rb')\n","data = pickle.load(file)\n","file.close()\n","\n","X_train_Full = data[0]\n","print(data[0].shape)\n","\n","Y_train_Full = data[1]\n","print(data[1].shape)\n","\n","X_test_Full = data[2]\n","print(data[2].shape)\n","\n","Y_test_Full = data[3]\n","print(data[3].shape)\n","\n","B_tensored = data[4]\n","print(data[4].shape)"]},{"cell_type":"markdown","metadata":{"id":"GbG33zwAfVls"},"source":["## Subset Data"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":131,"status":"ok","timestamp":1712171311486,"user":{"displayName":"Lakshitha Ramanayake Mudiyanselage","userId":"09058794058790427589"},"user_tz":240},"id":"wuhNbclbEkQT"},"outputs":[],"source":["n_train_all = [650]\n","n_test = 100\n","\n","tensor_dimensions = np.array([32, 32])\n","tensor_mode_ranks = np.array([4, 4])\n","separation_rank = 2\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y1zfvEkAfVlt","outputId":"ca512bba-2dc2-4330-a2e6-f1582720dfe9"},"outputs":[],"source":["for i,n_train in enumerate(n_train_all):\n","  print('Number of Samples:',n_train,'---------------------------------------------------------------------------------------------------------------------------')\n","\n","  #Subset X_train and Y_train\n","  X_train = X_train_Full[0:(n_train),:,:]\n","  Y_train = Y_train_Full[0:(n_train)]\n","\n","  #Subset X_test and Y_test\n","  X_test = X_test_Full[0:(n_test),:,:]\n","  Y_test = Y_test_Full[0:(n_test)]\n","\n","  #Preprocessing\n","\n","  # Reshape the 3D array to a 2D array where each row represents a sample\n","  # The shape of the original 3D array is (n_samples, n_features_per_sample, n_dimensions)\n","  # We reshape it to (n_samples, n_features_per_sample * n_dimensions)\n","\n","\n","  X_train_2D = X_train.reshape(n_train, -1)\n","  X_test_2D = X_test.reshape(n_test,-1)\n","\n","\n","  # Initialize StandardScaler\n","  scaler = StandardScaler(with_std = False) #standard scalar only\n","\n","  # Fit scaler on train data and transform train data\n","  X_train_scaled = scaler.fit_transform(X_train_2D)\n","  # Transform test data using the scaler fitted on train data\n","  X_test_scaled = scaler.transform(X_test_2D)\n","\n","  # Reshape the scaled data back to 3D\n","  X_train = X_train_scaled.reshape(n_train, tensor_dimensions[0],tensor_dimensions[1])\n","  X_test  = X_test_scaled.reshape(n_test, tensor_dimensions[0],tensor_dimensions[1])\n","\n","  #average response value\n","  Y_train_mean = np.mean(Y_train)\n","  # Mean centering y_train and y_test\n","  Y_train = Y_train - Y_train_mean\n","\n","\n","  print(\"Sample mean for each feature (across samples):\",scaler.mean_)\n","  print(\"Sample variance for each feature (across samples):\",scaler.var_)\n","  print('Response Average:',Y_train_mean)\n","\n","  #For now, define finite alpha set that we are searching over\n","  alphas = [0,0.1,0.3, 0.5, 0.7, 0.9, 1, 1.5, 2, 2.5, 3, 5, 4,10,15,20,50,100]\n","\n","  #Define Number of Folds we want\n","  k_folds = 10\n","  hypers = {'max_iter': 200, 'threshold': 1e-4, 'ranks': tuple(tensor_mode_ranks), 'separation_rank': separation_rank}\n","  lambda1, validation_normalized_estimation_error, validation_nmse_losses, validation_correlations, validation_R2_scores, objective_function_information = KFoldCV(X_train, Y_train, B_tensored, alphas, k_folds, hypers, intercept= False)\n","\n","  hypers = {'max_iter': 200, 'threshold': 1e-4, 'ranks': tuple(tensor_mode_ranks), 'separation_rank': separation_rank}\n","  normalized_estimation_error, test_nmse_loss, test_R2_loss, test_correlation, objective_function_values = train_test(X_train, Y_train, X_test, Y_test, B_tensored, lambda1, hypers, Y_train_mean,intercept= False)\n","\n","  #Get current time and store in variable\n","  formatted_time = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n","  max_iter = hypers['max_iter']\n","  pkl_file = f\"/Users/lakrama/Neuro Project Codes/LSR-Tensor-Ridge-Regression/Closed_Form_Solver/Experimental Results/ExecutionTime_intercept_5_{formatted_time}, n_train_{n_train},n_test_{n_test}, tensor_dimensions:{tensor_dimensions}, tensor_mode_= ranks:{tensor_mode_ranks}, separation_rank:{separation_rank}, max_iter={max_iter}.pkl\"\n","\n","  print(\"Error Record on Training _ After K-Fold CV\")\n","  print(\"Validation Normalized Estimation Error: \", validation_normalized_estimation_error)\n","  print(\"Validation NMSE Losses: \", validation_nmse_losses)\n","  print(\"Validation Correlations: \", validation_correlations)\n","  print(\"Validation R2 Scores: \", validation_R2_scores)\n","\n","  print(\"Error Report on Testing _ With best Lambda\")\n","  print(\"Alpha chosen for model: \", lambda1)\n","  print(\"Test Normalized Estimation Error: \", normalized_estimation_error)\n","  print(\"Test NMSE Loss: \", test_nmse_loss)\n","  print(\"Test R2 Loss: \", test_R2_loss)\n","  print(\"Test Correlation: \", test_correlation)\n","\n","  with open(pkl_file, \"wb\") as file:\n","    dill.dump((X_train_Full, Y_train_Full, X_test_Full, Y_test_Full, B_tensored, lambda1, validation_normalized_estimation_error, validation_nmse_losses, validation_correlations, validation_R2_scores, objective_function_information, normalized_estimation_error, test_nmse_loss, test_R2_loss, test_correlation, objective_function_values), file)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yzMXrkDVGKvU"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}
