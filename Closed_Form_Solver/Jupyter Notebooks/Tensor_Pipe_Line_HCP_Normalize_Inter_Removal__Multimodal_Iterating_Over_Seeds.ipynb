{"cells":[{"cell_type":"markdown","metadata":{"id":"KzKHJMnbfVlm"},"source":["# Tensor Pipeline\n","\n","This note book runs the  lsr structured tensor ridge regression model with the following parameters fixed.\n","\n","1. Ridge regression coefficient: \n","2. Max iterations\n","3. Separation Rank\n","4. Tucker Rank\n","\n"]},{"cell_type":"markdown","metadata":{"id":"uGRufj_xfVlo"},"source":["## Install Libraries"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12586,"status":"ok","timestamp":1711494542524,"user":{"displayName":"Lakshitha Ramanayake Mudiyanselage","userId":"09058794058790427589"},"user_tz":240},"id":"Z5jGDIH5fVlo","outputId":"c31a3910-211e-4265-d209-a5cda34ef592"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: dill in c:\\users\\raman\\anaconda3\\envs\\torchtensor\\lib\\site-packages (0.3.8)\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["%pip install dill"]},{"cell_type":"markdown","metadata":{"id":"7KNNEWuzfVlp"},"source":["## System Path"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"v1121WdGM3gG"},"outputs":[],"source":["import sys\n","import platform\n","\n","# Check the operating system\n","if platform.system() == \"Windows\":\n","    # Using double backslashes\n","    sys.path.append(r\"D:\\Tensor Based ML for Neuro Imaging\\INSPIRE_CAHBHIR\\Python Scripts\\LSR-Tensor-Ridge-Regression\\Closed_Form_Solver\\Code Files\")\n","elif platform.system() == \"Darwin\":  # macOS\n","    # Append path for macOS\n","    sys.path.append(\"/Users/lakrama/Neuro Project Codes/LSR-Tensor-Ridge-Regression/Closed_Form_Solver/Code Files\")\n","    sys.path.append(\"/Users/lakrama/Neuro Project Codes/Datasets/Data_Sets/HCP/Resting State FMRI\")"]},{"cell_type":"markdown","metadata":{"id":"BMZmvyrBfVlq"},"source":["## Import Libraries"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"tH57r_wdfVlq"},"outputs":[],"source":["#Import sklearn stuff\n","import datetime\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import re\n","import scipy\n","from sklearn.metrics import r2_score \n","from sklearn.preprocessing import StandardScaler\n","import seaborn as sns\n","import scipy\n","import time \n","\n","\n","#Used to load data from pkl file\n","import dill\n","\n","#Import External Files\n","from KFoldCV import KFoldCV\n","from train_test import train_test\n","from DataGenerationB import *"]},{"cell_type":"markdown","metadata":{},"source":["## Important Functions "]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["#conversion of the HCP data from Vector --> Matrix\n","\n","def merging_modalities(vector1,vector2,outputdim): \n","    \n","    '''\n","    This code take vector1 and vector2 of same length as the input and reconstructed the matrix with the vector1 element as the\n","    upper triangle and the vector 2 elements as the lower triangle.\n","    '''\n","    \n","    #checking whether we can generate the symmetric matrix \n","    vector1_length = vector1.shape[0]\n","    vector2_length = vector2.shape[0]\n","    desired_length = outputdim * (outputdim - 1) / 2\n","    \n","    if vector1_length != vector2_length:\n","        raise ValueError(\"Vector length mismatch.\")\n","    elif vector1_length != desired_length:\n","        raise ValueError(\"Vector length is insufficient to construct the symmetric matrix.\")\n","\n","    #matrix_1\n","    \n","    matrix_1 = np.zeros((outputdim, outputdim))\n","    \n","    #counter for the vector \n","    q = 0\n","    \n","    for i in range (outputdim):\n","        for j in range(i,outputdim):\n","            if i == j:\n","                matrix_1[i,j] = 1\n","            else:\n","                matrix_1[i,j] = vector1[q]\n","                q = q+1 \n","    \n","    \n","    #matrix 2\n","    \n","    matrix_2 = np.zeros((outputdim, outputdim))\n","    \n","    #counter for the vector \n","    p = 0\n","    \n","    for i in range (outputdim):\n","        for j in range(i,outputdim):\n","            if i == j:\n","                matrix_2[i,j] = 1\n","            else:\n","                matrix_2[i,j] = vector2[p]\n","                p = p+1 \n","                \n","    #upper and lower triangle \n","    upper_triangle = matrix_1\n","    lower_triangle = matrix_2.T\n","    \n","    #constructing the matrix\n","    matrix_eye = np.eye(outputdim)\n","    matrix =  upper_triangle + lower_triangle - matrix_eye\n","    \n","    return matrix\n","\n","def samplestomat(dataset1,dataset2,outputdim):\n","    \n","    '''\n","    This code is developed to convert the vectorized data matrix in to a 3D data tensor.\n","    \n","    dataset : nd:array - (samples*features)\n","    outputdim : scalar\n","\n","    '''\n","\n","    #number of samples\n","    n_samples = dataset1.shape[0]\n","    #3D matrix to hold the output\n","    out_dataset = np.zeros((n_samples,outputdim,outputdim))\n","\n","    for p in range(n_samples):\n","        \n","        vector_1 = dataset1[p]\n","        vector_2 = dataset2[p] \n","        sample = merging_modalities(vector_1,vector_2,outputdim)\n","        out_dataset[p] = sample\n","\n","    random_index = np.random.randint(0, n_samples)\n","    random_sample = out_dataset[random_index]\n","\n","    # Plot the heatmap\n","    plt.figure(figsize=(10, 8))\n","    sns.heatmap(random_sample, cmap='viridis', cbar=True)\n","    plt.title(f'Heatmap of Random Sample {random_index}')\n","    plt.show()\n","\n","    return out_dataset\n","\n","# normalizing using frobenious norm\n","\n","def normalize_by_frobenius_norm(samples):\n","    \"\"\"\n","    Normalizes each sample (2D matrix) in the array by its Frobenius norm.\n","\n","    Parameters:\n","    samples (numpy.ndarray): A 3D numpy array with dimensions [samples, rows, columns].\n","\n","    Returns:\n","    numpy.ndarray: A 3D numpy array with each sample normalized by its Frobenius norm.\n","    \"\"\"\n","    # Calculate the Frobenius norm for each sample\n","    frobenius_norms = np.linalg.norm(samples, axis=(1, 2))\n","    \n","    # Reshape the norms to broadcast correctly for division\n","    frobenius_norms = frobenius_norms[:, np.newaxis, np.newaxis]\n","    \n","    # Normalize each sample by its Frobenius norm\n","    normalized_samples = samples / frobenius_norms\n","    \n","    return normalized_samples\n","        \n"]},{"cell_type":"markdown","metadata":{"id":"RraGuUhGfVls"},"source":["## Import Data"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["0\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[1;32mIn[17], line 106\u001b[0m\n\u001b[0;32m    103\u001b[0m X_test_vec_dmri \u001b[38;5;241m=\u001b[39m dMRI_streamlog[test_subjects]\n\u001b[0;32m    105\u001b[0m \u001b[38;5;66;03m#merging the modalities\u001b[39;00m\n\u001b[1;32m--> 106\u001b[0m X_train \u001b[38;5;241m=\u001b[39m \u001b[43msamplestomat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_vec_fmri\u001b[49m\u001b[43m,\u001b[49m\u001b[43mX_train_vec_dmri\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m400\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    107\u001b[0m X_test  \u001b[38;5;241m=\u001b[39m samplestomat(X_test_vec_fmri,X_test_vec_dmri,\u001b[38;5;241m400\u001b[39m)\n\u001b[0;32m    108\u001b[0m Y_train \u001b[38;5;241m=\u001b[39m Y_train\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n","Cell \u001b[1;32mIn[10], line 80\u001b[0m, in \u001b[0;36msamplestomat\u001b[1;34m(dataset1, dataset2, outputdim)\u001b[0m\n\u001b[0;32m     78\u001b[0m     vector_1 \u001b[38;5;241m=\u001b[39m dataset1[p]\n\u001b[0;32m     79\u001b[0m     vector_2 \u001b[38;5;241m=\u001b[39m dataset2[p] \n\u001b[1;32m---> 80\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[43mmerging_modalities\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvector_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43mvector_2\u001b[49m\u001b[43m,\u001b[49m\u001b[43moutputdim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     81\u001b[0m     out_dataset[p] \u001b[38;5;241m=\u001b[39m sample\n\u001b[0;32m     83\u001b[0m random_index \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, n_samples)\n","Cell \u001b[1;32mIn[10], line 56\u001b[0m, in \u001b[0;36mmerging_modalities\u001b[1;34m(vector1, vector2, outputdim)\u001b[0m\n\u001b[0;32m     53\u001b[0m lower_triangle \u001b[38;5;241m=\u001b[39m matrix_2\u001b[38;5;241m.\u001b[39mT\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m#constructing the matrix\u001b[39;00m\n\u001b[1;32m---> 56\u001b[0m matrix_eye \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meye\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputdim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m matrix \u001b[38;5;241m=\u001b[39m  upper_triangle \u001b[38;5;241m+\u001b[39m lower_triangle \u001b[38;5;241m-\u001b[39m matrix_eye\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m matrix\n","File \u001b[1;32mc:\\Users\\raman\\anaconda3\\envs\\torchtensor\\Lib\\site-packages\\numpy\\lib\\twodim_base.py:211\u001b[0m, in \u001b[0;36meye\u001b[1;34m(N, M, k, dtype, order, like)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m M \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m     M \u001b[38;5;241m=\u001b[39m N\n\u001b[1;32m--> 211\u001b[0m m \u001b[38;5;241m=\u001b[39m \u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mM\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m M:\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m m\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["### FMRI - Resting State \n","\n","    \n","if platform.system() == \"Windows\": \n","    file_path = r\"D:\\Tensor Based ML for Neuro Imaging\\INSPIRE_CAHBHIR\\Python Scripts\\LSR-Tensor-Ridge-Regression\\Data_Sets\\HCP\\Resting State FMRI\\fmri_rs.npy\"\n","elif platform.system() == \"Darwin\":\n","    file_path = r\"/Users/lakrama/Neuro Project Codes/Datasets/Data_Sets/HCP/Resting State FMRI/fmri_rs.npy\"  # Adjust the path for macOS\n","\n","with open(file_path, \"rb\") as f:\n","    fmri_rs = np.load(f)\n","    \n","      \n","### FMRI - Language\n","#loading data\n","\n","if platform.system() == 'Windows':\n","    file_path = r\"D:\\Tensor Based ML for Neuro Imaging\\INSPIRE_CAHBHIR\\Python Scripts\\LSR-Tensor-Ridge-Regression\\Data_Sets\\HCP\\Resting State FMRI\\features_lang.mat\"\n","elif platform.system() == 'Darwin':\n","    file_path = 'add the path'\n","\n","dMRI_streamlog = (scipy.io.loadmat(file_path)['features_lang'])\n","\n","# Each sample is a row\n","fmri_rs = fmri_rs.T\n","dMRI_streamlog = dMRI_streamlog.T\n","\n","\n","# Determine the platform and load the appropriate file\n","if platform.system() == \"Windows\": \n","    mat_file_path = r\"D:\\Tensor Based ML for Neuro Imaging\\INSPIRE_CAHBHIR\\Python Scripts\\LSR-Tensor-Ridge-Regression\\Data_Sets\\HCP\\Resting State FMRI\\MMP_HCP_60_splits.mat\"\n","elif platform.system() == \"Darwin\":\n","    mat_file_path = \"/Users/lakrama/Neuro Project Codes/Datasets/Data_Sets/HCP/Resting State FMRI/MMP_HCP_60_splits.mat\"\n","else:\n","    raise ValueError(\"Unsupported platform\")\n","\n","# The dataframe to hold the results for a seed\n","columns = ['Seed', 'Best Lambda', 'NMSE', 'CORR', 'R2', 'Time Taken', 'Gradient']\n","results_df = pd.DataFrame(columns=columns)\n","\n","number_of_seeds = 2\n","\n","#iterating over the seeds\n","for seed in range (number_of_seeds):\n","\n","    # Load the .mat file\n","    mat_file = scipy.io.loadmat(mat_file_path)\n","\n","    # Extract subject lists from the loaded file\n","    seed_1 = mat_file['folds'][f'seed_{seed+1}'][0, 0]\n","    subject_lists = seed_1['sub_fold'][0, 0]['subject_list']\n","    test_subjects = [int(item[0]) for item in subject_lists[0, 0].flatten()]\n","\n","    print(seed)\n","\n","    #Getting the HCP test subjects \n","\n","    #array to hold the  subjects\n","    HCP_753_Subjects = []\n","    #file path\n","    file_path = ''\n","    #setting the file path\n","    if platform.system() == \"Windows\":\n","        file_path = r'D:\\Tensor Based ML for Neuro Imaging\\INSPIRE_CAHBHIR\\Python Scripts\\LSR-Tensor-Ridge-Regression\\Data_Sets\\HCP\\Resting State FMRI\\MMP_HCP_753_subs.txt'\n","    elif platform.system() == \"Darwin\":\n","        file_path = '/Users/lakrama/Neuro Project Codes/Datasets/Data_Sets/HCP/Resting State FMRI/MMP_HCP_753_subs.txt'\n","    #if file pat  h is returned then load\n","    if file_path:\n","        try:\n","            with open(file_path, 'r') as file:\n","                HCP_753_Subjects = [int(line.strip()) for line in file.readlines()]\n","        except Exception as e:\n","            print(f\"An error occurred: {e}\")\n","\n","\n","    #Put the HCP test subjects into a dataframe\n","    # Determine the platform and load the appropriate file\n","    if platform.system() == \"Windows\":\n","        csv_file_path = r\"D:\\Tensor Based ML for Neuro Imaging\\INSPIRE_CAHBHIR\\Python Scripts\\LSR-Tensor-Ridge-Regression\\Data_Sets\\HCP\\Resting State FMRI\\MMP_HCP_componentscores.csv\"\n","    elif platform.system() == \"Darwin\":\n","        csv_file_path = \"/Users/lakrama/Neuro Project Codes/Datasets/Data_Sets/HCP/Resting State FMRI/MMP_HCP_componentscores.csv\"\n","\n","    df = pd.read_csv(csv_file_path)\n","    df['Subject'] = pd.to_numeric(df['Subject'], errors='coerce')\n","    df = df[df['Subject'].isin(HCP_753_Subjects)].reset_index(drop = True)\n","\n","    #Split all our data into a Train and Test Set\n","    df_train, df_test = df[~df['Subject'].isin(test_subjects)], df[df['Subject'].isin(test_subjects)]\n","\n","    #Create train and test arrays\n","    train_subjects = df_train.index.to_list()\n","    test_subjects = df_test.index.to_list()\n","\n","    #Reshape labels into column vector\n","\n","    #Functional Connectivity Matrices\n","\n","    X_train_vec_fmri, Y_train = fmri_rs[train_subjects], df_train[\"varimax_cog\"].to_numpy().reshape((-1, 1))\n","    X_test_vec_fmri, Y_test = fmri_rs[test_subjects], df_test[\"varimax_cog\"].to_numpy().reshape((-1, 1))\n","\n","    #Structural Connectivity\n","\n","    X_train_vec_dmri = dMRI_streamlog[train_subjects]\n","    X_test_vec_dmri = dMRI_streamlog[test_subjects]\n","\n","    #merging the modalities\n","    X_train = samplestomat(X_train_vec_fmri,X_train_vec_dmri,400)\n","    X_test  = samplestomat(X_test_vec_fmri,X_test_vec_dmri,400)\n","    Y_train = Y_train.reshape(-1)\n","    Y_test = Y_test.reshape(-1)\n","\n","\n","    print(X_train.shape)\n","    print(Y_train.shape)\n","    print(X_test.shape)\n","    print(Y_test.shape)\n","\n","    #Kernel Equivalent Normalization Block\n","    X_train = normalize_by_frobenius_norm(X_train)\n","    X_test = normalize_by_frobenius_norm(X_test)\n","\n","    #number of samples in train and test \n","    n_train = X_train.shape[0]\n","    n_test = X_test.shape[0]\n","\n","    # Reshape the 3D array to a 2D array where each row represents a sample\n","    # The shape of the original 3D array is (n_samples, n_features_per_sample, n_dimensions)\n","    # We reshape it to (n_samples, n_features_per_sample * n_dimensions)\n","\n","    X_train_2D = X_train.reshape(n_train, -1)\n","    X_test_2D = X_test.reshape(n_test,-1)\n","\n","    # Initialize StandardScaler\n","    scaler = StandardScaler(with_std = False) #standard scalar only\n","\n","    # Fit scaler on train data and transform train data\n","    X_train_scaled = scaler.fit_transform(X_train_2D)\n","    # Transform test data using the scaler fitted on train data\n","    X_test_scaled = scaler.transform(X_test_2D)\n","\n","    # Reshape the scaled data back to 3D\n","    X_train = X_train_scaled.reshape(n_train, X_train.shape[1],X_train.shape[2])\n","    X_test  = X_test_scaled.reshape(n_test, X_test.shape[1],X_train.shape[2])\n","\n","    #average response value\n","    Y_train_mean = np.mean(Y_train)\n","    # Mean centering y_train and y_test\n","    Y_train = Y_train - Y_train_mean\n","\n","\n","    print(\"Sample mean for each feature (across samples):\",scaler.mean_)\n","    print(\"Sample variance for each feature (across samples):\",scaler.var_)\n","    print('Response Average:',Y_train_mean)\n","\n","    tensor_dimensions = np.array([X_train.shape[1], X_train.shape[2]])\n","    tensor_mode_ranks = np.array([4, 4])\n","    separation_rank = 2\n","\n","    #For now, define finite alpha set that we are searching over\n","    alphas =  [0.1,0.4] #,0.7,1,1.5,2,2.5,3,3.5,4,5,10,15,20]\n","\n","    #Define Number of Folds we want\n","    k_folds = 2\n","\n","    #Training \n","    hypers = {'max_iter': 50, 'threshold': 1e-4, 'ranks': tuple(tensor_mode_ranks), 'separation_rank': separation_rank}\n","    lambda1, validation_normalized_estimation_error, validation_nmse_losses, validation_correlations, validation_R2_scores, objective_function_information,gradient_information = KFoldCV(X_train, Y_train, alphas, k_folds, hypers, B_tensored = None, intercept= False)\n","\n","    #Testing \n","\n","    start_time = time.time()\n","    hypers = {'max_iter': 50, 'threshold': 1e-4, 'ranks': tuple(tensor_mode_ranks), 'separation_rank': separation_rank}\n","    test_normalized_estimation_error, test_nmse_loss, test_R2_loss, test_correlation, objective_function_values,gradient_values,iterate_level_values,factor_core_iteration = train_test(X_train, Y_train, X_test, Y_test, lambda1, hypers,Y_train_mean, B_tensored = None, intercept= False)\n","    end_time = time.time()\n","    elapsed_time = end_time - start_time\n","\n","\n","    #saving data of one seed\n","\n","    formatted_time = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n","    max_iter = hypers['max_iter']\n","\n","    if platform.system() == \"Windows\":\n","        pkl_file = rf\"D:\\Tensor Based ML for Neuro Imaging\\INSPIRE_CAHBHIR\\Python Scripts\\LSR-Tensor-Ridge-Regression_All_Data\\Closed_Form_Solver\\Multimodal\\LSR\\Frobenious\\Resting-Language\\Across_Seeds\\HCP_lambdas_{alphas}_seed_{seed}_sep_{separation_rank}_tucker_{tensor_mode_ranks}.pkl\"\n","    elif platform.system() == \"Darwin\":\n","        pkl_file = f\"/Users/lakrama/Neuro Project Codes/LSR-Tensor-Ridge-Regression/Stochastic LSR TRR/Experimental Results/ExecutionTime_intercept_5_{formatted_time}, n_train_{n_train},n_test_{n_test}, tensor_dimensions:{tensor_dimensions}, tensor_mode_= ranks:{tensor_mode_ranks}, separation_rank:{separation_rank}, max_iter={max_iter}.pkl\"\n","\n","    #printing error matrices for one seed\n","\n","    print(f'SEED: {seed+1} : lambda1:{lambda1} : TNMSE:{test_nmse_loss} : TCORR:{test_correlation} : TR2: {test_R2_loss} ')\n","\n","    #saving the data for one seed\n","    with open(pkl_file, \"wb\") as file:\n","        dill.dump((X_train,Y_train,X_test,Y_test, lambda1, validation_normalized_estimation_error, validation_nmse_losses, validation_correlations, validation_R2_scores, objective_function_information,gradient_information,test_normalized_estimation_error, test_nmse_loss, test_R2_loss, test_correlation, objective_function_values,gradient_values,iterate_level_values,factor_core_iteration), file)\n","\n","    #loading the results to the dataframe\n","\n","    best_lambda = lambda1\n","    nmse = test_nmse_loss\n","    corr = test_correlation\n","    r2 = test_R2_loss\n","    time_taken = elapsed_time\n","    gradient = gradient_values[-1,:,:]\n","    \n","    # Append the results to the dataframe\n","    seed_result_df = pd.DataFrame([{\n","        'Seed': seed,\n","        'Best Lambda': best_lambda,\n","        'NMSE': nmse,\n","        'CORR': corr,\n","        'R2': r2,\n","        'Time Taken': time_taken,\n","        'Gradient': gradient\n","    }])\n","    \n","    #concatenating the results\n","    results_df = pd.concat([results_df,seed_result_df], ignore_index = True) \n","\n","    # Define platform-specific file paths\n","    if platform.system() == 'Windows':\n","        file_path = r'D:\\Tensor Based ML for Neuro Imaging\\INSPIRE_CAHBHIR\\Python Scripts\\LSR-Tensor-Ridge-Regression_All_Data\\Closed_Form_Solver\\Multimodal\\LSR\\Frobenious\\Resting-Language\\Across_Seeds\\Seed_Results'\n","    elif platform.system() == 'Darwin':\n","        file_path = 'addpath'  # Replace 'addpath' with the actual path for macOS\n","\n","    # Save the result as a CSV\n","    results_df.to_csv(f'{file_path}/results_with_matrix_{seed}.csv', index=False)\n","\n","    \n","\n","    \n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"}},"nbformat":4,"nbformat_minor":0}
