{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Needed Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "###system \n",
    "import sys\n",
    "\n",
    "\n",
    "###loading and saving data\n",
    "import pickle\n",
    "import dill\n",
    "import datetime\n",
    "\n",
    "\n",
    "###bases\n",
    "import numpy as np\n",
    "\n",
    "import copy\n",
    "\n",
    "#preprocessing \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading all the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 32, 32)\n",
      "(5000,)\n",
      "(1000, 32, 32)\n",
      "(1000,)\n",
      "(32, 32)\n",
      "(4, 4)\n",
      "(6000,)\n",
      "factor matrix 1_1:(32, 4)\n",
      "factor matrix 1_1:(32, 4)\n",
      "factor matrix 1_1:(32, 4)\n",
      "factor matrix 1_1:(32, 4)\n"
     ]
    }
   ],
   "source": [
    "file= open(\"/Users/lakrama/Neuro Project Codes/Datasets/Data_Sets/Synthetic Data/Uncentered X/Bounded_Var_Time:2024-04-15 19:35:40, intercept:5,n_train:5000, n_test:1000, tensor_dimensions:[32 32], tensor_mode_ranks:[4 4], separation_rank:2.pkl\", 'rb')\n",
    "data = pickle.load(file)\n",
    "file.close()\n",
    "\n",
    "X_train_Full = data[0]\n",
    "print(data[0].shape)\n",
    "\n",
    "Y_train_Full = data[1]\n",
    "print(data[1].shape)\n",
    "\n",
    "X_test_Full = data[2]\n",
    "print(data[2].shape)\n",
    "\n",
    "Y_test_Full = data[3]\n",
    "print(data[3].shape)\n",
    "\n",
    "B_tensored = data[4]\n",
    "print(data[4].shape)\n",
    "\n",
    "G1 = data[5]\n",
    "print(data[5].shape)\n",
    "\n",
    "all_factormatrices = data[6]\n",
    "#print(data[6].shape)\n",
    "\n",
    "Y_train_nonoise = data[7]\n",
    "print(data[7].shape)\n",
    "\n",
    "#all the factor matrices \n",
    "B_1_1 = all_factormatrices[0][0]\n",
    "print(f'factor matrix 1_1:{B_1_1.shape}')\n",
    "B_1_2 = all_factormatrices[0][1]\n",
    "print(f'factor matrix 1_1:{B_1_2.shape}')\n",
    "B_2_1 = all_factormatrices[1][0]\n",
    "print(f'factor matrix 1_1:{B_2_1.shape}')\n",
    "B_2_2 = all_factormatrices[0][0]\n",
    "print(f'factor matrix 1_1:{B_2_2.shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All Needed Scripts "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LSR Tensor Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Low Separation Rank Tensor Decomposition\n",
    "class LSR_tensor_dot():\n",
    "    # Constructor\n",
    "    def __init__(self, shape, ranks, separation_rank, dtype = np.float32, intercept = False ,initialize = True):\n",
    "        super(LSR_tensor_dot, self).__init__()\n",
    "        self.shape = shape\n",
    "        self.ranks = ranks\n",
    "        self.separation_rank = separation_rank\n",
    "        self.dtype = dtype\n",
    "        self.order = len(shape)\n",
    "        self.init_params(initialize)\n",
    "        self.init_params(intercept)\n",
    "\n",
    "    # Initialize Parameters\n",
    "    def init_params(self, intercept = False ,initialize = True):\n",
    "        # Initialize core tensor as independent standard gaussians\n",
    "        if not initialize:\n",
    "            self.core_tensor = np.zeros(shape = self.ranks)\n",
    "        else:\n",
    "            self.core_tensor = np.random.normal(size = self.ranks)\n",
    "\n",
    "        # Set up Factor Matrices\n",
    "        self.factor_matrices = []\n",
    "\n",
    "        # Initialize all factor matrices\n",
    "        for s in range(self.separation_rank):\n",
    "            factors_s = []\n",
    "            for k in range(self.order):\n",
    "                if not initialize:\n",
    "                    factor_matrix_B = np.eye(self.shape[k])[:, self.ranks[k]]\n",
    "                    factors_s.append(factor_matrix_B)\n",
    "                else:\n",
    "                    factor_matrix_A = np.random.normal(0,1,size= (self.shape[k], self.ranks[k]))\n",
    "                    factors_s.append(factor_matrix_A)\n",
    "\n",
    "            self.factor_matrices.append(factors_s)\n",
    "\n",
    "        if intercept:\n",
    "          ('intercept is initialized')\n",
    "          self.b = np. random.normal(0,1)\n",
    "        else:\n",
    "          (print('intercept is not initialized'))\n",
    "          self.b = 0\n",
    "\n",
    "    # Expand core tensor and factor matrices to full tensor, optionally excluding\n",
    "    # a given term from the separation rank decomposition\n",
    "    def expand_to_tensor(self, skip_term = None):\n",
    "        full_lsr_tensor = np.zeros(shape = self.shape)\n",
    "\n",
    "        #Calculate Expanded Tensor\n",
    "        for s, term_s_factors in enumerate(self.factor_matrices):\n",
    "            if s == skip_term: continue\n",
    "            expanded_tucker_term = term_s_factors[0] @ self.core_tensor @ term_s_factors[1].T\n",
    "            full_lsr_tensor += expanded_tucker_term\n",
    "\n",
    "        #Column Wise Flatten full_lsr_tensor\n",
    "        full_lsr_tensor = full_lsr_tensor.flatten(order = 'F')\n",
    "        return full_lsr_tensor\n",
    "\n",
    "    # Absorb all factor matrices and core tensor into the input tensor except for matrix s, k\n",
    "    # Used during a factor matrix update step of block coordiante descent\n",
    "    def bcd_factor_update_x_y(self, s, k, x, y):\n",
    "        #Take x and swap axes 1 and 2 so that vectorization occurs \"COLUMN WISE\"\n",
    "        x_transpose = np.transpose(x, (0, 2, 1))\n",
    "        x_transpose_vectorized = np.reshape(x_transpose, newshape = (x_transpose.shape[0], -1))\n",
    "\n",
    "        #if we are unfolding along mode 1, use x. Else, if we are unfolding along mode, use x_transpose\n",
    "        x_partial_unfold = x if k == 0 else x_transpose\n",
    "\n",
    "        #If k = 0(skip first factor matrix), we have 2nd factor matrix. If k= 1(skip second factor matrix), we have first factor matrix\n",
    "        kronecker_term = self.factor_matrices[s][1] if k == 0 else self.factor_matrices[s][0]\n",
    "\n",
    "        #if k = 0, G^T. Else if k = 1, put G\n",
    "        core_tensor_term = self.core_tensor.T if k == 0 else self.core_tensor\n",
    "\n",
    "        omega = x_partial_unfold @ kronecker_term @ core_tensor_term\n",
    "        omega = np.transpose(omega, (0, 2, 1))\n",
    "        omega = np.reshape(omega, newshape = (omega.shape[0], -1))\n",
    "\n",
    "        X_tilde = omega\n",
    "        y_tilde = y\n",
    "\n",
    "        if self.separation_rank == 1:\n",
    "            pass\n",
    "        else:\n",
    "            gamma = np.dot(x_transpose_vectorized,self.expand_to_tensor(skip_term = s))\n",
    "            #gamma = gamma.reshape(-1,1)\n",
    "            y_tilde = y - gamma\n",
    "\n",
    "        return X_tilde, y_tilde\n",
    "\n",
    "    # Absorb all factor matrices the input tensor (not the core tensor)\n",
    "    # Used during a core tensor update step of block coordiante descent\n",
    "    def bcd_core_update_x_y(self, x, y):\n",
    "        #Take x and swap axes 1 and 2 so that vectorization occurs \"COLUMN WISE\"\n",
    "        x_transpose = np.transpose(x, (0, 2, 1))\n",
    "        x_transpose_vectorized = np.reshape(x_transpose, newshape = (x_transpose.shape[0], -1))\n",
    "\n",
    "        #Calculate y_tilde\n",
    "        y_tilde = y\n",
    "\n",
    "        #Calculate Kronecker Factor Sum\n",
    "        kron_factor_sum = 0\n",
    "        for term_s_factors in self.factor_matrices:\n",
    "            kron_factor_sum += np.kron(term_s_factors[1], term_s_factors[0])\n",
    "\n",
    "        #Return Core Update\n",
    "        return (kron_factor_sum.T @ x_transpose_vectorized.T).T, y_tilde\n",
    "\n",
    "\n",
    "    #Retrieve factor matrix\n",
    "    def get_factor_matrix(self, s, k):\n",
    "      return self.factor_matrices[s][k]\n",
    "\n",
    "    #Update factor matrix\n",
    "    def update_factor_matrix(self, s, k, updated_factor_matrix: np.ndarray):\n",
    "      self.factor_matrices[s][k] = updated_factor_matrix\n",
    "\n",
    "    def update_intercept(self,updated_b):\n",
    "      self.b = updated_b\n",
    "\n",
    "    #Retrieve Core Matrix\n",
    "    def get_core_matrix(self):\n",
    "      return self.core_tensor\n",
    "\n",
    "    #Update core matrix\n",
    "    def update_core_matrix(self, updated_core_matrix: np.ndarray):\n",
    "      self.core_tensor = updated_core_matrix\n",
    "\n",
    "    #Retrive intercept\n",
    "    def get_intercept(self):\n",
    "      return self.b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BCD Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lsr_ten: LSR Tensor\n",
    "#training_data: X\n",
    "#training_labels: Y\n",
    "#hypers: hyperparameters\n",
    "def lsr_bcd_regression(lsr_ten, training_data: np.ndarray, training_labels: np.ndarray, hypers: dict,intercept = False, Initializer = None):\n",
    "    #Get LSR Tensor Information and other hyperparameters\n",
    "    shape, ranks, sep_rank, order = lsr_ten.shape, lsr_ten.ranks, lsr_ten.separation_rank, lsr_ten.order\n",
    "    lambda1 = hypers[\"weight_decay\"]\n",
    "    max_iter = hypers[\"max_iter\"]\n",
    "    threshold = hypers[\"threshold\"]\n",
    "    lr        = hypers[\"learning_rate\"]\n",
    "    epochs    = hypers[\"epochs\"]\n",
    "    batch_size = hypers[\"batch_size\"]\n",
    "    decay_factor = hypers[\"decay_factor\"]\n",
    "    b_intercept = intercept\n",
    "\n",
    "    #Create models for each factor matrix and core matrix\n",
    "    #factor_matrix_models = [[Ridge(alpha = lambda1, solver = 'svd', fit_intercept = intercept) for k in range(len(ranks))] for s in range(sep_rank)]\n",
    "    #core_tensor_model = Ridge(alpha = lambda1, solver = 'svd', fit_intercept = intercept)\n",
    "\n",
    "    #Store objective function values\n",
    "    objective_function_values = np.ones(shape = (max_iter, sep_rank, len(ranks) + 1)) * np.inf\n",
    "\n",
    "    X, y = training_data, training_labels\n",
    "    if intercept: b_start = lsr_ten.get_intercept()\n",
    "    expanded_lsr_start  = lsr_ten.expand_to_tensor()\n",
    "    expanded_lsr_start  = np.reshape(expanded_lsr_start, X[0].shape, order = 'F')\n",
    "    objective_function_value_star = objective_function_tensor_sep(y, X, expanded_lsr_start,lsr_ten, lambda1, b if intercept else None)\n",
    "    print('Objective Function Value:',objective_function_value_star)\n",
    "\n",
    "    #Normalized Estimation Error\n",
    "    iterations_normalized_estimation_error = np.zeros(shape = (max_iter,))\n",
    "    \n",
    "    #Gradient Values\n",
    "    gradient_values = np.ones(shape = (max_iter, sep_rank, len(ranks) + 1)) * np.inf\n",
    "\n",
    "    #Epoch Level Function Values \n",
    "    epoch_level_function_values = np.ones(shape = (max_iter,sep_rank,len(ranks)+1,epochs))*np.inf\n",
    "\n",
    "    #Epoch Level Gradients\n",
    "    epoch_gradient_values = np.ones(shape = (max_iter,sep_rank,len(ranks)+1,epochs))*np.inf\n",
    "\n",
    "    #saving the tensor\n",
    "    tensor_iteration = []\n",
    "    #saving iterate-wise data\n",
    "    factor_core_iterates = []\n",
    "\n",
    "    #iterate differences \n",
    "    iterate_difference = np.ones(shape = (max_iter, sep_rank, len(ranks) + 1)) * np.inf\n",
    "\n",
    "    #Run at most max_iter iterations of Block Coordinate Descent\n",
    "    for iteration in range(max_iter):\n",
    "        #print('')\n",
    "        print('--------------------------------------------------------------BCD iteration',iteration,'--------------------------------------------------------------')\n",
    "        factor_residuals = np.zeros(shape = (sep_rank, len(ranks)))\n",
    "        core_residual = 0\n",
    "\n",
    "        #Store updates to factor matrices and core tensor\n",
    "        updated_factor_matrices = np.empty((sep_rank, len(ranks)), dtype=object)\n",
    "        updated_core_tensor = None\n",
    "\n",
    "        #Iterate over the Factor Matrices.\n",
    "        for s in range(sep_rank):\n",
    "            for k in range(len(ranks)):\n",
    "                \n",
    "                #Absorb Factor Matrices into X aside from (s, k) to get X_tilde\n",
    "                print('---------------------------------------------Sep',s,'Factor',k,'-------------------------------------------------')\n",
    "                X, y = training_data, training_labels\n",
    "                X_tilde, y_tilde = lsr_ten.bcd_factor_update_x_y(s, k, X, y) #y tilde should now be y-b-<Q,X>\n",
    "                \n",
    "\n",
    "                #Solve the sub-problem pertaining to the factor tensor\n",
    "                hypers = {'lambda': lambda1, 'lr': lr, 'epochs': epochs, 'batch_size': batch_size, 'bias': b_intercept, 'decay_factor': decay_factor}\n",
    "                #weights, bias, loss_values, gap_to_optimality, nmse_values, corr_values, R2_values,sub_problem_gradient = SGD(X_tilde, y_tilde.reshape(-1,1), cost_function_code = 1, hypers = hypers , optimizer_code = 0, p_star = 0)\n",
    "                \n",
    "                sub_problem_gradient = 0\n",
    "                loss_values = 0\n",
    "                bias = 0\n",
    "\n",
    "\n",
    "                #printing the subproblem gradients\n",
    "                #print(f\"Final gradient of the subproblem {s,k} : {sub_problem_gradient[-1]}\")\n",
    "                epoch_gradient_values[iteration,s,k,:] = sub_problem_gradient\n",
    "                epoch_level_function_values[iteration,s,k,:] = loss_values\n",
    "\n",
    "                #Retrieve Original and Updated Factor Matrices\n",
    "                #Bk = lsr_ten.get_factor_matrix(s, k)\n",
    "                #Bk1 = weights\n",
    "                if intercept: b = bias\n",
    "\n",
    "                #Shape Bk1 as needed\n",
    "                #Bk1 = np.reshape(Bk1, (shape[k], ranks[k]), order = 'F') #if there is an error check here\n",
    "\n",
    "                #fixing the factor matrices to be the true value \n",
    "                Bk  = all_factormatrices[s][k]\n",
    "                Bk1 = all_factormatrices[s][k]\n",
    "\n",
    "\n",
    "                #Update Residuals and store updated factor matrix\n",
    "                factor_residuals[s][k] = np.linalg.norm(Bk1 - Bk)\n",
    "                updated_factor_matrices[s, k] = Bk1\n",
    "\n",
    "                iterate_difference[iteration,s,k] = factor_residuals[s][k]\n",
    "\n",
    "\n",
    "                #Update Factor Matrix\n",
    "                lsr_ten.update_factor_matrix(s, k, updated_factor_matrices[s, k])\n",
    "\n",
    "                #update the intercept\n",
    "                if intercept: lsr_ten.update_intercept(b)\n",
    "\n",
    "                #Calculate Objective Function Value\n",
    "                expanded_lsr = lsr_ten.expand_to_tensor()\n",
    "                expanded_lsr = np.reshape(expanded_lsr, X[0].shape, order = 'F')\n",
    "                objective_function_value = objective_function_tensor_sep(y, X, expanded_lsr,lsr_ten,lambda1)\n",
    "                objective_function_values[iteration, s, k] = objective_function_value\n",
    "\n",
    "                #Print Objective Function Values\n",
    "                print(f\"Iteration: {iteration}, Separation Rank: {s}, Factor Matrix: {k}, Objective Function Value: {objective_function_value}\")\n",
    "                \n",
    "                #Calculate Gradient Values\n",
    "                bk = np.reshape(Bk, (-1, 1), order = 'F') #Flatten Factor Matrix Column Wise\n",
    "                Omega = X_tilde\n",
    "                z = bias\n",
    "                gradient_value = (-2 * Omega.T) @ (y_tilde.reshape(-1,1) - Omega @ bk  - z) + (2 * lambda1 * bk)\n",
    "                \n",
    "                #Store Gradient Values\n",
    "                gradient_values[iteration, s, k] = np.linalg.norm(gradient_value, ord = 'fro')\n",
    "\n",
    "\n",
    "        #Absorb necessary matrices into X, aside from core tensor, to get X_tilde\n",
    "        print('---------------------------------------------Core-------------------------------------------------')\n",
    "        X, y = training_data, training_labels\n",
    "        X_tilde, y_tilde = lsr_ten.bcd_core_update_x_y(X, y)\n",
    "\n",
    "        #Solve the sub-problem pertaining to the core tensor\n",
    "        hypers = {'lambda': lambda1, 'lr': lr, 'epochs': epochs, 'batch_size': batch_size, 'bias': intercept, 'decay_factor':decay_factor}\n",
    "        weights, bias, loss_values, gap_to_optimality, nmse_values, corr_values, R2_values,sub_problem_gradient,loss_values = SGD(X_tilde, y_tilde.reshape(-1,1), cost_function_code = 1, hypers = hypers, optimizer_code = 0, p_star = 0, Initializer = Initializer)\n",
    "\n",
    "        print(f\"Final gradient of the subproblem Core : {sub_problem_gradient[-1]}\")\n",
    "        epoch_gradient_values[iteration,:,len(ranks),:] = sub_problem_gradient\n",
    "        epoch_level_function_values[iteration,:,len(ranks),:] = loss_values\n",
    "\n",
    "        #Get Original and Updated Core Tensor\n",
    "        Gk = lsr_ten.get_core_matrix()\n",
    "        Gk1 = np.reshape(weights, ranks, order = 'F')\n",
    "        b = bias\n",
    "\n",
    "        #Update Residuals and store updated Core Tensor\n",
    "        core_residual = np.linalg.norm(Gk1 - Gk)\n",
    "        updated_core_tensor = Gk1\n",
    "\n",
    "        #saving iterate differece in a list\n",
    "        iterate_difference[iteration,:,len(ranks)] = core_residual\n",
    "\n",
    "        \n",
    "        #Update Core Tensor\n",
    "        lsr_ten.update_core_matrix(updated_core_tensor)\n",
    "\n",
    "\n",
    "        #Update Intercept\n",
    "\n",
    "        if intercept: lsr_ten.update_intercept(b)\n",
    "\n",
    "        #Calculate Objective Function Value\n",
    "        if intercept: b = lsr_ten.get_intercept()\n",
    "        expanded_lsr = lsr_ten.expand_to_tensor()\n",
    "        expanded_lsr = np.reshape(expanded_lsr, X[0].shape, order = 'F')\n",
    "\n",
    "        #saving the lsr tensor \n",
    "        tensor_iteration.append(expanded_lsr)\n",
    "\n",
    "        objective_function_value = objective_function_tensor_sep(y, X, expanded_lsr,lsr_ten, lambda1, b if intercept else None)\n",
    "        objective_function_values[iteration, :, (len(ranks))] = objective_function_value\n",
    "        \n",
    "        #print('')\n",
    "        #Print Objective Function Value\n",
    "        # print(f\"BCD Regression, Iteration: {iteration}, Core Tensor, Objective Function Value: {objective_function_value}\")\n",
    "        \n",
    "        #Calculate Gradient Values\n",
    "        g = np.reshape(Gk1, (-1, 1), order = 'F') #Flatten Core Matrix Column Wise\n",
    "        Omega = X_tilde\n",
    "        z = bias\n",
    "        gradient_value = (-2 * Omega.T) @ (y_tilde.reshape(-1,1) - Omega @ g  - z) + (2 * lambda1 * g)\n",
    "        \n",
    "        #Store Gradient Value\n",
    "        gradient_values[iteration, :, (len(ranks))] = np.linalg.norm(gradient_value, ord='fro')\n",
    "\n",
    "        #storing lsr_ten\n",
    "        factor_core_iterates.append(copy.deepcopy(lsr_ten))\n",
    "\n",
    "        #Stopping Criteria\n",
    "        diff = np.sum(factor_residuals.flatten()) + core_residual  #need to change this\n",
    "        # print('------------------------------------------------------------------------------------------')\n",
    "        # print(f\"Value of Stopping Criteria: {diff}\")\n",
    "        # print(f\"Expanded Tensor: {expanded_lsr}\")\n",
    "        # print('------------------------------------------------------------------------------------------')\n",
    "        if diff < threshold: \n",
    "            print(f'The threshold activated{diff}')\n",
    "            break\n",
    "\n",
    "\n",
    "    return lsr_ten, objective_function_values, gradient_values,iterate_difference,epoch_gradient_values,epoch_level_function_values,tensor_iteration,factor_core_iterates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Contains All Helper Functions for Optimization\n",
    "import numpy as np\n",
    "\n",
    "#Calculate value of objective function(vectorized case)\n",
    "def objective_function_vectorized(y: np.ndarray, X: np.ndarray, w: np.ndarray, alpha, b = None):\n",
    "    I = (X @ w).flatten()\n",
    "    y = y.flatten()\n",
    "    w = w.flatten()\n",
    "\n",
    "    if b is not None:\n",
    "      b = b.flatten()\n",
    "      function = (np.linalg.norm(y - I - b) ** 2) + (alpha * (np.linalg.norm(w) ** 2))\n",
    "    else:\n",
    "      function = (np.linalg.norm(y - I) ** 2) + (alpha * (np.linalg.norm(w) ** 2))\n",
    "    return function\n",
    "\n",
    "#Calculate value of objective function(tensor case)\n",
    "def objective_function_tensor(y: np.ndarray, X: np.ndarray, B: np.ndarray, alpha,b = None):\n",
    "    I = inner_product(X, B).flatten()\n",
    "    y = y.flatten()\n",
    "    B = B.flatten()\n",
    "    if b is not None:\n",
    "      b = b.flatten()\n",
    "      function = (np.linalg.norm(y - I -b) ** 2) + (alpha * (np.linalg.norm(B) ** 2))\n",
    "    else:\n",
    "      function = (np.linalg.norm(y - I) ** 2) + (alpha * (np.linalg.norm(B) ** 2))\n",
    "    return function\n",
    "\n",
    "def objective_function_tensor_sep(y: np.ndarray, X: np.ndarray, B: np.ndarray,lsr_ten, alpha,b = None):\n",
    "    I = inner_product(X, B).flatten()\n",
    "    y = y.flatten()\n",
    "    B = B.flatten()\n",
    "    regularizer = 0\n",
    "\n",
    "    #developing the separable regularizing term\n",
    "   \n",
    "    separation = len(lsr_ten.factor_matrices) \n",
    "    tucker = len(lsr_ten.factor_matrices[0])\n",
    "    \n",
    "    for s in range(separation):\n",
    "       for k in range(tucker):\n",
    "          regularizer += (np.linalg.norm(lsr_ten.factor_matrices[s][k])**2)\n",
    "    regularizer = regularizer + (np.linalg.norm(lsr_ten.core_tensor)**2)\n",
    "    if b is not None:\n",
    "      b = b.flatten()\n",
    "      function = (np.linalg.norm(y - I -b) ** 2) + (alpha * regularizer)\n",
    "    else:\n",
    "      function = (np.linalg.norm(y - I) ** 2) + (alpha * regularizer)\n",
    "    return function\n",
    "\n",
    "#Calculate x* and p* for Objective Function(Tensor Case)\n",
    "#X_train is a Tensor of samples x m x n\n",
    "#Y_train is a normal vector of size samples x 1. It can also be a flattened array of size (samples, )\n",
    "def calculate_optimal_iterate_and_function_value(X_train: np.ndarray, Y_train: np.ndarray, lambda1):\n",
    "    X_train = X_train.reshape((X_train.shape[0], -1))\n",
    "    Y_train = Y_train.reshape((-1, 1))\n",
    "\n",
    "    #Calculate Optimal Weight Tensor and Optimal Objective Function Value\n",
    "    B_optimal = np.linalg.inv(X_train.T @ X_train + lambda1 * np.eye(X_train.shape[1])) @ X_train.T @ Y_train\n",
    "    I = X_train @ B_optimal\n",
    "    p_star = (np.linalg.norm(Y_train - I) ** 2) + (lambda1 * (np.linalg.norm(B_optimal) ** 2))\n",
    "\n",
    "    return B_optimal, p_star\n",
    "\n",
    "\n",
    "#Inner product of two tensors\n",
    "#tensor1: samples x m x n\n",
    "#tensor2: m x n\n",
    "def inner_product(tensor1: np.ndarray, tensor2: np.ndarray):\n",
    "    tensor1 = tensor1.reshape(tensor1.shape[0], -1)\n",
    "    tensor2 = tensor2.reshape(-1, 1)\n",
    "    return tensor1 @ tensor2\n",
    "\n",
    "#Calculate R2 Score\n",
    "def R2(y_true, y_pred):\n",
    "    #Flatten for insurance\n",
    "    y_true = y_true.flatten()\n",
    "    y_pred = y_pred.flatten()\n",
    "\n",
    "    #Calculate R2 Score\n",
    "    y_true_mean = np.mean(y_true)\n",
    "    tss = np.sum((y_true - y_true_mean) ** 2)\n",
    "    rss = np.sum((y_true - y_pred) ** 2)\n",
    "    r2 = 1 - (rss / tss)\n",
    "    return r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import Optimization Toolkits\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import r2_score\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "\n",
    "#Cost Function[Least Squares]\n",
    "#||(XW + b) - Y||_2^2\n",
    "class LeastSquares(nn.Module):\n",
    "    def __init__(self, input_dim, uses_bias = False):\n",
    "        super(LeastSquares, self).__init__() #Initialize class\n",
    "        self.linear = nn.Linear(input_dim, 1, uses_bias) #input_dim = dimension of each sample in X, output_dim = 1 = number of y values for each sample\n",
    "                \n",
    "    #Evaluate the Cost Function given X and y\n",
    "    def evaluate(self, X, Y, reduction = 'sum'):\n",
    "        mse_loss = nn.MSELoss(reduction = reduction)\n",
    "        return mse_loss(self.linear(X), Y.rehshape(-1,1))\n",
    "\n",
    "#Cost Function[Least Squares + L2 Regularization Term]\n",
    "#||(XW + b) - Y ||_2^2 + lambda * ||w||^2_2\n",
    "class RidgeRegression(nn.Module):\n",
    "    def __init__(self, input_dim, lmbd, uses_bias = False,Initializer = None):\n",
    "        super(RidgeRegression, self).__init__()\n",
    "\n",
    "        #if we want to initialize the weights\n",
    "        if Initializer is not None:\n",
    "            initialized_weights = Initializer # weight matrix for the initialization \n",
    "            self.linear = nn.Linear(input_dim, 1, uses_bias) #input_dim = dimension of each sample in X, output_dim = 1 = number of y values for each sample\n",
    "            self.linear.weight.data = initialized_weights # wegiht initialization  \n",
    "        #else the function randomly intializes the weights\n",
    "        else:\n",
    "            self.linear = nn.Linear(input_dim, 1, uses_bias) #input_dim = dimension of each sample in X, output_dim = 1 = number of y values for each sample\n",
    "        \n",
    "        self.lmbd = lmbd #Ridge Regression Lambda Value\n",
    "        \n",
    "    #Evaluate the Cost Function given X and y\n",
    "    def evaluate(self, X, Y, reduction = 'sum'):\n",
    "        mse_loss = nn.MSELoss(reduction = reduction)\n",
    "        return mse_loss(self.linear(X), Y) + self.l2_regularization()\n",
    "            \n",
    "    #Calculate value of lambda * ||w||^2_2\n",
    "    def l2_regularization(self):\n",
    "        return self.lmbd * (torch.norm(self.linear.weight) ** 2)\n",
    "    \n",
    "#Perform Exact Line Search for Ridge Regression\n",
    "#Ridge Regression: ||(XW + b) - Y ||_2^2 + lambda * ||w||^2_2\n",
    "def exact_line_search_RR(X: np.ndarray, Y: np.ndarray, lmbd, cost_function, uses_bias):\n",
    "    #Get Model Parameters\n",
    "    W = cost_function.linear.weight.data.numpy().reshape((-1, 1)) \n",
    "    b = cost_function.linear.bias.item() if uses_bias else 0\n",
    "    \n",
    "    #Search Direction\n",
    "    DeltaW = -1 * cost_function.linear.weight.grad.numpy().reshape((-1, 1))\n",
    "    Deltab = -1 * cost_function.linear.bias.grad if uses_bias else 0\n",
    "    \n",
    "    #Compute value of t\n",
    "    numerator = -((X@W + b - Y).T @ (X @ DeltaW + Deltab)) - (lmbd * (W.T @ DeltaW))\n",
    "    denominator = (np.linalg.norm(X @ DeltaW + Deltab) ** 2) + (lmbd * (np.linalg.norm(DeltaW) ** 2))\n",
    "    t = (numerator / denominator) [0, 0]\n",
    "    \n",
    "    return t    \n",
    "    \n",
    "#Optimize a Cost Function via Stochastic Gradient Descent\n",
    "#X: Shape n x d where n is the number of samples and d is the number of features\n",
    "#Y: Shape n x 1 where n is the number of samples\n",
    "#cost_function_code: 0 for Normal Least Squares, 1 for Ridge Regression\n",
    "#hypers: hyperparameters\n",
    "#optimizer_code: 0 for SGD, 1 for Adagrad, 2 for RMSProp\n",
    "#p_star: estimated optimal value\n",
    "#W_true: true weights\n",
    "def SGD(X: np.ndarray, Y: np.ndarray, cost_function_code = 1, hypers = {}, optimizer_code = 0, p_star = 0, W_true = None, Initializer = None):\n",
    "    hypers = defaultdict(int, hypers) #Convert hypers to defaultdict\n",
    "\n",
    "    #Get necessary hyperparameters\n",
    "    uses_bias = hypers['bias'] #determine whether the bias term is needed\n",
    "    lmbd = hypers['lambda'] #Lambda for ridge regression\n",
    "    lr = hypers['lr'] #learning rate\n",
    "    epochs = hypers['epochs'] #number of epochs\n",
    "    batch_size = hypers['batch_size'] #batch size to use for SGD\n",
    "\n",
    "\n",
    "    #Get additional hyperparameters\n",
    "    momentum = hypers['momentum']\n",
    "    dampening = hypers['dampening']\n",
    "    nesterov = hypers['nesterov']\n",
    "    decay_factor = hypers['decay_factor']\n",
    "    \n",
    "    if Initializer is not None:\n",
    "        Initializer = Initializer.flatten(order = 'F')\n",
    "        Initializer = torch.tensor(Initializer, dtype = torch.float32).reshape(1,-1)\n",
    "      \n",
    "    #Initialize Cost Function\n",
    "    if cost_function_code == 0:\n",
    "        cost_function = LeastSquares(X.shape[1], uses_bias)\n",
    "    elif cost_function_code == 1:\n",
    "        cost_function = RidgeRegression(X.shape[1], lmbd, uses_bias,Initializer)\n",
    "    \n",
    "    #Convert X and Y to pytorch tensors\n",
    "    X = torch.tensor(X, dtype = torch.float32)\n",
    "    Y = torch.tensor(Y, dtype = torch.float32)\n",
    "    \n",
    "    #Splitting data into minibatches \n",
    "    dataset = TensorDataset(X,Y)\n",
    "    dataloader = DataLoader(dataset, batch_size = batch_size,  shuffle=True )\n",
    "    \n",
    "    #Initialize Optimizer\n",
    "    if optimizer_code == 0:\n",
    "        optimizer = optim.SGD(cost_function.parameters(), lr = lr, momentum = momentum, dampening = dampening, nesterov = nesterov)\n",
    "    elif optimizer_code == 1:\n",
    "        optimizer = optim.Adagrad(cost_function.parameters(), lr = lr)\n",
    "    elif optimizer_code == 2:\n",
    "        optimizer = optim.RMSprop(cost_function.parameters(), lr = lr, alpha = decay_factor, momentum = momentum)\n",
    "\n",
    "    #scheduler = MultiStepLR(optimizer,milestones=[80],gamma = 100)\n",
    "    \n",
    "    #Store batch loss values\n",
    "    loss_values = []\n",
    "    \n",
    "\n",
    "    #Store gap to optimality\n",
    "    gap_to_optimality = []\n",
    "\n",
    "    #saving gradient \n",
    "    sub_problem_gradient =  []\n",
    "    \n",
    "    #Store Metric Values\n",
    "\n",
    "    nmse_values = []\n",
    "    corr_values = []\n",
    "    R2_values = []\n",
    "\n",
    "    #Training Loop\n",
    "    for epoch in range(epochs):\n",
    "    \n",
    "        num_batches = 0\n",
    "        sum_gradient_norm = 0\n",
    "\n",
    "        for X_sample, Y_sample in dataloader:\n",
    "        \n",
    "            Y_sample = Y_sample.reshape(-1,1)\n",
    "            \n",
    "            # Zero gradients\n",
    "            #optimizer.zero_grad()\n",
    "\n",
    "            \n",
    "            # Zero the gradients\n",
    "            for param in cost_function.parameters():\n",
    "                if param.grad is not None:\n",
    "                    param.grad.zero_()\n",
    "\n",
    "            # Compute stochastic loss\n",
    "            stochastic_loss = cost_function.evaluate(X_sample, Y_sample, 'sum')\n",
    "\n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Backward pass to compute stochastic gradient\n",
    "            stochastic_loss.backward()\n",
    "\n",
    "\n",
    "            # Update parameters\n",
    "            t = exact_line_search_RR(X, Y, lmbd, cost_function, uses_bias)\n",
    "            print(f\"Value of t is: {t}\")\n",
    "        \n",
    "            # Manually update the weights and biases\n",
    "            with torch.no_grad():\n",
    "                for param in cost_function.parameters():\n",
    "                    param -= t * param.grad\n",
    "            \n",
    "            #storing the batch wise gradients \n",
    "            for param in cost_function.parameters():\n",
    "               #print(f\"Gradient Norm: {torch.norm(param.grad)}\")\n",
    "               stochastic_gradient  = torch.norm(param.grad)\n",
    "               sum_gradient_norm += stochastic_gradient\n",
    "        \n",
    "            \n",
    "            num_batches += 1\n",
    "\n",
    "        #Print and Store batch loss values\n",
    "        batch_loss = cost_function.evaluate(X, Y, 'sum')\n",
    "        loss_values.append(batch_loss.item())\n",
    "        gap_to_optimality.append(batch_loss.item() - p_star)\n",
    "        \n",
    "        #Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Backward pass to compute stochastic gradient\n",
    "        batch_loss.backward()\n",
    "\n",
    "        for param in cost_function.parameters():\n",
    "                print(f\"Batch Gradient Norm: {torch.norm(param.grad)}\")\n",
    "                gradient_after_epoch = torch.norm(param.grad)\n",
    "                sub_problem_gradient.append(gradient_after_epoch)\n",
    "\n",
    "        epcoh_gradient = sum_gradient_norm/num_batches\n",
    "        # sub_problem_gradient.append(epcoh_gradient)\n",
    "        print('Average Gradient Over Batches:',epcoh_gradient)\n",
    "        \n",
    "        #Calculate Metrics\n",
    "        print(cost_function.linear.weight.data.numpy().shape)\n",
    "        weights = cost_function.linear.weight.data.numpy().reshape((-1, 1))\n",
    "        bias = cost_function.linear.bias.item() if uses_bias else 0\n",
    "        X_numpy = X.numpy()\n",
    "        Y_predicted = X_numpy @ weights + bias\n",
    "        Y_numpy = Y.numpy()\n",
    "        \n",
    "\n",
    "        nmse = np.sum(np.square((Y_predicted - Y_numpy))) / np.sum(np.square(Y_numpy))\n",
    "        correlation = np.corrcoef(Y_predicted.flatten(), Y_numpy.flatten())[0, 1]\n",
    "        R2_score = r2_score(Y_numpy, Y_predicted)\n",
    "        \n",
    "        nmse_values.append(nmse)\n",
    "        corr_values.append(correlation)\n",
    "        R2_values.append(R2_score)\n",
    "        \n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {batch_loss:.4f}, Gap to Optimality: {gap_to_optimality[-1]:.4f}, NMSE: {nmse}, Correlation: {correlation}, R2: {R2_score}')\n",
    "\n",
    "    # Create a figure with two horizontal subplots\n",
    "    #fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "    # Plotting in the first subplot (ax1)\n",
    "    #ax1.plot(range(1, len(loss_values) + 1), loss_values, marker='o')\n",
    "    #ax1.set_title('Subproblem Loss')\n",
    "    #ax1.set_yscale('log')\n",
    "    #ax1.set_xlabel('Epoch')\n",
    "    #ax1.set_ylabel('Loss')\n",
    "    #ax1.grid(True)\n",
    "\n",
    "    # Plotting in the second subplot (ax2)\n",
    "    #ax2.plot(range(1, len(sub_problem_gradient) + 1), sub_problem_gradient, marker='o')\n",
    "    #ax2.set_title('Subproblem Gradient')\n",
    "    #ax2.set_yscale('log')\n",
    "    #ax2.set_xlabel('Epoch')\n",
    "    #ax2.set_ylabel('Loss')\n",
    "    #ax2.grid(True)\n",
    "\n",
    "    # Adjust the layout to prevent overlap\n",
    "    #plt.tight_layout()\n",
    "\n",
    "    # Display the plot\n",
    "    #plt.show()\n",
    "\n",
    "    weights = cost_function.linear.weight.data.numpy().reshape((-1, 1)) #Return weights as numpy array\n",
    "\n",
    "    #return weights and bias and loss metrics\n",
    "    if uses_bias:\n",
    "        return weights, cost_function.linear.bias.item(), loss_values, gap_to_optimality, nmse_values, corr_values, R2_values,sub_problem_gradient,loss_values\n",
    "    else:\n",
    "        return weights, 0, loss_values, gap_to_optimality, nmse_values, corr_values, R2_values,sub_problem_gradient,loss_values\n",
    "    \n",
    "\n",
    "#Optimize a Cost Function via Gradient Descent with Exact Line Search\n",
    "#X: Shape n x d where n is the number of samples and d is the number of features\n",
    "#Y: Shape n x 1 where n is the number of samples\n",
    "#cost_function_code: 0 for Normal Least Squares, 1 for Ridge Regression\n",
    "#hypers: hyperparameters\n",
    "#p_star: estimated optimal value\n",
    "#W_true: true weights\n",
    "def GD2(X: np.ndarray, Y: np.ndarray, cost_function_code = 1, hypers = {}, p_star = 0, W_true = None):\n",
    "    hypers = defaultdict(int, hypers) #Convert hypers to defaultdict\n",
    "    \n",
    "    #Get necessary hyperparameters\n",
    "    uses_bias = hypers['bias'] #determine whether the bias term is needed\n",
    "    lmbd = hypers['lambda'] #Lambda for ridge regression\n",
    "    epochs = hypers['epochs'] #number of epochs\n",
    "    \n",
    "    #Initialize Cost Function\n",
    "    if cost_function_code == 0:\n",
    "        cost_function = LeastSquares(X.shape[1], uses_bias)\n",
    "    elif cost_function_code == 1:\n",
    "        cost_function = RidgeRegression(X.shape[1], lmbd, uses_bias)\n",
    "    \n",
    "    #Convert X and Y to pytorch tensors\n",
    "    X_tensor = torch.tensor(X, dtype = torch.float32)\n",
    "    Y_tensor = torch.tensor(Y, dtype = torch.float32)\n",
    "    \n",
    "    #If W_true is None, set it to a zero vector\n",
    "    #if not isinstance(W_true, np.ndarray):\n",
    "    #    W_true = np.zeros(shape = (X.shape[1], 1))\n",
    "        \n",
    "    #Store batch loss values\n",
    "    loss_values = []\n",
    "    \n",
    "    #Store gap to optimality\n",
    "    gap_to_optimality = []\n",
    "    \n",
    "    #Store Metric Values \n",
    "    #nee_values = []\n",
    "    nmse_values = []\n",
    "    corr_values = []\n",
    "    R2_values = []\n",
    "\n",
    "    #Training Loop\n",
    "    for epoch in range(epochs):\n",
    "        # Zero the gradients\n",
    "        for param in cost_function.parameters():\n",
    "            if param.grad is not None:\n",
    "                param.grad.zero_()\n",
    "            \n",
    "        # Compute loss\n",
    "        loss = cost_function.evaluate(X_tensor, Y_tensor, 'sum')\n",
    "\n",
    "        # Backward pass to compute gradient\n",
    "        loss.backward()\n",
    "\n",
    "        # Update parameters\n",
    "        t = exact_line_search_RR(X, Y, lmbd, cost_function, uses_bias)\n",
    "        print(f\"Value of t is: {t}\")\n",
    "        \n",
    "        # Manually update the weights and biases\n",
    "        with torch.no_grad():\n",
    "            for param in cost_function.parameters():\n",
    "                param -= t * param.grad\n",
    "        \n",
    "        #Print and Store loss values\n",
    "        loss_value = cost_function.evaluate(X_tensor, Y_tensor, 'sum').item()\n",
    "        loss_values.append(loss_value)\n",
    "        gap_to_optimality.append(loss_value - p_star)\n",
    "        \n",
    "        #Calculate Metrics\n",
    "        weights = cost_function.linear.weight.data.numpy().reshape((-1, 1))\n",
    "        bias = cost_function.linear.bias.item() if uses_bias else 0\n",
    "        X_numpy = X_tensor.numpy()\n",
    "        Y_predicted = X_numpy @ weights + bias\n",
    "        Y_numpy = Y_tensor.numpy()\n",
    "        \n",
    "        #nee = ((np.linalg.norm(weights - W_true)) ** 2) /  ((np.linalg.norm(W_true)) ** 2)\n",
    "        nmse = np.sum(np.square((Y_predicted - Y_numpy))) / np.sum(np.square(Y_numpy))\n",
    "        correlation = np.corrcoef(Y_predicted.flatten(), Y_numpy.flatten())[0, 1]\n",
    "        R2_score = r2_score(Y_numpy, Y_predicted)\n",
    "        \n",
    "        #nee_values.append(nee)\n",
    "        nmse_values.append(nmse)\n",
    "        corr_values.append(correlation)\n",
    "        R2_values.append(R2_score)\n",
    "                \n",
    "        #print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss_value:.4f}, Gap to Optimality: {gap_to_optimality[-1]:.4f}, NMSE: {nmse}, Correlation: {correlation}, R2: {R2_score}')\n",
    "        \n",
    "        # Stopping Criteria\n",
    "        criteria_satisfied = True\n",
    "        for name, param in cost_function.named_parameters():\n",
    "            if param.grad is not None:\n",
    "                print(f\"Gradient Norm for {name}: {torch.norm(param.grad)}\")\n",
    "                criteria_satisfied = criteria_satisfied and (torch.norm(param.grad) <= 0.001)\n",
    "            else:\n",
    "                print(f\"No gradient Norm for {name}\")\n",
    "        \n",
    "        if criteria_satisfied:\n",
    "            break\n",
    "\n",
    "    weights = cost_function.linear.weight.data.numpy().reshape((-1, 1)) #Return weights as numpy array\n",
    "\n",
    "    #return weights and bias and loss metrics\n",
    "    if uses_bias:\n",
    "        return weights, cost_function.linear.bias.item(), loss_values, gap_to_optimality, nmse_values, corr_values, R2_values\n",
    "    else:\n",
    "        return weights, 0, loss_values, gap_to_optimality, nmse_values, corr_values, R2_values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_sgd(X_train: np.ndarray, Y_train: np.ndarray, X_test: np.ndarray, Y_test: np.ndarray, lambda1, hypers,Y_train_mean,lsr_tensor_SGD,B_tensored = None,intercept = False, Initializer = None):\n",
    "  hypers['weight_decay'] = lambda1\n",
    "\n",
    "  \n",
    "  #Define LSR Tensor Hyperparameters\n",
    "  ranks = hypers['ranks']\n",
    "  separation_rank = hypers['separation_rank']\n",
    "  LSR_tensor_dot_shape = tuple(X_train.shape)[1:]\n",
    "  need_intercept = intercept\n",
    "\n",
    "  #Construct LSR Tensor\n",
    "  lsr_tensor = lsr_tensor_SGD\n",
    "  lsr_tensor, objective_function_values, gradient_values,iterate_differences,epoch_level_gradients,epoch_level_function,tensor_iteration,factor_core_iterate = lsr_bcd_regression(lsr_tensor, X_train, Y_train, hypers,intercept = need_intercept, Initializer = Initializer)\n",
    "  expanded_lsr = lsr_tensor.expand_to_tensor()\n",
    "  expanded_lsr = np.reshape(expanded_lsr, X_train[0].shape, order = 'F')\n",
    "  print(expanded_lsr.shape)\n",
    "  Y_test_predicted = inner_product(np.transpose(X_test, (0, 2, 1)), expanded_lsr.flatten(order ='F')) + lsr_tensor.b + Y_train_mean\n",
    "\n",
    "  print('---------------------------Testing with Best Lambda------------------------------')\n",
    "  #print(f\"Y_test_predicted: {Y_test_predicted.flatten()}, Y_test: {Y_test.flatten()}\")\n",
    "  test_nmse_loss = np.sum(np.square((Y_test_predicted.flatten() - Y_test.flatten()))) / np.sum(np.square(Y_test.flatten()))\n",
    "  if B_tensored is not None:\n",
    "    normalized_estimation_error = ((np.linalg.norm(expanded_lsr - B_tensored)) ** 2) /  ((np.linalg.norm(B_tensored)) ** 2)\n",
    "  test_R2_loss = R2(Y_test.flatten(), Y_test_predicted.flatten())\n",
    "  test_correlation = np.corrcoef(Y_test_predicted.flatten(), Y_test.flatten())[0, 1]\n",
    "\n",
    "  print(\"Y Test Predicted: \", Y_test_predicted.flatten())\n",
    "  print(\"Y Test Actual: \", Y_test.flatten())\n",
    "\n",
    "  if B_tensored is not None:\n",
    "    return normalized_estimation_error, test_nmse_loss, test_R2_loss, test_correlation, objective_function_values,gradient_values,iterate_differences,epoch_level_gradients,epoch_level_function,tensor_iteration,factor_core_iterate\n",
    "  else:\n",
    "    normalized_estimation_error = np.inf\n",
    "    return normalized_estimation_error, test_nmse_loss, test_R2_loss, test_correlation, objective_function_values,gradient_values,iterate_differences,epoch_level_gradients,epoch_level_function,tensor_iteration,factor_core_iterate\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting the regression implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Setting up global parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_dimensions = np.array([32, 32])\n",
    "tensor_mode_ranks = np.array([4, 4])\n",
    "separation_rank = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample mean for each feature (across samples): [4.99871322 4.98420917 5.00873521 ... 5.01840176 4.98600789 5.01023623]\n",
      "Sample variance for each feature (across samples): None\n",
      "Response Average: 83.56008733093063\n"
     ]
    }
   ],
   "source": [
    "n_train = 650\n",
    "n_test = 100\n",
    "\n",
    "#Subset X_train and Y_train\n",
    "X_train = X_train_Full[0:(n_train),:,:]\n",
    "Y_train = Y_train_Full[0:(n_train)]\n",
    "\n",
    "#Subset X_test and Y_test\n",
    "X_test = X_test_Full[0:(n_test),:,:]\n",
    "Y_test = Y_test_Full[0:(n_test)]\n",
    "\n",
    "\n",
    "#Preprocessing\n",
    "\n",
    "# Reshape the 3D array to a 2D array where each row represents a sample\n",
    "# The shape of the original 3D array is (n_samples, n_features_per_sample, n_dimensions)\n",
    "# We reshape it to (n_samples, n_features_per_sample * n_dimensions)\n",
    "\n",
    "\n",
    "X_train_2D = X_train.reshape(n_train, -1)\n",
    "X_test_2D = X_test.reshape(n_test,-1)\n",
    "\n",
    "\n",
    "# Initialize StandardScaler\n",
    "scaler = StandardScaler(with_std = False) #standard scalar only\n",
    "\n",
    "# Fit scaler on train data and transform train data\n",
    "X_train_scaled = scaler.fit_transform(X_train_2D)\n",
    "# Transform test data using the scaler fitted on train data\n",
    "X_test_scaled = scaler.transform(X_test_2D)\n",
    "\n",
    "# Reshape the scaled data back to 3D\n",
    "X_train = X_train_scaled.reshape(n_train, tensor_dimensions[0],tensor_dimensions[1])\n",
    "X_test  = X_test_scaled.reshape(n_test, tensor_dimensions[0],tensor_dimensions[1])\n",
    "\n",
    "#average response value\n",
    "Y_train_mean = np.mean(Y_train)\n",
    "#Mean centering y_train and y_test\n",
    "Y_train = Y_train - Y_train_mean\n",
    "\n",
    "\n",
    "print(\"Sample mean for each feature (across samples):\",scaler.mean_)\n",
    "print(\"Sample variance for each feature (across samples):\",scaler.var_)\n",
    "print('Response Average:',Y_train_mean)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Intializing the tensor object "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intializing the tensor object \n",
    "\n",
    "hypers = {'max_iter': 50, 'threshold': 1e-8, 'ranks': tuple(tensor_mode_ranks), 'separation_rank': separation_rank}\n",
    "\n",
    "ranks = hypers['ranks']\n",
    "separation_rank = hypers['separation_rank']\n",
    "LSR_tensor_dot_shape = tuple(X_train.shape)[1:]\n",
    "need_intercept = False\n",
    "\n",
    "#initializing the tensor object\n",
    "#lsr_tensor = LSR_tensor_dot(shape = LSR_tensor_dot_shape, ranks = ranks, separation_rank = separation_rank, intercept = need_intercept)\n",
    "\n",
    "#regularization parameter\n",
    "lambda1 = 50\n",
    "\n",
    "\n",
    "#saving the initializer\n",
    "#formatted_time = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "#pkl_file = f\"/Users/lakrama/Neuro Project Codes/LSR-Tensor-Ridge-Regression_All_Data/Platforms for Experments/Data/Experiments_Isolating_Sub_Problems/Initializers_ExecutionTime_intercept_5_{formatted_time}, tensor_dimensions:{tensor_dimensions}, tensor_mode_= ranks:{tensor_mode_ranks}, separation_rank:{separation_rank}.pkl\"\n",
    "\n",
    "#with open(pkl_file, \"wb\") as file:\n",
    "#      dill.dump((lsr_tensor), file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the initializer\n",
    "\n",
    "# intializing the tensor object\n",
    "import sys\n",
    "sys.path.append('/Users/lakrama/Neuro Project Codes/LSR-Tensor-Ridge-Regression/Platforms_for_Experments/CodeFiles') \n",
    "from LSR_Tensor_2D_v1 import LSR_tensor_dot\n",
    "\n",
    "import pickle\n",
    "pkl_file = \"/Users/lakrama/Neuro Project Codes/LSR-Tensor-Ridge-Regression_All_Data/Platforms for Experments/Data/Experiments_Isolating_Sub_Problems/Initializers_ExecutionTime_intercept_5_2024-06-23 21:06:37, tensor_dimensions:[32 32], tensor_mode_= ranks:[4 4], separation_rank:2.pkl\"\n",
    "file= open(pkl_file, 'rb')\n",
    "lsr_tensor_initializer = pickle.load(file)\n",
    "file.close()\n",
    "\n",
    "lsr_tensor = copy.deepcopy(lsr_tensor_initializer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Trainning and Testing SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------------------Learning Rate 1.5e-06 -----------------------------------------\n",
      "Objective Function Value: 4831437.893446013\n",
      "--------------------------------------------------------------BCD iteration 0 --------------------------------------------------------------\n",
      "---------------------------------------------Sep 0 Factor 0 -------------------------------------------------\n",
      "Iteration: 0, Separation Rank: 0, Factor Matrix: 0, Objective Function Value: 4233888.149107747\n",
      "---------------------------------------------Sep 0 Factor 1 -------------------------------------------------\n",
      "Iteration: 0, Separation Rank: 0, Factor Matrix: 1, Objective Function Value: 3210320.131039708\n",
      "---------------------------------------------Sep 1 Factor 0 -------------------------------------------------\n",
      "Iteration: 0, Separation Rank: 1, Factor Matrix: 0, Objective Function Value: 3743681.83633678\n",
      "---------------------------------------------Sep 1 Factor 1 -------------------------------------------------\n",
      "Iteration: 0, Separation Rank: 1, Factor Matrix: 1, Objective Function Value: 3414681.423897649\n",
      "---------------------------------------------Core-------------------------------------------------\n",
      "Value of t is: 1.2386309435896692e-06\n",
      "Batch Gradient Norm: 148650.46875\n",
      "Average Gradient Over Batches: tensor(851896.3750)\n",
      "(1, 16)\n",
      "Epoch [1/500], Loss: 21757.3848, Gap to Optimality: 21757.3848, NMSE: 0.06293714046478271, Correlation: 0.9682713840618546, R2: 0.9370628636088234\n",
      "Value of t is: 1.562960619594378e-06\n",
      "Batch Gradient Norm: 62330.05078125\n",
      "Average Gradient Over Batches: tensor(148650.4844)\n",
      "(1, 16)\n",
      "Epoch [2/500], Loss: 4489.0400, Gap to Optimality: 4489.0400, NMSE: 0.01287528034299612, Correlation: 0.9937374220412641, R2: 0.9871247191589947\n",
      "Value of t is: 1.5360840279754484e-06\n",
      "Batch Gradient Norm: 32298.421875\n",
      "Average Gradient Over Batches: tensor(62330.0430)\n",
      "(1, 16)\n",
      "Epoch [3/500], Loss: 1505.1710, Gap to Optimality: 1505.1710, NMSE: 0.004217862617224455, Correlation: 0.9979182573495005, R2: 0.9957821377057213\n",
      "Value of t is: 1.7512628573967959e-06\n",
      "Batch Gradient Norm: 20219.48046875\n",
      "Average Gradient Over Batches: tensor(32298.4258)\n",
      "(1, 16)\n",
      "Epoch [4/500], Loss: 591.7228, Gap to Optimality: 591.7228, NMSE: 0.0015720621449872851, Correlation: 0.9992343279098599, R2: 0.9984279378358213\n",
      "Value of t is: 1.6058327219070634e-06\n",
      "Batch Gradient Norm: 11942.15234375\n",
      "Average Gradient Over Batches: tensor(20219.4824)\n",
      "(1, 16)\n",
      "Epoch [5/500], Loss: 263.4685, Gap to Optimality: 263.4685, NMSE: 0.0006182977231219411, Correlation: 0.9996957987639262, R2: 0.9993817022580584\n",
      "Value of t is: 1.787143219189602e-06\n",
      "Batch Gradient Norm: 7924.12548828125\n",
      "Average Gradient Over Batches: tensor(11942.1523)\n",
      "(1, 16)\n",
      "Epoch [6/500], Loss: 136.0317, Gap to Optimality: 136.0317, NMSE: 0.0002499774273019284, Correlation: 0.9998784114641277, R2: 0.9997500225587812\n",
      "Value of t is: 1.6201885273403605e-06\n",
      "Batch Gradient Norm: 4795.8134765625\n",
      "Average Gradient Over Batches: tensor(7924.1274)\n",
      "(1, 16)\n",
      "Epoch [7/500], Loss: 85.1644, Gap to Optimality: 85.1644, NMSE: 0.0001016927053569816, Correlation: 0.9999499245490238, R2: 0.9998983072987793\n",
      "Value of t is: 1.7969401824302622e-06\n",
      "Batch Gradient Norm: 3228.400146484375\n",
      "Average Gradient Over Batches: tensor(4795.8135)\n",
      "(1, 16)\n",
      "Epoch [8/500], Loss: 64.4997, Gap to Optimality: 64.4997, NMSE: 4.227434692438692e-05, Correlation: 0.9999795082620457, R2: 0.999957725651928\n",
      "Value of t is: 1.6251924535026774e-06\n",
      "Batch Gradient Norm: 1970.0357666015625\n",
      "Average Gradient Over Batches: tensor(3228.4026)\n",
      "(1, 16)\n",
      "Epoch [9/500], Loss: 56.0303, Gap to Optimality: 56.0303, NMSE: 1.7395723261870444e-05, Correlation: 0.9999913992107836, R2: 0.9999826042779323\n",
      "Value of t is: 1.8010993017014698e-06\n",
      "Batch Gradient Norm: 1334.0418701171875\n",
      "Average Gradient Over Batches: tensor(1970.0358)\n",
      "(1, 16)\n",
      "Epoch [10/500], Loss: 52.5353, Gap to Optimality: 52.5353, NMSE: 7.472826837329194e-06, Correlation: 0.999996414188796, R2: 0.9999925271734783\n",
      "Value of t is: 1.6276450196528458e-06\n",
      "Batch Gradient Norm: 817.3182983398438\n",
      "Average Gradient Over Batches: tensor(1334.0433)\n",
      "(1, 16)\n",
      "Epoch [11/500], Loss: 51.0869, Gap to Optimality: 51.0869, NMSE: 3.1420049708685838e-06, Correlation: 0.9999984342474123, R2: 0.9999968579948448\n",
      "Value of t is: 1.803407371880894e-06\n",
      "Batch Gradient Norm: 555.2259521484375\n",
      "Average Gradient Over Batches: tensor(817.3188)\n",
      "(1, 16)\n",
      "Epoch [12/500], Loss: 50.4846, Gap to Optimality: 50.4846, NMSE: 1.4847242937321425e-06, Correlation: 0.9999993060205388, R2: 0.999998515275645\n",
      "Value of t is: 1.6290706525978749e-06\n",
      "Batch Gradient Norm: 340.9519958496094\n",
      "Average Gradient Over Batches: tensor(555.2261)\n",
      "(1, 16)\n",
      "Epoch [13/500], Loss: 50.2335, Gap to Optimality: 50.2335, NMSE: 7.02552824805025e-07, Correlation: 0.9999996495414298, R2: 0.999999297447188\n",
      "Value of t is: 1.8048127685688087e-06\n",
      "Batch Gradient Norm: 232.05714416503906\n",
      "Average Gradient Over Batches: tensor(340.9518)\n",
      "(1, 16)\n",
      "Epoch [14/500], Loss: 50.1286, Gap to Optimality: 50.1286, NMSE: 4.360859122698457e-07, Correlation: 0.9999998047956367, R2: 0.9999995639140589\n",
      "Value of t is: 1.6299852632073453e-06\n",
      "Batch Gradient Norm: 142.71456909179688\n",
      "Average Gradient Over Batches: tensor(232.0558)\n",
      "(1, 16)\n",
      "Epoch [15/500], Loss: 50.0847, Gap to Optimality: 50.0847, NMSE: 2.86404826965736e-07, Correlation: 0.9999998618759244, R2: 0.9999997135951806\n",
      "Value of t is: 1.8054994370686472e-06\n",
      "Batch Gradient Norm: 97.23656463623047\n",
      "Average Gradient Over Batches: tensor(142.7130)\n",
      "(1, 16)\n",
      "Epoch [16/500], Loss: 50.0663, Gap to Optimality: 50.0663, NMSE: 2.490358497198031e-07, Correlation: 0.9999998905437496, R2: 0.999999750964168\n",
      "Value of t is: 1.6306921679642983e-06\n",
      "Batch Gradient Norm: 59.854156494140625\n",
      "Average Gradient Over Batches: tensor(97.2390)\n",
      "(1, 16)\n",
      "Epoch [17/500], Loss: 50.0586, Gap to Optimality: 50.0586, NMSE: 2.1733887933805818e-07, Correlation: 0.9999998993053103, R2: 0.9999997826611152\n",
      "Value of t is: 1.8061882656184025e-06\n",
      "Batch Gradient Norm: 40.82136535644531\n",
      "Average Gradient Over Batches: tensor(59.8539)\n",
      "(1, 16)\n",
      "Epoch [18/500], Loss: 50.0554, Gap to Optimality: 50.0554, NMSE: 2.1468291322435107e-07, Correlation: 0.9999999049627005, R2: 0.9999997853170791\n",
      "Value of t is: 1.630659994589223e-06\n",
      "Batch Gradient Norm: 25.13434600830078\n",
      "Average Gradient Over Batches: tensor(40.8202)\n",
      "(1, 16)\n",
      "Epoch [19/500], Loss: 50.0540, Gap to Optimality: 50.0540, NMSE: 2.0684112200797244e-07, Correlation: 0.9999999059696014, R2: 0.9999997931588853\n",
      "Value of t is: 1.807381750040804e-06\n",
      "Batch Gradient Norm: 17.15575408935547\n",
      "Average Gradient Over Batches: tensor(25.1340)\n",
      "(1, 16)\n",
      "Epoch [20/500], Loss: 50.0534, Gap to Optimality: 50.0534, NMSE: 2.080319347896875e-07, Correlation: 0.9999999072228242, R2: 0.9999997919680653\n",
      "Value of t is: 1.6303597476508003e-06\n",
      "Batch Gradient Norm: 10.562865257263184\n",
      "Average Gradient Over Batches: tensor(17.1549)\n",
      "(1, 16)\n",
      "Epoch [21/500], Loss: 50.0532, Gap to Optimality: 50.0532, NMSE: 2.0570537628827879e-07, Correlation: 0.9999999071745933, R2: 0.9999997942946178\n",
      "Value of t is: 1.8071731346935849e-06\n",
      "Batch Gradient Norm: 7.207739353179932\n",
      "Average Gradient Over Batches: tensor(10.5622)\n",
      "(1, 16)\n",
      "Epoch [22/500], Loss: 50.0531, Gap to Optimality: 50.0531, NMSE: 2.0661141775235592e-07, Correlation: 0.9999999075032895, R2: 0.9999997933885845\n",
      "Value of t is: 1.6328684750988032e-06\n",
      "Batch Gradient Norm: 4.44474983215332\n",
      "Average Gradient Over Batches: tensor(7.2045)\n",
      "(1, 16)\n",
      "Epoch [23/500], Loss: 50.0531, Gap to Optimality: 50.0531, NMSE: 2.058021379980346e-07, Correlation: 0.9999999073968492, R2: 0.999999794197853\n",
      "Value of t is: 1.8061683704218012e-06\n",
      "Batch Gradient Norm: 3.0283617973327637\n",
      "Average Gradient Over Batches: tensor(4.4439)\n",
      "(1, 16)\n",
      "Epoch [24/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.062540147562686e-07, Correlation: 0.9999999075031389, R2: 0.9999997937459741\n",
      "Value of t is: 1.6309988950524712e-06\n",
      "Batch Gradient Norm: 1.8699082136154175\n",
      "Average Gradient Over Batches: tensor(3.0283)\n",
      "(1, 16)\n",
      "Epoch [25/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0594555394382041e-07, Correlation: 0.9999999074427143, R2: 0.9999997940544402\n",
      "Value of t is: 1.8021627283815178e-06\n",
      "Batch Gradient Norm: 1.2721290588378906\n",
      "Average Gradient Over Batches: tensor(1.8712)\n",
      "(1, 16)\n",
      "Epoch [26/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.061576367395901e-07, Correlation: 0.9999999074763529, R2: 0.9999997938423547\n",
      "Value of t is: 1.6421896589235985e-06\n",
      "Batch Gradient Norm: 0.7881213426589966\n",
      "Average Gradient Over Batches: tensor(1.2717)\n",
      "(1, 16)\n",
      "Epoch [27/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0603171435595868e-07, Correlation: 0.9999999074512903, R2: 0.9999997939682662\n",
      "Value of t is: 1.7877666778076673e-06\n",
      "Batch Gradient Norm: 0.5316582918167114\n",
      "Average Gradient Over Batches: tensor(0.7893)\n",
      "(1, 16)\n",
      "Epoch [28/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0611420836758043e-07, Correlation: 0.9999999074654653, R2: 0.9999997938857855\n",
      "Value of t is: 1.6494535657329834e-06\n",
      "Batch Gradient Norm: 0.32131174206733704\n",
      "Average Gradient Over Batches: tensor(0.5315)\n",
      "(1, 16)\n",
      "Epoch [29/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.060681936200126e-07, Correlation: 0.9999999074522061, R2: 0.9999997939318005\n",
      "Value of t is: 1.912765810629935e-06\n",
      "Batch Gradient Norm: 0.22825680673122406\n",
      "Average Gradient Over Batches: tensor(0.3229)\n",
      "(1, 16)\n",
      "Epoch [30/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.060953931959375e-07, Correlation: 0.9999999074598194, R2: 0.9999997939046085\n",
      "Value of t is: 1.5631101177859819e-06\n",
      "Batch Gradient Norm: 0.13487115502357483\n",
      "Average Gradient Over Batches: tensor(0.2294)\n",
      "(1, 16)\n",
      "Epoch [31/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.060739916487364e-07, Correlation: 0.9999999074565834, R2: 0.999999793926015\n",
      "Value of t is: 1.957398580998415e-06\n",
      "Batch Gradient Norm: 0.1004549115896225\n",
      "Average Gradient Over Batches: tensor(0.1344)\n",
      "(1, 16)\n",
      "Epoch [32/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609535056337336e-07, Correlation: 0.9999999074596077, R2: 0.9999997939046624\n",
      "Value of t is: 1.5171093536991975e-06\n",
      "Batch Gradient Norm: 0.05709969624876976\n",
      "Average Gradient Over Batches: tensor(0.0997)\n",
      "(1, 16)\n",
      "Epoch [33/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0607770068181708e-07, Correlation: 0.9999999074578163, R2: 0.9999997939223052\n",
      "Value of t is: 2.1271894183882978e-06\n",
      "Batch Gradient Norm: 0.046829450875520706\n",
      "Average Gradient Over Batches: tensor(0.0564)\n",
      "(1, 16)\n",
      "Epoch [34/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.060896946431967e-07, Correlation: 0.9999999074572272, R2: 0.9999997939103129\n",
      "Value of t is: 1.4708323305967497e-06\n",
      "Batch Gradient Norm: 0.032471854239702225\n",
      "Average Gradient Over Batches: tensor(0.0443)\n",
      "(1, 16)\n",
      "Epoch [35/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.060815660342996e-07, Correlation: 0.9999999074567392, R2: 0.9999997939184159\n",
      "Value of t is: 1.4125766938377637e-06\n",
      "Batch Gradient Norm: 0.0280664823949337\n",
      "Average Gradient Over Batches: tensor(0.0326)\n",
      "(1, 16)\n",
      "Epoch [36/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609293471807177e-07, Correlation: 0.9999999074550409, R2: 0.9999997939070823\n",
      "Value of t is: 1.2693325288637425e-06\n",
      "Batch Gradient Norm: 0.028721150010824203\n",
      "Average Gradient Over Batches: tensor(0.0281)\n",
      "(1, 16)\n",
      "Epoch [37/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0608273132438626e-07, Correlation: 0.9999999074559842, R2: 0.9999997939172726\n",
      "Value of t is: 1.4084920394452638e-06\n",
      "Batch Gradient Norm: 0.023580312728881836\n",
      "Average Gradient Over Batches: tensor(0.0274)\n",
      "(1, 16)\n",
      "Epoch [38/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0608992201687215e-07, Correlation: 0.9999999074559388, R2: 0.9999997939100823\n",
      "Value of t is: 1.169777533505112e-06\n",
      "Batch Gradient Norm: 0.022236136719584465\n",
      "Average Gradient Over Batches: tensor(0.0240)\n",
      "(1, 16)\n",
      "Epoch [39/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0608955253464956e-07, Correlation: 0.9999999074560497, R2: 0.9999997939104364\n",
      "Value of t is: 1.268202709070465e-06\n",
      "Batch Gradient Norm: 0.022406848147511482\n",
      "Average Gradient Over Batches: tensor(0.0223)\n",
      "(1, 16)\n",
      "Epoch [40/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0608901252217038e-07, Correlation: 0.9999999074563923, R2: 0.9999997939109803\n",
      "Value of t is: 1.3014700925850775e-06\n",
      "Batch Gradient Norm: 0.021756963804364204\n",
      "Average Gradient Over Batches: tensor(0.0219)\n",
      "(1, 16)\n",
      "Epoch [41/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0608331396942958e-07, Correlation: 0.9999999074572872, R2: 0.9999997939166849\n",
      "Value of t is: 1.6389997199439676e-06\n",
      "Batch Gradient Norm: 0.021800553426146507\n",
      "Average Gradient Over Batches: tensor(0.0199)\n",
      "(1, 16)\n",
      "Epoch [42/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609115836123237e-07, Correlation: 0.9999999074557776, R2: 0.9999997939088547\n",
      "Value of t is: 1.1409961189201567e-06\n",
      "Batch Gradient Norm: 0.022074701264500618\n",
      "Average Gradient Over Batches: tensor(0.0228)\n",
      "(1, 16)\n",
      "Epoch [43/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.060863124597745e-07, Correlation: 0.9999999074563812, R2: 0.999999793913707\n",
      "Value of t is: 1.6115958487716853e-06\n",
      "Batch Gradient Norm: 0.022011900320649147\n",
      "Average Gradient Over Batches: tensor(0.0182)\n",
      "(1, 16)\n",
      "Epoch [44/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609139994576253e-07, Correlation: 0.9999999074556261, R2: 0.9999997939086029\n",
      "Value of t is: 1.09162886019476e-06\n",
      "Batch Gradient Norm: 0.019159920513629913\n",
      "Average Gradient Over Batches: tensor(0.0229)\n",
      "(1, 16)\n",
      "Epoch [45/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609154205430968e-07, Correlation: 0.9999999074554077, R2: 0.9999997939084616\n",
      "Value of t is: 1.1810340083684423e-06\n",
      "Batch Gradient Norm: 0.019131559878587723\n",
      "Average Gradient Over Batches: tensor(0.0202)\n",
      "(1, 16)\n",
      "Epoch [46/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609162731943798e-07, Correlation: 0.9999999074553709, R2: 0.9999997939083911\n",
      "Value of t is: 1.1876304597535636e-06\n",
      "Batch Gradient Norm: 0.019131561741232872\n",
      "Average Gradient Over Batches: tensor(0.0202)\n",
      "(1, 16)\n",
      "Epoch [47/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609162731943798e-07, Correlation: 0.9999999074553552, R2: 0.9999997939083576\n",
      "Value of t is: 1.1895422176166903e-06\n",
      "Batch Gradient Norm: 0.019132431596517563\n",
      "Average Gradient Over Batches: tensor(0.0202)\n",
      "(1, 16)\n",
      "Epoch [48/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609166995200212e-07, Correlation: 0.9999999074553453, R2: 0.9999997939083524\n",
      "Value of t is: 1.1908297210538876e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0202)\n",
      "(1, 16)\n",
      "Epoch [49/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.20596666874917e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [50/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2059733762725955e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [51/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2075208815076621e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [52/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2064025440849946e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [53/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2063459280398092e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [54/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2068794603692368e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [55/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2058083029842237e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [56/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2051141311530955e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0204)\n",
      "(1, 16)\n",
      "Epoch [57/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2058892480126815e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [58/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2068496744177537e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [59/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.207161972160975e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [60/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2058891343258438e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [61/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2058308129780926e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [62/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2060251037837588e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [63/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.206091155836475e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [64/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2064479051332455e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [65/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2063092071912251e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [66/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2072284789610421e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [67/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2049964652760536e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [68/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2063085250701988e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [69/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.206366050610086e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [70/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2067879424648709e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [71/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2055138540745247e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [72/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2072197250745376e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [73/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2060537528668647e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [74/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2065544297001907e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [75/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2062731684636674e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [76/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2061780125804944e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [77/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2068132946296828e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [78/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2076792472726083e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [79/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2061273082508706e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [80/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2064251677657012e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [81/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2071973287675064e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [82/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.20529853120388e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [83/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2063353551639011e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [84/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2060852441209136e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [85/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2052942111040466e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0204)\n",
      "(1, 16)\n",
      "Epoch [86/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2061541383445729e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [87/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2062808991686325e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [88/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2066711860825308e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [89/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.206301135425747e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [90/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2059930440955213e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [91/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2067395118720015e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [92/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2065024748153519e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [93/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2068783235008596e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [94/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2068652495145216e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [95/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2068112482666038e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [96/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2062142786817276e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [97/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.205413695970492e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [98/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2064313068549382e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [99/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2064959946656018e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [100/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2064012935297797e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [101/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2064263046340784e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [102/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2059178970957873e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [103/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2058438869644306e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [104/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2058106904078159e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [105/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2065626151525066e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [106/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.20666823022475e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [107/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.206546926368901e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [108/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2062446330673993e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [109/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2068327350789332e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [110/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.206177785206819e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [111/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2055804745614296e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [112/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2062423593306448e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [113/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2074958704033634e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [114/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2054509852532647e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [115/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2070751154169557e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [116/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2058229685862898e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [117/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2065880810041563e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [118/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2053656064381357e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [119/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2060264680258115e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [120/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.205753846988955e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [121/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.206155729960301e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [122/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2054539411110454e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [123/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.205780790769495e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0204)\n",
      "(1, 16)\n",
      "Epoch [124/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2055592151227756e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [125/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2066802810295485e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [126/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.205506350743235e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0204)\n",
      "(1, 16)\n",
      "Epoch [127/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2056560763085145e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0204)\n",
      "(1, 16)\n",
      "Epoch [128/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2068455816915957e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [129/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2061755114700645e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [130/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2071262744939304e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [131/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2065945611539064e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [132/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2056535751980846e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0204)\n",
      "(1, 16)\n",
      "Epoch [133/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2057911362717277e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0204)\n",
      "(1, 16)\n",
      "Epoch [134/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2058010270266095e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0204)\n",
      "(1, 16)\n",
      "Epoch [135/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.206589786306722e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [136/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.205753051181091e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [137/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2057105323037831e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [138/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2065057717336458e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [139/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2061962024745299e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [140/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2061764209647663e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [141/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.206626961902657e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [142/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2056767673129798e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [143/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2063983376719989e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [144/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2063181884514051e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [145/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2055759270879207e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [146/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2067664556525415e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [147/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2064033398928586e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [148/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2062297400916577e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0204)\n",
      "(1, 16)\n",
      "Epoch [149/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.207161517413624e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [150/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2053767477482324e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [151/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2063168242093525e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [152/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2065532928318135e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [153/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2065994496879284e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [154/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2058524134772597e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [155/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2057492995154462e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0204)\n",
      "(1, 16)\n",
      "Epoch [156/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.206732349601225e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [157/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2061962024745299e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [158/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.206110823659401e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [159/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2066154795320472e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [160/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2061660754625336e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [161/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2067440593455103e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [162/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2062257610523375e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [163/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2063553640473401e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [164/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2066932413290488e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [165/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2073508059984306e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [166/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.206455863211886e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [167/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2048175221934798e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0204)\n",
      "(1, 16)\n",
      "Epoch [168/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.205812282023544e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [169/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2054764511049143e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [170/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2061734651069855e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [171/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2066860790582723e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [172/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2064845122949919e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [173/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2071038781868992e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [174/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2068236401319155e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [175/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.206435854328447e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0204)\n",
      "(1, 16)\n",
      "Epoch [176/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2058995935149142e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0204)\n",
      "(1, 16)\n",
      "Epoch [177/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2067401939930278e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [178/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2064324437233154e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [179/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2070562434018939e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [180/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.206775664286397e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [181/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.207252239510126e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [182/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.205701551043603e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0204)\n",
      "(1, 16)\n",
      "Epoch [183/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2069984904883313e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [184/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2060284007020527e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [185/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.20639379019849e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [186/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2057370213369722e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0204)\n",
      "(1, 16)\n",
      "Epoch [187/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2071756145815016e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [188/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.206950400955975e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [189/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2055285196765908e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [190/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2061408369845594e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [191/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2066979024893953e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [192/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2055515981046483e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0204)\n",
      "(1, 16)\n",
      "Epoch [193/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2066429917467758e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [194/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2060453400408733e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [195/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2063854910593363e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [196/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2074717687937664e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [197/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2060595508955885e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [198/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2067540637872298e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [199/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2062829455317114e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [200/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2055493243678939e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [201/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2063425174346776e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [202/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.205608100462996e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [203/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.207238369715924e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [204/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2064805332556716e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [205/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2061473171343096e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [206/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2066971066815313e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [207/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2069224339938955e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [208/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2062804444212816e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [209/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2058618494847906e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [210/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.206741330861405e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [211/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.205426315209479e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [212/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2062785117450403e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [213/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2060094150001532e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [214/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2063507028869935e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [215/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2077302926627453e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [216/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.207989384965913e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [217/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2060154404025525e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [218/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2070657930962625e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [219/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.206412207466201e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [220/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.205951889460266e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [221/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2066419685652363e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [222/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.206086267302453e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [223/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2062547511959565e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [224/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2073859352312866e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [225/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2065775081282482e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [226/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2058025049555e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0204)\n",
      "(1, 16)\n",
      "Epoch [227/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2060727385687642e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0204)\n",
      "(1, 16)\n",
      "Epoch [228/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2082914508937392e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [229/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2055259048793232e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0204)\n",
      "(1, 16)\n",
      "Epoch [230/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2063237591064535e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [231/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2064934935551719e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [232/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.205774651680258e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [233/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.205982925966964e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [234/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2063657095495728e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [235/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.207312266160443e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [236/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2068951491528424e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [237/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2065082728440757e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [238/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2066224144291482e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [239/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2064978136550053e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [240/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2067571333318483e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [241/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.20595518637856e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [242/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2055498928020825e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0204)\n",
      "(1, 16)\n",
      "Epoch [243/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2064491556884605e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [244/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2063633221259806e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [245/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2064467682648683e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [246/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2075084896423505e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [247/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.205987018693122e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [248/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2071233186361496e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [249/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2068835530953947e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [250/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.204745558425202e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0204)\n",
      "(1, 16)\n",
      "Epoch [251/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2063740086887265e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [252/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2058768561473698e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [253/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2062197356499382e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [254/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.206712340717786e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [255/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.205827402372961e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [256/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2056292462148122e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0204)\n",
      "(1, 16)\n",
      "Epoch [257/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2055695606250083e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0204)\n",
      "(1, 16)\n",
      "Epoch [258/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2060361314070178e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [259/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2063269423379097e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [260/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2069012882420793e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [261/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2060922927048523e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [262/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.204899149342964e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [263/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.207214268106327e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [264/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.206274987453071e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [265/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2060628478138824e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [266/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2064247130183503e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [267/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.205889020639006e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [268/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2054852049914189e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0204)\n",
      "(1, 16)\n",
      "Epoch [269/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2062068890372757e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [270/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2066194585713674e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [271/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2063036365361768e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [272/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2060561402904568e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0204)\n",
      "(1, 16)\n",
      "Epoch [273/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.206718479807023e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [274/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2076105804226245e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [275/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2058681022608653e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [276/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2062403129675658e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [277/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2065662531313137e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [278/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.205798753289855e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [279/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2064285783708328e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [280/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2070062211932964e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [281/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2060212384312763e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [282/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.206088768412883e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [283/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2065189594068215e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [284/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2055605793648283e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [285/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2071195669705048e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [286/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2061562983944896e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [287/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2065038390574045e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [288/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2064464272043551e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [289/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2059912251061178e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [290/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2053664022459998e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0204)\n",
      "(1, 16)\n",
      "Epoch [291/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2062838550264132e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [292/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2064941756761982e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [293/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2057925005137804e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [294/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2063512713211821e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [295/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2054078979417682e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [296/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.206547722176765e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [297/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2062342875651666e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [298/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2060357903465047e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [299/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2067673651472433e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [300/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2060704648320097e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [301/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2070490811311174e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [302/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2066058161508408e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [303/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2062216683261795e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [304/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2046884876326658e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0204)\n",
      "(1, 16)\n",
      "Epoch [305/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2073948028046289e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [306/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2069031072314829e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [307/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2060260132784606e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0204)\n",
      "(1, 16)\n",
      "Epoch [308/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.206955744237348e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [309/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2060112339895568e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [310/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2058209222232108e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [311/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2066158205925603e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [312/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2060125982316094e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [313/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2067501984347473e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [314/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2059834944011527e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [315/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2063933354511391e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [316/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2066319641235168e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [317/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2064331258443417e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [318/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2057997764713946e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [319/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2060125982316094e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [320/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.206042611556768e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [321/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.206998604175169e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [322/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2065644341419102e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [323/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2059080063409056e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [324/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2057331559844897e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0204)\n",
      "(1, 16)\n",
      "Epoch [325/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2054383660142776e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [326/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2066659564879956e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [327/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2061836969223805e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [328/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2070044022038928e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [329/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2063360372849274e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [330/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2056441391905537e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0204)\n",
      "(1, 16)\n",
      "Epoch [331/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2078983218088979e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [332/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2066133194821305e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [333/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2070825050614076e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [334/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.206628212457872e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [335/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.205961325467797e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [336/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2064374459441751e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [337/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2060787639711634e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [338/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2061470897606341e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [339/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2064159591318457e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [340/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2065074770362116e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [341/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2062962468917249e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [342/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2065050896126195e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [343/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2076477560185594e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [344/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2065245300618699e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [345/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2071000128344167e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [346/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.206055230795755e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [347/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.206527713293326e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [348/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.207629566124524e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [349/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2066263934684685e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [350/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2061558436471387e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [351/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2064335805916926e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [352/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2058693528160802e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [353/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2066965382473427e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [354/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.20733386665961e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [355/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.207030095429218e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [356/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2060052085871575e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [357/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2062801033607684e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [358/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2062391760991886e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [359/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2063393342032214e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [360/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2059238088113489e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [361/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2060212384312763e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [362/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2062358791808947e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [363/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2067788475178531e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [364/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2066236649843631e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [365/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2063018175467732e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [366/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2055230627083802e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [367/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2058950460414053e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [368/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2058389984304085e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [369/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2057482763339067e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [370/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2064534757882939e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [371/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.206920160257141e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [372/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2063845815646346e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [373/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2066043382219505e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [374/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2061294683007873e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [375/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.206053184432676e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [376/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2065352166246157e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [377/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2065891041856958e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [378/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2067386023772997e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [379/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2063809435858275e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [380/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2058785614499357e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [381/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2063910617143847e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [382/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2066974477420445e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [383/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2059562095600995e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [384/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2063319445587695e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0204)\n",
      "(1, 16)\n",
      "Epoch [385/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2064401744282804e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [386/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2065504506608704e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [387/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2064458587701665e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [388/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2064721204296802e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [389/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2072215440639411e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [390/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2064821248713997e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [391/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2072927120243548e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [392/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2060754670528695e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [393/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2063064787071198e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [394/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2058685570082162e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [395/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.207071704811824e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [396/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2065670489391778e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [397/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2069871218045591e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [398/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.207048057949578e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [399/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2073296602466144e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [400/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2067463330822648e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [401/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2061778988936567e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [402/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.207313516715658e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [403/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.205777948598552e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0204)\n",
      "(1, 16)\n",
      "Epoch [404/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.207456421070674e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [405/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2070023558408138e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [406/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2065687542417436e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [407/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2067540637872298e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [408/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2062881751262466e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [409/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2076582152076298e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [410/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2065320333931595e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [411/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2081549130016356e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [412/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2068038586221519e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [413/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2069853028151556e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [414/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2060417020620662e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [415/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2069895092281513e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [416/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.206927549901593e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [417/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.206426873068267e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [418/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.205882540489256e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [419/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2064621159879607e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [420/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2062392897860263e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [421/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2067871466570068e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [422/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2062826044711983e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [423/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2060734206897905e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [424/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.206911178996961e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [425/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2062673704349436e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [426/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.206675165121851e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [427/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.207185277962708e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [428/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2057706726409378e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [429/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2060276048941887e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [430/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2065070222888608e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [431/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2064332395311794e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [432/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2067027910234174e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [433/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.20599111141928e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [434/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2066872159266495e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [435/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2069992862961954e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [436/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2066807357768994e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [437/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2061318557243794e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [438/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2044175718983752e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0204)\n",
      "(1, 16)\n",
      "Epoch [439/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2061801726304111e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [440/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.206689034916053e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [441/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2062877203788958e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [442/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2061138932040194e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [443/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2070538559783017e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [444/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2063279655194492e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [445/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2054073295075796e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [446/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2065398777849623e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [447/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2058770835210453e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0204)\n",
      "(1, 16)\n",
      "Epoch [448/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2063270560247474e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [449/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2059640539519023e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [450/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2061286724929232e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [451/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2063539998052875e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [452/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.205626062983356e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [453/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.206640490636346e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [454/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2067987427144544e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [455/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2057978437951533e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0204)\n",
      "(1, 16)\n",
      "Epoch [456/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2068190926584066e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [457/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2067855550412787e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [458/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2056169680363382e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [459/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2059284699716954e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [460/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.206384013130446e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [461/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.207647528644884e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [462/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2070102002326166e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [463/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2059630307703628e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [464/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.206688239108189e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [465/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2062140513080521e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0204)\n",
      "(1, 16)\n",
      "Epoch [466/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2057180356350727e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [467/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2067142733940273e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [468/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.207054538099328e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [469/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.205131070491916e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [470/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2060804692737292e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [471/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.206475076287461e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [472/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2057062122039497e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [473/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2064085694873938e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [474/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2061204870406073e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [475/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2059438176947879e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0204)\n",
      "(1, 16)\n",
      "Epoch [476/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2067279158145539e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [477/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2067150692018913e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [478/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2061734651069855e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [479/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.205896410283458e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [480/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2065141845596372e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [481/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2065980854458758e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [482/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2072197250745376e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [483/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.205837975248869e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [484/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2063044323440408e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [485/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2066255976606044e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [486/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.206755996463471e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [487/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.207132982017356e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [488/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2062150744895916e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [489/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2062777159371763e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [490/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2056729019604973e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [491/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2065252121828962e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [492/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2053109230691916e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0204)\n",
      "(1, 16)\n",
      "Epoch [493/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2067820307493093e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [494/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2066238923580386e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [495/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2056634659529664e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [496/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.205849798679992e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [497/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2059409755238448e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [498/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2066334420524072e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [499/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Value of t is: 1.2070495358784683e-06\n",
      "Batch Gradient Norm: 0.019297383725643158\n",
      "Average Gradient Over Batches: tensor(0.0203)\n",
      "(1, 16)\n",
      "Epoch [500/500], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609203943422472e-07, Correlation: 0.9999999074552323, R2: 0.9999997939079497\n",
      "Final gradient of the subproblem Core : 0.019297383725643158\n",
      "(32, 32)\n",
      "---------------------------Testing with Best Lambda------------------------------\n",
      "Y Test Predicted:  [ 67.54851998  64.19090824 100.9045593  106.79195294  66.53165902\n",
      "  96.7957129  112.76082833  87.08074136  58.88887489  60.74356578\n",
      "  65.75629082  64.15954585  82.80079854  68.56555719 142.45236902\n",
      "  98.01732846  98.35003061  55.42036386  96.74615569  87.26501725\n",
      "  47.9842439   91.13714211  73.56809211  77.46542017  46.96969988\n",
      "  79.38050563  84.71070383  45.49777649  87.56671263  73.48045039\n",
      "  86.90829616  88.2663345  124.64858414  81.42774588  87.54720702\n",
      "  82.79557885  98.73351237  69.04116347  80.63626753  93.08204302\n",
      "  68.41206296  87.2802022   62.04371158  57.34458513  56.02428\n",
      "  89.30909064 110.53951919 134.60484134  43.27187287  74.48265489\n",
      "  96.23124302  87.89203263  79.23336347  90.72654231 125.88036675\n",
      " 160.36058473  60.51482847  86.88878836  89.57342035  85.4977277\n",
      "  79.97210141  95.54164971  57.10211092  50.37365963  70.86327491\n",
      " 131.22864406  94.5097423   83.16108635  62.92848465  84.10487049\n",
      "  76.25991139  96.47914993  77.08529085  65.79279488  71.78157714\n",
      "  65.15276528  61.02853444  68.75485984  66.41329241 104.95440688\n",
      "  79.74195802  91.80726093 110.44403912  62.24932868  57.61790773\n",
      "  98.12796634  67.8377722   72.12938212 127.84632248  63.3117683\n",
      " 103.69507568 150.07057369  67.42141854  92.38849773 123.67422608\n",
      "  46.91580633  64.66215542 108.2167598  101.13736341 113.37588403]\n",
      "Y Test Actual:  [ 67.54932471  64.19367989 100.89907103 106.80033802  66.5296898\n",
      "  96.80085769 112.75716496  87.08379176  58.86645104  60.75871896\n",
      "  65.76651181  64.14603363  82.80477245  68.55912818 142.45923938\n",
      "  98.00053671  98.36489423  55.40988723  96.73410604  87.25275894\n",
      "  47.97392502  91.14302569  73.57729565  77.46144622  46.97758628\n",
      "  79.38435102  84.7040345   45.4879342   87.55672081  73.49886432\n",
      "  86.90598491  88.25175908 124.63253198  81.43265772  87.55287141\n",
      "  82.8080788   98.72768177  69.0468243   80.62856336  93.08545506\n",
      "  68.40807412  87.26847367  62.04717769  57.33990441  56.01334774\n",
      "  89.31934273 110.55219975 134.61307846  43.26935217  74.47617341\n",
      "  96.21843559  87.89937403  79.21862917  90.71786386 125.8824493\n",
      " 160.38439066  60.50387741  86.87508783  89.5739495   85.50880057\n",
      "  79.97185552  95.54965657  57.09355445  50.35928763  70.86185745\n",
      " 131.24078613  94.50837386  83.15863626  62.93422433  84.09831139\n",
      "  76.26182003  96.48126457  77.07300387  65.77838739  71.76861932\n",
      "  65.14937109  61.0440168   68.74400314  66.41967439 104.95340089\n",
      "  79.75508697  91.80406117 110.44770514  62.24491669  57.60974417\n",
      "  98.13365195  67.82489044  72.14320497 127.85744229  63.30804812\n",
      " 103.70015697 150.07489827  67.42743244  92.3906977  123.66379363\n",
      "  46.92967844  64.66629945 108.22935093 101.14612067 113.38031778]\n",
      "Error Report on Testing _ With best Lambda\n",
      "SGD_Alpha chosen for model:  50\n",
      "SGD_Test Normalized Estimation Error:  3.6489439602442444e-08\n",
      "SGD_Test NMSE Loss:  1.1993084164211057e-08\n",
      "SGD_Test R2 Loss:  0.9999998337232412\n",
      "SGD_Test Correlation:  0.999999923064121\n",
      "Objective Function Values 22199.189397278587\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final gradient: 0.019297383725643158\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learning_rates = [0.0000015]\n",
    "\n",
    "\n",
    "for idx,lr in enumerate (learning_rates):   \n",
    "      \n",
    "    try:\n",
    "      print('')\n",
    "      print('-------------------------------------Learning Rate',lr,'-----------------------------------------')\n",
    "      lsr_tensor_SGD = copy.deepcopy(lsr_tensor_initializer)\n",
    "      learning_rate = lr\n",
    "      epochs = 500\n",
    "      batch_size = 650\n",
    "\n",
    "      momentum = 0\n",
    "      nesterov = False\n",
    "      decay_factor = 0\n",
    "      hypers = {'max_iter': 1, 'threshold': 1e-4, 'ranks': tuple(tensor_mode_ranks), 'separation_rank': separation_rank,'learning_rate':learning_rate,'epochs':epochs,'batch_size': batch_size, 'momentum':momentum, 'nesterov': nesterov, 'decay_factor': decay_factor}\n",
    "\n",
    "      normalized_estimation_error_SGD, test_nmse_loss_SGD, test_R2_loss_SGD, test_correlation_SGD, objective_function_values_SGD,gradient_values_SGD,iterate_differences_SGD,epoch_level_gradients_SGD,epoch_level_function,tensor_iteration_SGD,factor_core_iterate_SGD = train_test_sgd(X_train, Y_train, X_test, Y_test, lambda1, hypers, Y_train_mean,lsr_tensor_SGD,B_tensored,intercept= False)\n",
    "\n",
    "    \n",
    "      #Get current time and store in variable\n",
    "      formatted_time = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "      max_iter = hypers['max_iter']\n",
    "      pkl_file = f\"/Users/lakrama/Neuro Project Codes/LSR-Tensor-Ridge-Regression_All_Data/Platforms for Experments/Data/Experiments_Isolating_Sub_Problems/Final lr search/After Indentifinig Batch Size Issue/Line Search/SGD_learning_rate_{learning_rate}_batch_size_{batch_size}_decay_{decay_factor}_intercept5_,ExecutionTime{formatted_time}, n_train_{n_train},n_test_{n_test}, tensor_dimensions:{tensor_dimensions}, tensor_mode_= ranks:{tensor_mode_ranks}, separation_rank:{separation_rank}, max_iter={max_iter}.pkl\"\n",
    "\n",
    "     #with open(pkl_file, \"wb\") as file:\n",
    "       # dill.dump((lsr_tensor_SGD,lambda1, normalized_estimation_error_SGD, test_nmse_loss_SGD, test_R2_loss_SGD, test_correlation_SGD, objective_function_values_SGD,gradient_values_SGD, iterate_differences_SGD,epoch_level_gradients_SGD,epoch_level_function,tensor_iteration_SGD,factor_core_iterate_SGD), file)\n",
    "\n",
    "\n",
    "      print(\"Error Report on Testing _ With best Lambda\")\n",
    "      print(\"SGD_Alpha chosen for model: \", lambda1)\n",
    "      print(\"SGD_Test Normalized Estimation Error: \", normalized_estimation_error_SGD)\n",
    "      print(\"SGD_Test NMSE Loss: \", test_nmse_loss_SGD)\n",
    "      print(\"SGD_Test R2 Loss: \", test_R2_loss_SGD)\n",
    "      print(\"SGD_Test Correlation: \", test_correlation_SGD)\n",
    "      print(\"Objective Function Values\", objective_function_values_SGD[0,1,2])\n",
    "\n",
    "      # Looking at the  variation of function values within a BCD iteration\n",
    "\n",
    "      import matplotlib.pyplot as plt\n",
    "\n",
    "      # Create a line plot\n",
    "      plt.plot(epoch_level_gradients_SGD[0,1,2,:], marker='o')\n",
    "\n",
    "      # Add title and labels\n",
    "      plt.title('Gradient Value')\n",
    "      plt.xlabel('X-axis')\n",
    "      plt.yscale('log')\n",
    "      plt.ylabel('Y-axis')\n",
    "\n",
    "      plt.show()\n",
    "\n",
    "      print(f'final gradient: {epoch_level_gradients_SGD[0,1,2,-1]}')\n",
    "      # fucnion value\n",
    "\n",
    "      # Create a line plot\n",
    "      plt.plot(epoch_level_function[0,1,2,:], marker='o')\n",
    "\n",
    "      # Add title and labels\n",
    "      plt.title('Function Value')\n",
    "      plt.xlabel('X-axis')\n",
    "      plt.yscale('log') \n",
    "      plt.ylabel('Y-axis')\n",
    "\n",
    "      plt.show()\n",
    "\n",
    "    except Exception as e:\n",
    "      print(f\"An error occurred for learning rate {lr}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this error is there because I am not reshaping the core tensor initialization before sending it to the nn.Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.34496707  0.48097593 -0.2687225   0.30525142]\n",
      " [-0.14356062 -0.12139282 -0.14575419 -0.15485618]\n",
      " [-0.04789635  0.2987569  -0.058126    0.18263052]\n",
      " [ 0.01656343 -0.04144901 -0.5225539   0.00063095]]\n",
      "0.9998193\n"
     ]
    }
   ],
   "source": [
    "print(lsr_tensor_SGD.core_tensor)\n",
    "print(np.linalg.norm(lsr_tensor_SGD.core_tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------------------Learning Rate 1.5e-06 -----------------------------------------\n",
      "Objective Function Value: 22200.018682486334\n",
      "--------------------------------------------------------------BCD iteration 0 --------------------------------------------------------------\n",
      "---------------------------------------------Sep 0 Factor 0 -------------------------------------------------\n",
      "Iteration: 0, Separation Rank: 0, Factor Matrix: 0, Objective Function Value: 22200.018682486334\n",
      "---------------------------------------------Sep 0 Factor 1 -------------------------------------------------\n",
      "Iteration: 0, Separation Rank: 0, Factor Matrix: 1, Objective Function Value: 22200.018682486334\n",
      "---------------------------------------------Sep 1 Factor 0 -------------------------------------------------\n",
      "Iteration: 0, Separation Rank: 1, Factor Matrix: 0, Objective Function Value: 22200.018682486334\n",
      "---------------------------------------------Sep 1 Factor 1 -------------------------------------------------\n",
      "Iteration: 0, Separation Rank: 1, Factor Matrix: 1, Objective Function Value: 22200.018682486334\n",
      "---------------------------------------------Core-------------------------------------------------\n",
      "Gradient Norm: 284.7720642089844\n",
      "(1, 16)\n",
      "Epoch [1/100], Loss: 50.1360, Gap to Optimality: 50.1360, NMSE: 4.786093086295296e-07, Correlation: 0.9999997651541283, R2: 0.999999521390636\n",
      "Gradient Norm: 118.2552490234375\n",
      "(1, 16)\n",
      "Epoch [2/100], Loss: 50.0704, Gap to Optimality: 50.0704, NMSE: 3.035655424810102e-07, Correlation: 0.9999998818183192, R2: 0.9999996964344474\n",
      "Gradient Norm: 57.841434478759766\n",
      "(1, 16)\n",
      "Epoch [3/100], Loss: 50.0577, Gap to Optimality: 50.0577, NMSE: 2.3242877489337843e-07, Correlation: 0.9999998957254149, R2: 0.9999997675712443\n",
      "Gradient Norm: 30.130828857421875\n",
      "(1, 16)\n",
      "Epoch [4/100], Loss: 50.0544, Gap to Optimality: 50.0544, NMSE: 2.2002672039889148e-07, Correlation: 0.9999999035333766, R2: 0.999999779973292\n",
      "Gradient Norm: 16.3952693939209\n",
      "(1, 16)\n",
      "Epoch [5/100], Loss: 50.0535, Gap to Optimality: 50.0535, NMSE: 2.1137716998964606e-07, Correlation: 0.9999999054559189, R2: 0.999999788622817\n",
      "Gradient Norm: 9.284897804260254\n",
      "(1, 16)\n",
      "Epoch [6/100], Loss: 50.0532, Gap to Optimality: 50.0532, NMSE: 2.092058508651462e-07, Correlation: 0.9999999065920796, R2: 0.99999979079416\n",
      "Gradient Norm: 5.444630146026611\n",
      "(1, 16)\n",
      "Epoch [7/100], Loss: 50.0531, Gap to Optimality: 50.0531, NMSE: 2.0750155727000674e-07, Correlation: 0.9999999069693972, R2: 0.9999997924984291\n",
      "Gradient Norm: 3.2890067100524902\n",
      "(1, 16)\n",
      "Epoch [8/100], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.069433548967936e-07, Correlation: 0.9999999072134781, R2: 0.9999997930566494\n",
      "Gradient Norm: 2.034191608428955\n",
      "(1, 16)\n",
      "Epoch [9/100], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0652936427723034e-07, Correlation: 0.9999999073096937, R2: 0.9999997934706502\n",
      "Gradient Norm: 1.2816413640975952\n",
      "(1, 16)\n",
      "Epoch [10/100], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0636215936065128e-07, Correlation: 0.9999999073778791, R2: 0.9999997936378388\n",
      "Gradient Norm: 0.8208673000335693\n",
      "(1, 16)\n",
      "Epoch [11/100], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0623387797513715e-07, Correlation: 0.9999999074091115, R2: 0.9999997937661268\n",
      "Gradient Norm: 0.526642918586731\n",
      "(1, 16)\n",
      "Epoch [12/100], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0618152518636634e-07, Correlation: 0.9999999074270065, R2: 0.9999997938184699\n",
      "Gradient Norm: 0.34256187081336975\n",
      "(1, 16)\n",
      "Epoch [13/100], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.061390489416226e-07, Correlation: 0.9999999074404649, R2: 0.9999997938609452\n",
      "Gradient Norm: 0.2259119302034378\n",
      "(1, 16)\n",
      "Epoch [14/100], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0612705498024297e-07, Correlation: 0.9999999074450604, R2: 0.9999997938729456\n",
      "Gradient Norm: 0.14738067984580994\n",
      "(1, 16)\n",
      "Epoch [15/100], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0611551576621423e-07, Correlation: 0.9999999074475433, R2: 0.9999997938844635\n",
      "Gradient Norm: 0.09849363565444946\n",
      "(1, 16)\n",
      "Epoch [16/100], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.060993153918389e-07, Correlation: 0.9999999074523707, R2: 0.9999997939006607\n",
      "Gradient Norm: 0.06721682846546173\n",
      "(1, 16)\n",
      "Epoch [17/100], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609293471807177e-07, Correlation: 0.9999999074543302, R2: 0.9999997939070834\n",
      "Gradient Norm: 0.04615115374326706\n",
      "(1, 16)\n",
      "Epoch [18/100], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609040518593247e-07, Correlation: 0.9999999074551359, R2: 0.999999793909594\n",
      "Gradient Norm: 0.035059623420238495\n",
      "(1, 16)\n",
      "Epoch [19/100], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609410000815842e-07, Correlation: 0.9999999074541359, R2: 0.999999793905888\n",
      "Gradient Norm: 0.03127063438296318\n",
      "(1, 16)\n",
      "Epoch [20/100], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609149942174554e-07, Correlation: 0.9999999074547571, R2: 0.9999997939085089\n",
      "Gradient Norm: 0.025981955230236053\n",
      "(1, 16)\n",
      "Epoch [21/100], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609850537312013e-07, Correlation: 0.9999999074555804, R2: 0.9999997939015091\n",
      "Gradient Norm: 0.024719154462218285\n",
      "(1, 16)\n",
      "Epoch [22/100], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609405737559428e-07, Correlation: 0.9999999074547941, R2: 0.9999997939059351\n",
      "Gradient Norm: 0.02281411923468113\n",
      "(1, 16)\n",
      "Epoch [23/100], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.061010917486783e-07, Correlation: 0.9999999074547804, R2: 0.9999997938989142\n",
      "Gradient Norm: 0.029333626851439476\n",
      "(1, 16)\n",
      "Epoch [24/100], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0608850093140063e-07, Correlation: 0.9999999074550028, R2: 0.9999997939114954\n",
      "Gradient Norm: 0.023926695808768272\n",
      "(1, 16)\n",
      "Epoch [25/100], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609843431884656e-07, Correlation: 0.9999999074554147, R2: 0.999999793901553\n",
      "Gradient Norm: 0.029727758839726448\n",
      "(1, 16)\n",
      "Epoch [26/100], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0608861461823835e-07, Correlation: 0.999999907454682, R2: 0.9999997939113777\n",
      "Gradient Norm: 0.025828620418906212\n",
      "(1, 16)\n",
      "Epoch [27/100], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.060989316987616e-07, Correlation: 0.9999999074553141, R2: 0.9999997939010676\n",
      "Gradient Norm: 0.02862093597650528\n",
      "(1, 16)\n",
      "Epoch [28/100], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0608801776234031e-07, Correlation: 0.9999999074549984, R2: 0.9999997939119811\n",
      "Gradient Norm: 0.025674903765320778\n",
      "(1, 16)\n",
      "Epoch [29/100], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.060992159158559e-07, Correlation: 0.9999999074552078, R2: 0.9999997939007885\n",
      "Gradient Norm: 0.028666820377111435\n",
      "(1, 16)\n",
      "Epoch [30/100], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0608796091892145e-07, Correlation: 0.9999999074550361, R2: 0.9999997939120412\n",
      "Gradient Norm: 0.025674905627965927\n",
      "(1, 16)\n",
      "Epoch [31/100], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.060990453855993e-07, Correlation: 0.9999999074552897, R2: 0.9999997939009637\n",
      "Gradient Norm: 0.02863902784883976\n",
      "(1, 16)\n",
      "Epoch [32/100], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.060880035514856e-07, Correlation: 0.9999999074549989, R2: 0.9999997939119787\n",
      "Gradient Norm: 0.025674905627965927\n",
      "(1, 16)\n",
      "Epoch [33/100], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609905959645403e-07, Correlation: 0.9999999074552841, R2: 0.9999997939009531\n",
      "Gradient Norm: 0.02863902971148491\n",
      "(1, 16)\n",
      "Epoch [34/100], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0608797512977617e-07, Correlation: 0.9999999074550101, R2: 0.9999997939120019\n",
      "Gradient Norm: 0.025674903765320778\n",
      "(1, 16)\n",
      "Epoch [35/100], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609905959645403e-07, Correlation: 0.9999999074552841, R2: 0.999999793900953\n",
      "Gradient Norm: 0.02866527996957302\n",
      "(1, 16)\n",
      "Epoch [36/100], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0608793249721202e-07, Correlation: 0.9999999074550369, R2: 0.9999997939120524\n",
      "Gradient Norm: 0.025674903765320778\n",
      "(1, 16)\n",
      "Epoch [37/100], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609918749414646e-07, Correlation: 0.9999999074552094, R2: 0.999999793900794\n",
      "Gradient Norm: 0.028666820377111435\n",
      "(1, 16)\n",
      "Epoch [38/100], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0608796091892145e-07, Correlation: 0.9999999074550361, R2: 0.9999997939120412\n",
      "Gradient Norm: 0.025674903765320778\n",
      "(1, 16)\n",
      "Epoch [39/100], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.060992159158559e-07, Correlation: 0.9999999074552078, R2: 0.9999997939007885\n",
      "Gradient Norm: 0.028666820377111435\n",
      "(1, 16)\n",
      "Epoch [40/100], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0608796091892145e-07, Correlation: 0.9999999074550361, R2: 0.9999997939120412\n",
      "Gradient Norm: 0.025674905627965927\n",
      "(1, 16)\n",
      "Epoch [41/100], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609905959645403e-07, Correlation: 0.9999999074552841, R2: 0.9999997939009531\n",
      "Gradient Norm: 0.02863902971148491\n",
      "(1, 16)\n",
      "Epoch [42/100], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0608797512977617e-07, Correlation: 0.9999999074550101, R2: 0.9999997939120019\n",
      "Gradient Norm: 0.025674903765320778\n",
      "(1, 16)\n",
      "Epoch [43/100], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609905959645403e-07, Correlation: 0.9999999074552841, R2: 0.999999793900953\n",
      "Gradient Norm: 0.02866527996957302\n",
      "(1, 16)\n",
      "Epoch [44/100], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0608793249721202e-07, Correlation: 0.9999999074550369, R2: 0.9999997939120524\n",
      "Gradient Norm: 0.025674903765320778\n",
      "(1, 16)\n",
      "Epoch [45/100], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.060992159158559e-07, Correlation: 0.9999999074552078, R2: 0.9999997939007885\n",
      "Gradient Norm: 0.028666820377111435\n",
      "(1, 16)\n",
      "Epoch [46/100], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0608796091892145e-07, Correlation: 0.9999999074550361, R2: 0.9999997939120412\n",
      "Gradient Norm: 0.025674905627965927\n",
      "(1, 16)\n",
      "Epoch [47/100], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609905959645403e-07, Correlation: 0.9999999074552841, R2: 0.9999997939009531\n",
      "Gradient Norm: 0.02863902971148491\n",
      "(1, 16)\n",
      "Epoch [48/100], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0608797512977617e-07, Correlation: 0.9999999074550101, R2: 0.9999997939120019\n",
      "Gradient Norm: 0.025674903765320778\n",
      "(1, 16)\n",
      "Epoch [49/100], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609905959645403e-07, Correlation: 0.9999999074552841, R2: 0.999999793900953\n",
      "Gradient Norm: 0.02866527996957302\n",
      "(1, 16)\n",
      "Epoch [50/100], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0608793249721202e-07, Correlation: 0.9999999074550369, R2: 0.9999997939120524\n",
      "Gradient Norm: 0.025674903765320778\n",
      "(1, 16)\n",
      "Epoch [51/100], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609918749414646e-07, Correlation: 0.9999999074552094, R2: 0.999999793900794\n",
      "Gradient Norm: 0.028666820377111435\n",
      "(1, 16)\n",
      "Epoch [52/100], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0608796091892145e-07, Correlation: 0.9999999074550361, R2: 0.9999997939120412\n",
      "Gradient Norm: 0.025674903765320778\n",
      "(1, 16)\n",
      "Epoch [53/100], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.060992159158559e-07, Correlation: 0.9999999074552094, R2: 0.9999997939007919\n",
      "Gradient Norm: 0.02879224717617035\n",
      "(1, 16)\n",
      "Epoch [54/100], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0608796091892145e-07, Correlation: 0.9999999074550325, R2: 0.9999997939120343\n",
      "Gradient Norm: 0.025631284341216087\n",
      "(1, 16)\n",
      "Epoch [55/100], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.060972548179052e-07, Correlation: 0.9999999074557161, R2: 0.9999997939027362\n",
      "Gradient Norm: 0.028480390086770058\n",
      "(1, 16)\n",
      "Epoch [56/100], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0608779038866487e-07, Correlation: 0.9999999074549508, R2: 0.9999997939122008\n",
      "Gradient Norm: 0.02470673993229866\n",
      "(1, 16)\n",
      "Epoch [57/100], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.060972548179052e-07, Correlation: 0.9999999074556478, R2: 0.9999997939027457\n",
      "Gradient Norm: 0.021905595436692238\n",
      "(1, 16)\n",
      "Epoch [58/100], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0608905515473452e-07, Correlation: 0.9999999074561596, R2: 0.9999997939109416\n",
      "Gradient Norm: 0.02068418264389038\n",
      "(1, 16)\n",
      "Epoch [59/100], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609834905371827e-07, Correlation: 0.999999907455055, R2: 0.9999997939016554\n",
      "Gradient Norm: 0.021721122786402702\n",
      "(1, 16)\n",
      "Epoch [60/100], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0608995043858158e-07, Correlation: 0.9999999074558474, R2: 0.999999793910062\n",
      "Gradient Norm: 0.020697681233286858\n",
      "(1, 16)\n",
      "Epoch [61/100], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.060988464336333e-07, Correlation: 0.9999999074548989, R2: 0.9999997939011674\n",
      "Gradient Norm: 0.021505050361156464\n",
      "(1, 16)\n",
      "Epoch [62/100], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.060899646494363e-07, Correlation: 0.9999999074558277, R2: 0.9999997939100354\n",
      "Gradient Norm: 0.02072773687541485\n",
      "(1, 16)\n",
      "Epoch [63/100], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609871853594086e-07, Correlation: 0.9999999074549341, R2: 0.9999997939012962\n",
      "Gradient Norm: 0.02163173072040081\n",
      "(1, 16)\n",
      "Epoch [64/100], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0608999307114573e-07, Correlation: 0.9999999074558129, R2: 0.9999997939100042\n",
      "Gradient Norm: 0.020697681233286858\n",
      "(1, 16)\n",
      "Epoch [65/100], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609888906619744e-07, Correlation: 0.9999999074548552, R2: 0.9999997939011029\n",
      "Gradient Norm: 0.02148328348994255\n",
      "(1, 16)\n",
      "Epoch [66/100], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609009254712873e-07, Correlation: 0.9999999074557524, R2: 0.9999997939098957\n",
      "Gradient Norm: 0.020697681233286858\n",
      "(1, 16)\n",
      "Epoch [67/100], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.060988464336333e-07, Correlation: 0.9999999074548905, R2: 0.9999997939011637\n",
      "Gradient Norm: 0.02154383435845375\n",
      "(1, 16)\n",
      "Epoch [68/100], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0608999307114573e-07, Correlation: 0.999999907455815, R2: 0.9999997939100054\n",
      "Gradient Norm: 0.02068418636918068\n",
      "(1, 16)\n",
      "Epoch [69/100], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.06098462740556e-07, Correlation: 0.9999999074550024, R2: 0.9999997939015646\n",
      "Gradient Norm: 0.02164187654852867\n",
      "(1, 16)\n",
      "Epoch [70/100], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0608999307114573e-07, Correlation: 0.9999999074558129, R2: 0.9999997939100042\n",
      "Gradient Norm: 0.020697681233286858\n",
      "(1, 16)\n",
      "Epoch [71/100], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609888906619744e-07, Correlation: 0.9999999074548552, R2: 0.9999997939011029\n",
      "Gradient Norm: 0.02148328348994255\n",
      "(1, 16)\n",
      "Epoch [72/100], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609009254712873e-07, Correlation: 0.9999999074557524, R2: 0.9999997939098957\n",
      "Gradient Norm: 0.020697681233286858\n",
      "(1, 16)\n",
      "Epoch [73/100], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.060988464336333e-07, Correlation: 0.9999999074548905, R2: 0.9999997939011637\n",
      "Gradient Norm: 0.021480068564414978\n",
      "(1, 16)\n",
      "Epoch [74/100], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0608999307114573e-07, Correlation: 0.9999999074558119, R2: 0.9999997939100066\n",
      "Gradient Norm: 0.02072773687541485\n",
      "(1, 16)\n",
      "Epoch [75/100], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609871853594086e-07, Correlation: 0.9999999074549341, R2: 0.9999997939012962\n",
      "Gradient Norm: 0.02163173072040081\n",
      "(1, 16)\n",
      "Epoch [76/100], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0608999307114573e-07, Correlation: 0.9999999074558129, R2: 0.9999997939100042\n",
      "Gradient Norm: 0.020697681233286858\n",
      "(1, 16)\n",
      "Epoch [77/100], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609888906619744e-07, Correlation: 0.9999999074548552, R2: 0.9999997939011029\n",
      "Gradient Norm: 0.02150031365454197\n",
      "(1, 16)\n",
      "Epoch [78/100], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.060901636014023e-07, Correlation: 0.9999999074557204, R2: 0.999999793909843\n",
      "Gradient Norm: 0.020697681233286858\n",
      "(1, 16)\n",
      "Epoch [79/100], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.060988464336333e-07, Correlation: 0.9999999074548989, R2: 0.9999997939011674\n",
      "Gradient Norm: 0.021505050361156464\n",
      "(1, 16)\n",
      "Epoch [80/100], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.060899646494363e-07, Correlation: 0.9999999074558277, R2: 0.9999997939100354\n",
      "Gradient Norm: 0.02072773687541485\n",
      "(1, 16)\n",
      "Epoch [81/100], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609871853594086e-07, Correlation: 0.9999999074549341, R2: 0.9999997939012962\n",
      "Gradient Norm: 0.02163173072040081\n",
      "(1, 16)\n",
      "Epoch [82/100], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0608999307114573e-07, Correlation: 0.9999999074558129, R2: 0.9999997939100042\n",
      "Gradient Norm: 0.020697681233286858\n",
      "(1, 16)\n",
      "Epoch [83/100], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609888906619744e-07, Correlation: 0.9999999074548552, R2: 0.9999997939011029\n",
      "Gradient Norm: 0.02148328348994255\n",
      "(1, 16)\n",
      "Epoch [84/100], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609009254712873e-07, Correlation: 0.9999999074557524, R2: 0.9999997939098957\n",
      "Gradient Norm: 0.020697681233286858\n",
      "(1, 16)\n",
      "Epoch [85/100], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.060988464336333e-07, Correlation: 0.9999999074548905, R2: 0.9999997939011637\n",
      "Gradient Norm: 0.02154383435845375\n",
      "(1, 16)\n",
      "Epoch [86/100], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0608999307114573e-07, Correlation: 0.999999907455815, R2: 0.9999997939100054\n",
      "Gradient Norm: 0.02068418636918068\n",
      "(1, 16)\n",
      "Epoch [87/100], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.06098462740556e-07, Correlation: 0.9999999074550024, R2: 0.9999997939015646\n",
      "Gradient Norm: 0.02164187654852867\n",
      "(1, 16)\n",
      "Epoch [88/100], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0608999307114573e-07, Correlation: 0.9999999074558129, R2: 0.9999997939100042\n",
      "Gradient Norm: 0.020697681233286858\n",
      "(1, 16)\n",
      "Epoch [89/100], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609888906619744e-07, Correlation: 0.9999999074548552, R2: 0.9999997939011029\n",
      "Gradient Norm: 0.02150031365454197\n",
      "(1, 16)\n",
      "Epoch [90/100], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.060901636014023e-07, Correlation: 0.9999999074557204, R2: 0.999999793909843\n",
      "Gradient Norm: 0.020697681233286858\n",
      "(1, 16)\n",
      "Epoch [91/100], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.060988464336333e-07, Correlation: 0.9999999074548989, R2: 0.9999997939011674\n",
      "Gradient Norm: 0.021480068564414978\n",
      "(1, 16)\n",
      "Epoch [92/100], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0608999307114573e-07, Correlation: 0.9999999074558119, R2: 0.9999997939100066\n",
      "Gradient Norm: 0.02072773687541485\n",
      "(1, 16)\n",
      "Epoch [93/100], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609871853594086e-07, Correlation: 0.9999999074549341, R2: 0.9999997939012962\n",
      "Gradient Norm: 0.02163173072040081\n",
      "(1, 16)\n",
      "Epoch [94/100], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0608999307114573e-07, Correlation: 0.9999999074558129, R2: 0.9999997939100042\n",
      "Gradient Norm: 0.020697681233286858\n",
      "(1, 16)\n",
      "Epoch [95/100], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609888906619744e-07, Correlation: 0.9999999074548552, R2: 0.9999997939011029\n",
      "Gradient Norm: 0.02148328348994255\n",
      "(1, 16)\n",
      "Epoch [96/100], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0609009254712873e-07, Correlation: 0.9999999074557524, R2: 0.9999997939098957\n",
      "Gradient Norm: 0.020697681233286858\n",
      "(1, 16)\n",
      "Epoch [97/100], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.060988464336333e-07, Correlation: 0.9999999074548905, R2: 0.9999997939011637\n",
      "Gradient Norm: 0.02154383435845375\n",
      "(1, 16)\n",
      "Epoch [98/100], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0608999307114573e-07, Correlation: 0.999999907455815, R2: 0.9999997939100054\n",
      "Gradient Norm: 0.02068418636918068\n",
      "(1, 16)\n",
      "Epoch [99/100], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.06098462740556e-07, Correlation: 0.9999999074550024, R2: 0.9999997939015646\n",
      "Gradient Norm: 0.02164187654852867\n",
      "(1, 16)\n",
      "Epoch [100/100], Loss: 50.0530, Gap to Optimality: 50.0530, NMSE: 2.0608999307114573e-07, Correlation: 0.9999999074558129, R2: 0.9999997939100042\n",
      "Final gradient of the subproblem Core : 0.02164187654852867\n",
      "(32, 32)\n",
      "---------------------------Testing with Best Lambda------------------------------\n",
      "Y Test Predicted:  [ 67.5485183   64.19090772 100.90456149 106.79195412  66.53165846\n",
      "  96.79571485 112.7608271   87.08073957  58.88887582  60.74356617\n",
      "  65.75628926  64.15954789  82.80079778  68.56555582 142.45236776\n",
      "  98.01732707  98.35002873  55.4203634   96.7461582   87.26501788\n",
      "  47.98424232  91.13714142  73.5680945   77.46542313  46.96969902\n",
      "  79.3805076   84.71070525  45.49777552  87.5667141   73.48045124\n",
      "  86.90829616  88.26633235 124.64858639  81.42774739  87.54720796\n",
      "  82.79557793  98.73351246  69.04116214  80.63626555  93.08204227\n",
      "  68.41206093  87.28020333  62.04370965  57.34458387  56.02427947\n",
      "  89.30909232 110.53952044 134.60484135  43.27187128  74.48265786\n",
      "  96.23124149  87.89203317  79.23336289  90.72654406 125.88036693\n",
      " 160.36058417  60.51482913  86.8887881   89.57341918  85.49772833\n",
      "  79.97210141  95.5416492   57.10210916  50.37365988  70.86327429\n",
      " 131.22864329  94.50974109  83.16108662  62.92848467  84.10486884\n",
      "  76.25991096  96.47914788  77.08528911  65.79279304  71.78157768\n",
      "  65.15276637  61.02853527  68.75485819  66.41329123 104.95440581\n",
      "  79.74195842  91.80725871 110.44403929  62.24932902  57.6179071\n",
      "  98.12796865  67.83777337  72.12938175 127.8463211   63.31176965\n",
      " 103.69507751 150.07057373  67.42141914  92.38850223 123.6742247\n",
      "  46.91580634  64.66215729 108.21675869 101.13736336 113.37588333]\n",
      "Y Test Actual:  [ 67.54932471  64.19367989 100.89907103 106.80033802  66.5296898\n",
      "  96.80085769 112.75716496  87.08379176  58.86645104  60.75871896\n",
      "  65.76651181  64.14603363  82.80477245  68.55912818 142.45923938\n",
      "  98.00053671  98.36489423  55.40988723  96.73410604  87.25275894\n",
      "  47.97392502  91.14302569  73.57729565  77.46144622  46.97758628\n",
      "  79.38435102  84.7040345   45.4879342   87.55672081  73.49886432\n",
      "  86.90598491  88.25175908 124.63253198  81.43265772  87.55287141\n",
      "  82.8080788   98.72768177  69.0468243   80.62856336  93.08545506\n",
      "  68.40807412  87.26847367  62.04717769  57.33990441  56.01334774\n",
      "  89.31934273 110.55219975 134.61307846  43.26935217  74.47617341\n",
      "  96.21843559  87.89937403  79.21862917  90.71786386 125.8824493\n",
      " 160.38439066  60.50387741  86.87508783  89.5739495   85.50880057\n",
      "  79.97185552  95.54965657  57.09355445  50.35928763  70.86185745\n",
      " 131.24078613  94.50837386  83.15863626  62.93422433  84.09831139\n",
      "  76.26182003  96.48126457  77.07300387  65.77838739  71.76861932\n",
      "  65.14937109  61.0440168   68.74400314  66.41967439 104.95340089\n",
      "  79.75508697  91.80406117 110.44770514  62.24491669  57.60974417\n",
      "  98.13365195  67.82489044  72.14320497 127.85744229  63.30804812\n",
      " 103.70015697 150.07489827  67.42743244  92.3906977  123.66379363\n",
      "  46.92967844  64.66629945 108.22935093 101.14612067 113.38031778]\n",
      "Error Report on Testing _ With best Lambda\n",
      "SGD_Alpha chosen for model:  50\n",
      "SGD_Test Normalized Estimation Error:  3.648692195148474e-08\n",
      "SGD_Test NMSE Loss:  1.1992978192131861e-08\n",
      "SGD_Test R2 Loss:  0.9999998337247104\n",
      "SGD_Test Correlation:  0.9999999230645\n",
      "Objective Function Values 22199.18939784684\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final gradient: 0.02164187654852867\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learning_rates = [0.0000015]\n",
    " \n",
    "\n",
    "for idx,lr in enumerate (learning_rates):   \n",
    "      \n",
    "      print('')\n",
    "      print('-------------------------------------Learning Rate',lr,'-----------------------------------------')\n",
    "      lsr_tensor_SGD_2 = copy.deepcopy(factor_core_iterate_SGD[-1])\n",
    "      learning_rate = lr\n",
    "      epochs = 100\n",
    "      batch_size = 650\n",
    "\n",
    "      momentum = 0\n",
    "      nesterov = False\n",
    "      decay_factor = 0\n",
    "      hypers = {'max_iter': 1, 'threshold': 1e-4, 'ranks': tuple(tensor_mode_ranks), 'separation_rank': separation_rank,'learning_rate':learning_rate,'epochs':epochs,'batch_size': batch_size, 'momentum':momentum, 'nesterov': nesterov, 'decay_factor': decay_factor}\n",
    "\n",
    "      normalized_estimation_error_SGD_2, test_nmse_loss_SGD_2, test_R2_loss_SGD_2, test_correlation_SGD_2, objective_function_values_SGD_2,gradient_values_SGD_2,iterate_differences_SGD_2,epoch_level_gradients_SGD_2,epoch_level_function_2,tensor_iteration_SGD_2,factor_core_iterate_SGD_2 = train_test_sgd(X_train, Y_train, X_test, Y_test, lambda1, hypers, Y_train_mean,lsr_tensor_SGD_2,B_tensored,intercept= False, Initializer = lsr_tensor_SGD_2.core_tensor)\n",
    "\n",
    "    \n",
    "      #Get current time and store in variable\n",
    "      formatted_time = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "      max_iter = hypers['max_iter']\n",
    "      pkl_file = f\"/Users/lakrama/Neuro Project Codes/LSR-Tensor-Ridge-Regression_All_Data/Platforms for Experments/Data/Experiments_Isolating_Sub_Problems/Final lr search/After Indentifinig Batch Size Issue/Gradient Descent/SGD_Level_2_SGD_Full_Batch_learning_rate_{learning_rate}_decay_{decay_factor}_intercept5_,ExecutionTime{formatted_time}, n_train_{n_train},n_test_{n_test}, tensor_dimensions:{tensor_dimensions}, tensor_mode_= ranks:{tensor_mode_ranks}, separation_rank:{separation_rank}, max_iter={max_iter}.pkl\"\n",
    "\n",
    "      with open(pkl_file, \"wb\") as file:\n",
    "        dill.dump((lsr_tensor_SGD_2,lambda1, normalized_estimation_error_SGD_2, test_nmse_loss_SGD_2, test_R2_loss_SGD_2, test_correlation_SGD_2, objective_function_values_SGD_2,gradient_values_SGD_2,iterate_differences_SGD_2,epoch_level_gradients_SGD_2,epoch_level_function_2,tensor_iteration_SGD_2,factor_core_iterate_SGD_2), file)\n",
    "\n",
    "\n",
    "      print(\"Error Report on Testing _ With best Lambda\")\n",
    "      print(\"SGD_Alpha chosen for model: \", lambda1)\n",
    "      print(\"SGD_Test Normalized Estimation Error: \", normalized_estimation_error_SGD_2)\n",
    "      print(\"SGD_Test NMSE Loss: \", test_nmse_loss_SGD_2)\n",
    "      print(\"SGD_Test R2 Loss: \", test_R2_loss_SGD_2)\n",
    "      print(\"SGD_Test Correlation: \", test_correlation_SGD_2)\n",
    "      print(\"Objective Function Values\", objective_function_values_SGD_2[0,1,2])\n",
    "\n",
    "      # Looking at the  variation of function values within a BCD iteration\n",
    "\n",
    "      import matplotlib.pyplot as plt\n",
    "\n",
    "      # Create a line plot\n",
    "      plt.plot(epoch_level_gradients_SGD_2[0,1,2,:], marker='o')\n",
    "\n",
    "      # Add title and labels\n",
    "      plt.title('Gradient Value')\n",
    "      plt.xlabel('X-axis')\n",
    "      plt.yscale('log')\n",
    "      plt.ylabel('Y-axis')\n",
    "\n",
    "      plt.show()\n",
    "\n",
    "      print(f'final gradient: {epoch_level_gradients_SGD_2[0,1,2,-1]}')\n",
    "      # fucnion value\n",
    "\n",
    "      # Create a line plot\n",
    "      plt.plot(epoch_level_function_2[0,1,2,:], marker='o')\n",
    "\n",
    "      # Add title and labels\n",
    "      plt.title('Function Value')\n",
    "      plt.xlabel('X-axis')\n",
    "      plt.yscale('log') \n",
    "      plt.ylabel('Y-axis')\n",
    "\n",
    "      plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#gradient of core tensor\n",
    "gradient_set_1 = epoch_level_gradients_SGD[0,1,-1]\n",
    "gradient_set_2 = epoch_level_gradients_SGD_2[0,1,-1]\n",
    "\n",
    "#concatanating the gradients \n",
    "concatanated_gradients = np.concatenate((gradient_set_1,gradient_set_2))\n",
    "\n",
    "# Plotting the concatenated array\n",
    "plt.plot(concatanated_gradients, marker = 'o')\n",
    "plt.title('Variation of Core Tensor Sub Problem Gradients')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Gradient')\n",
    "plt.yscale('log')\n",
    "plt.show()\n",
    "\n",
    "#gradient of core tensor\n",
    "function_set_1 = epoch_level_function[0,1,-1]\n",
    "function_set_2 = epoch_level_function_2[0,1,-1]\n",
    "\n",
    "#concatanating the gradients \n",
    "concatanated_gradients = np.concatenate((function_set_1,function_set_2))\n",
    "\n",
    "# Plotting the concatenated array\n",
    "plt.plot(concatanated_gradients, marker = 'o')\n",
    "plt.title('Variation of Core Tensor Sub Problem Function Value')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Gradient')\n",
    "plt.yscale('log')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to change the batch size of the core tensor sub problem from 64 - batch_size"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchtensor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
