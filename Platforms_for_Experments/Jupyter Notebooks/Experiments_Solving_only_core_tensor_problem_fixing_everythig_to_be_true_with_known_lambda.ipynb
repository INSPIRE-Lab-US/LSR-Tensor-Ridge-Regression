{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Needed Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "###system \n",
    "import sys\n",
    "import platform\n",
    "\n",
    "\n",
    "###loading and saving data\n",
    "import pickle\n",
    "import dill\n",
    "import datetime\n",
    "import time \n",
    "\n",
    "\n",
    "###bases\n",
    "import numpy as np\n",
    "\n",
    "import copy\n",
    "\n",
    "#preprocessing \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading all the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 32, 32)\n",
      "(5000,)\n",
      "(1000, 32, 32)\n",
      "(1000,)\n",
      "(32, 32)\n",
      "(4, 4)\n",
      "(6000,)\n",
      "factor matrix 1_1:(32, 4)\n",
      "factor matrix 1_1:(32, 4)\n",
      "factor matrix 1_1:(32, 4)\n",
      "factor matrix 1_1:(32, 4)\n"
     ]
    }
   ],
   "source": [
    "file= open(rf\"D:\\Tensor Based ML for Neuro Imaging\\INSPIRE_CAHBHIR\\Python Scripts\\LSR-Tensor-Ridge-Regression\\Data_Sets\\Synthetic Data\\Uncentered X\\Bounded_Var_Time2024-04-15 193540, intercept5,n_train5000, n_test1000, tensor_dimensions[32 32], tensor_mode_ranks[4 4], separation_rank2.pkl\", 'rb')\n",
    "data = pickle.load(file)\n",
    "file.close()\n",
    "\n",
    "X_train_Full = data[0]\n",
    "print(data[0].shape)\n",
    "\n",
    "Y_train_Full = data[1]\n",
    "print(data[1].shape)\n",
    "\n",
    "X_test_Full = data[2]\n",
    "print(data[2].shape)\n",
    "\n",
    "Y_test_Full = data[3]\n",
    "print(data[3].shape)\n",
    "\n",
    "B_tensored = data[4]\n",
    "print(data[4].shape)\n",
    "\n",
    "G1 = data[5]\n",
    "print(data[5].shape)\n",
    "\n",
    "all_factormatrices = data[6]\n",
    "#print(data[6].shape)\n",
    "\n",
    "Y_train_nonoise = data[7]\n",
    "print(data[7].shape)\n",
    "\n",
    "#all the factor matrices \n",
    "B_1_1 = all_factormatrices[0][0]\n",
    "print(f'factor matrix 1_1:{B_1_1.shape}')\n",
    "B_1_2 = all_factormatrices[0][1]\n",
    "print(f'factor matrix 1_1:{B_1_2.shape}')\n",
    "B_2_1 = all_factormatrices[1][0]\n",
    "print(f'factor matrix 1_1:{B_2_1.shape}')\n",
    "B_2_2 = all_factormatrices[0][0]\n",
    "print(f'factor matrix 1_1:{B_2_2.shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All Needed Scripts "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LSR Tensor Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Low Separation Rank Tensor Decomposition\n",
    "class LSR_tensor_dot():\n",
    "    # Constructor\n",
    "    def __init__(self, shape, ranks, separation_rank, dtype = np.float32, intercept = False ,initialize = True):\n",
    "        super(LSR_tensor_dot, self).__init__()\n",
    "        self.shape = shape\n",
    "        self.ranks = ranks\n",
    "        self.separation_rank = separation_rank\n",
    "        self.dtype = dtype\n",
    "        self.order = len(shape)\n",
    "        self.init_params(initialize)\n",
    "        self.init_params(intercept)\n",
    "\n",
    "    # Initialize Parameters\n",
    "    def init_params(self, intercept = False ,initialize = True):\n",
    "        # Initialize core tensor as independent standard gaussians\n",
    "        if not initialize:\n",
    "            self.core_tensor = np.zeros(shape = self.ranks)\n",
    "        else:\n",
    "            self.core_tensor = np.random.normal(size = self.ranks)\n",
    "\n",
    "        # Set up Factor Matrices\n",
    "        self.factor_matrices = []\n",
    "\n",
    "        # Initialize all factor matrices\n",
    "        for s in range(self.separation_rank):\n",
    "            factors_s = []\n",
    "            for k in range(self.order):\n",
    "                if not initialize:\n",
    "                    factor_matrix_B = np.eye(self.shape[k])[:, self.ranks[k]]\n",
    "                    factors_s.append(factor_matrix_B)\n",
    "                else:\n",
    "                    factor_matrix_A = np.random.normal(0,1,size= (self.shape[k], self.ranks[k]))\n",
    "                    factors_s.append(factor_matrix_A)\n",
    "\n",
    "            self.factor_matrices.append(factors_s)\n",
    "\n",
    "        if intercept:\n",
    "          ('intercept is initialized')\n",
    "          self.b = np. random.normal(0,1)\n",
    "        else:\n",
    "          (print('intercept is not initialized'))\n",
    "          self.b = 0\n",
    "\n",
    "    # Expand core tensor and factor matrices to full tensor, optionally excluding\n",
    "    # a given term from the separation rank decomposition\n",
    "    def expand_to_tensor(self, skip_term = None):\n",
    "        full_lsr_tensor = np.zeros(shape = self.shape)\n",
    "\n",
    "        #Calculate Expanded Tensor\n",
    "        for s, term_s_factors in enumerate(self.factor_matrices):\n",
    "            if s == skip_term: continue\n",
    "            expanded_tucker_term = term_s_factors[0] @ self.core_tensor @ term_s_factors[1].T\n",
    "            full_lsr_tensor += expanded_tucker_term\n",
    "\n",
    "        #Column Wise Flatten full_lsr_tensor\n",
    "        full_lsr_tensor = full_lsr_tensor.flatten(order = 'F')\n",
    "        return full_lsr_tensor\n",
    "\n",
    "    # Absorb all factor matrices and core tensor into the input tensor except for matrix s, k\n",
    "    # Used during a factor matrix update step of block coordiante descent\n",
    "    def bcd_factor_update_x_y(self, s, k, x, y):\n",
    "        #Take x and swap axes 1 and 2 so that vectorization occurs \"COLUMN WISE\"\n",
    "        x_transpose = np.transpose(x, (0, 2, 1))\n",
    "        x_transpose_vectorized = np.reshape(x_transpose, newshape = (x_transpose.shape[0], -1))\n",
    "\n",
    "        #if we are unfolding along mode 1, use x. Else, if we are unfolding along mode, use x_transpose\n",
    "        x_partial_unfold = x if k == 0 else x_transpose\n",
    "\n",
    "        #If k = 0(skip first factor matrix), we have 2nd factor matrix. If k= 1(skip second factor matrix), we have first factor matrix\n",
    "        kronecker_term = self.factor_matrices[s][1] if k == 0 else self.factor_matrices[s][0]\n",
    "\n",
    "        #if k = 0, G^T. Else if k = 1, put G\n",
    "        core_tensor_term = self.core_tensor.T if k == 0 else self.core_tensor\n",
    "\n",
    "        omega = x_partial_unfold @ kronecker_term @ core_tensor_term\n",
    "        omega = np.transpose(omega, (0, 2, 1))\n",
    "        omega = np.reshape(omega, newshape = (omega.shape[0], -1))\n",
    "\n",
    "        X_tilde = omega\n",
    "        y_tilde = y\n",
    "\n",
    "        if self.separation_rank == 1:\n",
    "            pass\n",
    "        else:\n",
    "            gamma = np.dot(x_transpose_vectorized,self.expand_to_tensor(skip_term = s))\n",
    "            #gamma = gamma.reshape(-1,1)\n",
    "            y_tilde = y - gamma\n",
    "\n",
    "        return X_tilde, y_tilde\n",
    "\n",
    "    # Absorb all factor matrices the input tensor (not the core tensor)\n",
    "    # Used during a core tensor update step of block coordiante descent\n",
    "    def bcd_core_update_x_y(self, x, y):\n",
    "        #Take x and swap axes 1 and 2 so that vectorization occurs \"COLUMN WISE\"\n",
    "        x_transpose = np.transpose(x, (0, 2, 1))\n",
    "        x_transpose_vectorized = np.reshape(x_transpose, newshape = (x_transpose.shape[0], -1))\n",
    "\n",
    "        #Calculate y_tilde\n",
    "        y_tilde = y\n",
    "\n",
    "        #Calculate Kronecker Factor Sum\n",
    "        kron_factor_sum = 0\n",
    "        for term_s_factors in self.factor_matrices:\n",
    "            kron_factor_sum += np.kron(term_s_factors[1], term_s_factors[0])\n",
    "\n",
    "        #Return Core Update\n",
    "        return (kron_factor_sum.T @ x_transpose_vectorized.T).T, y_tilde\n",
    "\n",
    "\n",
    "    #Retrieve factor matrix\n",
    "    def get_factor_matrix(self, s, k):\n",
    "      return self.factor_matrices[s][k]\n",
    "\n",
    "    #Update factor matrix\n",
    "    def update_factor_matrix(self, s, k, updated_factor_matrix: np.ndarray):\n",
    "      self.factor_matrices[s][k] = updated_factor_matrix\n",
    "\n",
    "    def update_intercept(self,updated_b):\n",
    "      self.b = updated_b\n",
    "\n",
    "    #Retrieve Core Matrix\n",
    "    def get_core_matrix(self):\n",
    "      return self.core_tensor\n",
    "\n",
    "    #Update core matrix\n",
    "    def update_core_matrix(self, updated_core_matrix: np.ndarray):\n",
    "      self.core_tensor = updated_core_matrix\n",
    "\n",
    "    #Retrive intercept\n",
    "    def get_intercept(self):\n",
    "      return self.b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BCD Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lsr_ten: LSR Tensor\n",
    "#training_data: X\n",
    "#training_labels: Y\n",
    "#hypers: hyperparameters\n",
    "def lsr_bcd_regression(lsr_ten, training_data: np.ndarray, training_labels: np.ndarray, hypers: dict,intercept = False, Initializer = None):\n",
    "    #Get LSR Tensor Information and other hyperparameters\n",
    "    shape, ranks, sep_rank, order = lsr_ten.shape, lsr_ten.ranks, lsr_ten.separation_rank, lsr_ten.order\n",
    "    lambda1 = hypers[\"weight_decay\"]\n",
    "    max_iter = hypers[\"max_iter\"]\n",
    "    threshold = hypers[\"threshold\"]\n",
    "    lr        = hypers[\"learning_rate\"]\n",
    "    epochs    = hypers[\"epochs\"]\n",
    "    batch_size = hypers[\"batch_size\"]\n",
    "    decay_factor = hypers[\"decay_factor\"]\n",
    "    b_intercept = intercept\n",
    "\n",
    "    #Create models for each factor matrix and core matrix\n",
    "    #factor_matrix_models = [[Ridge(alpha = lambda1, solver = 'svd', fit_intercept = intercept) for k in range(len(ranks))] for s in range(sep_rank)]\n",
    "    #core_tensor_model = Ridge(alpha = lambda1, solver = 'svd', fit_intercept = intercept)\n",
    "\n",
    "    #Store objective function values\n",
    "    objective_function_values = np.ones(shape = (max_iter, sep_rank, len(ranks) + 1)) * np.inf\n",
    "\n",
    "    X, y = training_data, training_labels\n",
    "    if intercept: b_start = lsr_ten.get_intercept()\n",
    "    expanded_lsr_start  = lsr_ten.expand_to_tensor()\n",
    "    expanded_lsr_start  = np.reshape(expanded_lsr_start, X[0].shape, order = 'F')\n",
    "    objective_function_value_star = objective_function_tensor_sep(y, X, expanded_lsr_start,lsr_ten, lambda1, b if intercept else None)\n",
    "    print('Objective Function Value:',objective_function_value_star)\n",
    "\n",
    "    #Normalized Estimation Error\n",
    "    iterations_normalized_estimation_error = np.zeros(shape = (max_iter,))\n",
    "    \n",
    "    #Gradient Values\n",
    "    gradient_values = np.ones(shape = (max_iter, sep_rank, len(ranks) + 1)) * np.inf\n",
    "\n",
    "    #Epoch Level Function Values \n",
    "    epoch_level_function_values = np.ones(shape = (max_iter,sep_rank,len(ranks)+1,epochs))*np.inf\n",
    "\n",
    "    #Epoch Level Gradients\n",
    "    epoch_gradient_values = np.ones(shape = (max_iter,sep_rank,len(ranks)+1,epochs))*np.inf\n",
    "\n",
    "    #saving the tensor\n",
    "    tensor_iteration = []\n",
    "    #saving iterate-wise data\n",
    "    factor_core_iterates = []\n",
    "\n",
    "    #iterate differences \n",
    "    iterate_difference = np.ones(shape = (max_iter, sep_rank, len(ranks) + 1)) * np.inf\n",
    "\n",
    "    #Run at most max_iter iterations of Block Coordinate Descent\n",
    "    for iteration in range(max_iter):\n",
    "        #print('')\n",
    "        print('--------------------------------------------------------------BCD iteration',iteration,'--------------------------------------------------------------')\n",
    "        factor_residuals = np.zeros(shape = (sep_rank, len(ranks)))\n",
    "        core_residual = 0\n",
    "\n",
    "        #Store updates to factor matrices and core tensor\n",
    "        updated_factor_matrices = np.empty((sep_rank, len(ranks)), dtype=object)\n",
    "        updated_core_tensor = None\n",
    "\n",
    "        #Iterate over the Factor Matrices.\n",
    "        for s in range(sep_rank):\n",
    "            for k in range(len(ranks)):\n",
    "                \n",
    "                #Absorb Factor Matrices into X aside from (s, k) to get X_tilde\n",
    "                print('---------------------------------------------Sep',s,'Factor',k,'-------------------------------------------------')\n",
    "                X, y = training_data, training_labels\n",
    "                X_tilde, y_tilde = lsr_ten.bcd_factor_update_x_y(s, k, X, y) #y tilde should now be y-b-<Q,X>\n",
    "                \n",
    "\n",
    "                #Solve the sub-problem pertaining to the factor tensor\n",
    "                hypers = {'lambda': lambda1, 'lr': lr, 'epochs': epochs, 'batch_size': batch_size, 'bias': b_intercept, 'decay_factor': decay_factor}\n",
    "                #weights, bias, loss_values, gap_to_optimality, nmse_values, corr_values, R2_values,sub_problem_gradient = SGD(X_tilde, y_tilde.reshape(-1,1), cost_function_code = 1, hypers = hypers , optimizer_code = 0, p_star = 0)\n",
    "                \n",
    "                sub_problem_gradient = 0\n",
    "                loss_values = 0\n",
    "                bias = 0\n",
    "\n",
    "\n",
    "                #printing the subproblem gradients\n",
    "                #print(f\"Final gradient of the subproblem {s,k} : {sub_problem_gradient[-1]}\")\n",
    "                epoch_gradient_values[iteration,s,k,:] = sub_problem_gradient\n",
    "                epoch_level_function_values[iteration,s,k,:] = loss_values\n",
    "\n",
    "                #Retrieve Original and Updated Factor Matrices\n",
    "                #Bk = lsr_ten.get_factor_matrix(s, k)\n",
    "                #Bk1 = weights\n",
    "                if intercept: b = bias\n",
    "\n",
    "                #Shape Bk1 as needed\n",
    "                #Bk1 = np.reshape(Bk1, (shape[k], ranks[k]), order = 'F') #if there is an error check here\n",
    "\n",
    "                #fixing the factor matrices to be the true value \n",
    "                Bk  = all_factormatrices[s][k]\n",
    "                Bk1 = all_factormatrices[s][k]\n",
    "\n",
    "\n",
    "                #Update Residuals and store updated factor matrix\n",
    "                factor_residuals[s][k] = np.linalg.norm(Bk1 - Bk)\n",
    "                updated_factor_matrices[s, k] = Bk1\n",
    "\n",
    "                iterate_difference[iteration,s,k] = factor_residuals[s][k]\n",
    "\n",
    "\n",
    "                #Update Factor Matrix\n",
    "                lsr_ten.update_factor_matrix(s, k, updated_factor_matrices[s, k])\n",
    "\n",
    "                #update the intercept\n",
    "                if intercept: lsr_ten.update_intercept(b)\n",
    "\n",
    "                #Calculate Objective Function Value\n",
    "                expanded_lsr = lsr_ten.expand_to_tensor()\n",
    "                expanded_lsr = np.reshape(expanded_lsr, X[0].shape, order = 'F')\n",
    "                objective_function_value = objective_function_tensor_sep(y, X, expanded_lsr,lsr_ten,lambda1)\n",
    "                objective_function_values[iteration, s, k] = objective_function_value\n",
    "\n",
    "                #Print Objective Function Values\n",
    "                print(f\"Iteration: {iteration}, Separation Rank: {s}, Factor Matrix: {k}, Objective Function Value: {objective_function_value}\")\n",
    "                \n",
    "                #Calculate Gradient Values\n",
    "                bk = np.reshape(Bk, (-1, 1), order = 'F') #Flatten Factor Matrix Column Wise\n",
    "                Omega = X_tilde\n",
    "                z = bias\n",
    "                gradient_value = (-2 * Omega.T) @ (y_tilde.reshape(-1,1) - Omega @ bk  - z) + (2 * lambda1 * bk)\n",
    "                \n",
    "                #Store Gradient Values\n",
    "                gradient_values[iteration, s, k] = np.linalg.norm(gradient_value, ord = 'fro')\n",
    "\n",
    "\n",
    "        #Absorb necessary matrices into X, aside from core tensor, to get X_tilde\n",
    "        print('---------------------------------------------Core-------------------------------------------------')\n",
    "        X, y = training_data, training_labels\n",
    "        X_tilde, y_tilde = lsr_ten.bcd_core_update_x_y(X, y)\n",
    "\n",
    "        #Solve the sub-problem pertaining to the core tensor\n",
    "        hypers = {'lambda': lambda1, 'lr': lr, 'epochs': epochs, 'batch_size': batch_size, 'bias': intercept, 'decay_factor':decay_factor}\n",
    "        weights, bias, loss_values, gap_to_optimality, nmse_values, corr_values, R2_values,sub_problem_gradient,loss_values = SGD(X_tilde, y_tilde.reshape(-1,1), cost_function_code = 1, hypers = hypers, optimizer_code = 0, p_star = 0, Initializer = Initializer)\n",
    "\n",
    "        print(f\"Final gradient of the subproblem Core : {sub_problem_gradient[-1]}\")\n",
    "        epoch_gradient_values[iteration,:,len(ranks),:] = sub_problem_gradient\n",
    "        epoch_level_function_values[iteration,:,len(ranks),:] = loss_values\n",
    "\n",
    "        #Get Original and Updated Core Tensor\n",
    "        Gk = lsr_ten.get_core_matrix()\n",
    "        Gk1 = np.reshape(weights, ranks, order = 'F')\n",
    "        b = bias\n",
    "\n",
    "        #Update Residuals and store updated Core Tensor\n",
    "        core_residual = np.linalg.norm(Gk1 - Gk)\n",
    "        updated_core_tensor = Gk1\n",
    "\n",
    "        #saving iterate differece in a list\n",
    "        iterate_difference[iteration,:,len(ranks)] = core_residual\n",
    "\n",
    "        \n",
    "        #Update Core Tensor\n",
    "        lsr_ten.update_core_matrix(updated_core_tensor)\n",
    "\n",
    "\n",
    "        #Update Intercept\n",
    "\n",
    "        if intercept: lsr_ten.update_intercept(b)\n",
    "\n",
    "        #Calculate Objective Function Value\n",
    "        if intercept: b = lsr_ten.get_intercept()\n",
    "        expanded_lsr = lsr_ten.expand_to_tensor()\n",
    "        expanded_lsr = np.reshape(expanded_lsr, X[0].shape, order = 'F')\n",
    "\n",
    "        #saving the lsr tensor \n",
    "        tensor_iteration.append(expanded_lsr)\n",
    "\n",
    "        objective_function_value = objective_function_tensor_sep(y, X, expanded_lsr,lsr_ten, lambda1, b if intercept else None)\n",
    "        objective_function_values[iteration, :, (len(ranks))] = objective_function_value\n",
    "        \n",
    "        #print('')\n",
    "        #Print Objective Function Value\n",
    "        # print(f\"BCD Regression, Iteration: {iteration}, Core Tensor, Objective Function Value: {objective_function_value}\")\n",
    "        \n",
    "        #Calculate Gradient Values\n",
    "        g = np.reshape(Gk1, (-1, 1), order = 'F') #Flatten Core Matrix Column Wise\n",
    "        Omega = X_tilde\n",
    "        z = bias\n",
    "        gradient_value = (-2 * Omega.T) @ (y_tilde.reshape(-1,1) - Omega @ g  - z) + (2 * lambda1 * g)\n",
    "        \n",
    "        #Store Gradient Value\n",
    "        gradient_values[iteration, :, (len(ranks))] = np.linalg.norm(gradient_value, ord='fro')\n",
    "\n",
    "        #storing lsr_ten\n",
    "        factor_core_iterates.append(copy.deepcopy(lsr_ten))\n",
    "\n",
    "        #Stopping Criteria\n",
    "        diff = np.sum(factor_residuals.flatten()) + core_residual  #need to change this\n",
    "        # print('------------------------------------------------------------------------------------------')\n",
    "        # print(f\"Value of Stopping Criteria: {diff}\")\n",
    "        # print(f\"Expanded Tensor: {expanded_lsr}\")\n",
    "        # print('------------------------------------------------------------------------------------------')\n",
    "        if diff < threshold: \n",
    "            print(f'The threshold activated{diff}')\n",
    "            break\n",
    "\n",
    "\n",
    "    return lsr_ten, objective_function_values, gradient_values,iterate_difference,epoch_gradient_values,epoch_level_function_values,tensor_iteration,factor_core_iterates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Contains All Helper Functions for Optimization\n",
    "import numpy as np\n",
    "\n",
    "#Calculate value of objective function(vectorized case)\n",
    "def objective_function_vectorized(y: np.ndarray, X: np.ndarray, w: np.ndarray, alpha, b = None):\n",
    "    I = (X @ w).flatten()\n",
    "    y = y.flatten()\n",
    "    w = w.flatten()\n",
    "\n",
    "    if b is not None:\n",
    "      b = b.flatten()\n",
    "      function = (np.linalg.norm(y - I - b) ** 2) + (alpha * (np.linalg.norm(w) ** 2))\n",
    "    else:\n",
    "      function = (np.linalg.norm(y - I) ** 2) + (alpha * (np.linalg.norm(w) ** 2))\n",
    "    return function\n",
    "\n",
    "#Calculate value of objective function(tensor case)\n",
    "def objective_function_tensor(y: np.ndarray, X: np.ndarray, B: np.ndarray, alpha,b = None):\n",
    "    I = inner_product(X, B).flatten()\n",
    "    y = y.flatten()\n",
    "    B = B.flatten()\n",
    "    if b is not None:\n",
    "      b = b.flatten()\n",
    "      function = (np.linalg.norm(y - I -b) ** 2) + (alpha * (np.linalg.norm(B) ** 2))\n",
    "    else:\n",
    "      function = (np.linalg.norm(y - I) ** 2) + (alpha * (np.linalg.norm(B) ** 2))\n",
    "    return function\n",
    "\n",
    "def objective_function_tensor_sep(y: np.ndarray, X: np.ndarray, B: np.ndarray,lsr_ten, alpha,b = None):\n",
    "    I = inner_product(X, B).flatten()\n",
    "    y = y.flatten()\n",
    "    B = B.flatten()\n",
    "    regularizer = 0\n",
    "\n",
    "    #developing the separable regularizing term\n",
    "   \n",
    "    separation = len(lsr_ten.factor_matrices) \n",
    "    tucker = len(lsr_ten.factor_matrices[0])\n",
    "    \n",
    "    for s in range(separation):\n",
    "       for k in range(tucker):\n",
    "          regularizer += (np.linalg.norm(lsr_ten.factor_matrices[s][k])**2)\n",
    "    regularizer = regularizer + (np.linalg.norm(lsr_ten.core_tensor)**2)\n",
    "    if b is not None:\n",
    "      b = b.flatten()\n",
    "      function = (np.linalg.norm(y - I -b) ** 2) + (alpha * regularizer)\n",
    "    else:\n",
    "      function = (np.linalg.norm(y - I) ** 2) + (alpha * regularizer)\n",
    "    return function\n",
    "\n",
    "#Calculate x* and p* for Objective Function(Tensor Case)\n",
    "#X_train is a Tensor of samples x m x n\n",
    "#Y_train is a normal vector of size samples x 1. It can also be a flattened array of size (samples, )\n",
    "def calculate_optimal_iterate_and_function_value(X_train: np.ndarray, Y_train: np.ndarray, lambda1):\n",
    "    X_train = X_train.reshape((X_train.shape[0], -1))\n",
    "    Y_train = Y_train.reshape((-1, 1))\n",
    "\n",
    "    #Calculate Optimal Weight Tensor and Optimal Objective Function Value\n",
    "    B_optimal = np.linalg.inv(X_train.T @ X_train + lambda1 * np.eye(X_train.shape[1])) @ X_train.T @ Y_train\n",
    "    I = X_train @ B_optimal\n",
    "    p_star = (np.linalg.norm(Y_train - I) ** 2) + (lambda1 * (np.linalg.norm(B_optimal) ** 2))\n",
    "\n",
    "    return B_optimal, p_star\n",
    "\n",
    "\n",
    "#Inner product of two tensors\n",
    "#tensor1: samples x m x n\n",
    "#tensor2: m x n\n",
    "def inner_product(tensor1: np.ndarray, tensor2: np.ndarray):\n",
    "    tensor1 = tensor1.reshape(tensor1.shape[0], -1)\n",
    "    tensor2 = tensor2.reshape(-1, 1)\n",
    "    return tensor1 @ tensor2\n",
    "\n",
    "#Calculate R2 Score\n",
    "def R2(y_true, y_pred):\n",
    "    #Flatten for insurance\n",
    "    y_true = y_true.flatten()\n",
    "    y_pred = y_pred.flatten()\n",
    "\n",
    "    #Calculate R2 Score\n",
    "    y_true_mean = np.mean(y_true)\n",
    "    tss = np.sum((y_true - y_true_mean) ** 2)\n",
    "    rss = np.sum((y_true - y_pred) ** 2)\n",
    "    r2 = 1 - (rss / tss)\n",
    "    return r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import Optimization Toolkits\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import r2_score\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "\n",
    "#Cost Function[Least Squares]\n",
    "#||(XW + b) - Y||_2^2\n",
    "\n",
    "class LeastSquares(nn.Module):\n",
    "    def __init__(self, input_dim, uses_bias = False):\n",
    "        super(LeastSquares, self).__init__()                #Initialize class\n",
    "        self.linear = nn.Linear(input_dim, 1, uses_bias)    #input_dim = dimension of each sample in X, output_dim = 1 = number of y values for each sample\n",
    "                \n",
    "    #Evaluate the Cost Function given X and y\n",
    "    def evaluate(self, X, Y, reduction = 'sum'):\n",
    "        mse_loss = nn.MSELoss(reduction = reduction)\n",
    "        return mse_loss(self.linear(X), Y.rehshape(-1,1))\n",
    "\n",
    "\n",
    "#Cost Function[Least Squares + L2 Regularization Term]\n",
    "#||(XW + b) - Y ||_2^2 + lambda * ||w||^2_2\n",
    "\n",
    "class RidgeRegression(nn.Module):\n",
    "    def __init__(self, input_dim, lmbd, uses_bias = False,Initializer = None):\n",
    "        super(RidgeRegression, self).__init__()\n",
    "\n",
    "        #if we want to initialize the weights\n",
    "        if Initializer is not None:\n",
    "            initialized_weights = Initializer                       # weight matrix for the initialization \n",
    "            self.linear = nn.Linear(input_dim, 1, uses_bias)        #input_dim = dimension of each sample in X, output_dim = 1 = number of y values for each sample\n",
    "            self.linear.weight.data = initialized_weights           # wegiht initialization  \n",
    "        #else the function randomly intializes the weights\n",
    "        else:\n",
    "            self.linear = nn.Linear(input_dim, 1, uses_bias)        #input_dim = dimension of each sample in X, output_dim = 1 = number of y values for each sample\n",
    "        \n",
    "        self.lmbd = lmbd                                            #Ridge Regression Lambda Value\n",
    "        \n",
    "    \n",
    "    \n",
    "    #Evaluate the Cost Function given X and y\n",
    "    def evaluate(self, X, Y, reduction = 'sum'):\n",
    "        mse_loss = nn.MSELoss(reduction = reduction)\n",
    "        return mse_loss(self.linear(X), Y) + self.l2_regularization()\n",
    "            \n",
    "    #Calculate value of lambda * ||w||^2_2\n",
    "    def l2_regularization(self):\n",
    "        return self.lmbd * (torch.norm(self.linear.weight) ** 2)\n",
    "    \n",
    "\n",
    "\n",
    "#Perform Exact Line Search for Ridge Regression\n",
    "#Ridge Regression: ||(XW + b) - Y ||_2^2 + lambda * ||w||^2_2\n",
    "\n",
    "def exact_line_search_RR(X: np.ndarray, Y: np.ndarray, lmbd, cost_function, uses_bias):\n",
    "    #Get Model Parameters\n",
    "    W = cost_function.linear.weight.data.numpy().reshape((-1, 1)) \n",
    "    b = cost_function.linear.bias.item() if uses_bias else 0\n",
    "    \n",
    "    #Search Direction\n",
    "    DeltaW = -1 * cost_function.linear.weight.grad.numpy().reshape((-1, 1))\n",
    "    Deltab = -1 * cost_function.linear.bias.grad if uses_bias else 0\n",
    "    \n",
    "    #Compute value of t\n",
    "    numerator = -((X@W + b - Y).T @ (X @ DeltaW + Deltab)) - (lmbd * (W.T @ DeltaW))\n",
    "    denominator = (np.linalg.norm(X @ DeltaW + Deltab) ** 2) + (lmbd * (np.linalg.norm(DeltaW) ** 2))\n",
    "    t = (numerator / denominator) [0, 0]\n",
    "    \n",
    "    return t    \n",
    "    \n",
    "\n",
    "\n",
    "'''\n",
    "#Optimize a Cost Function via Stochastic Gradient Descent\n",
    "#X: Shape n x d where n is the number of samples and d is the number of features\n",
    "#Y: Shape n x 1 where n is the number of samples\n",
    "#cost_function_code: 0 for Normal Least Squares, 1 for Ridge Regression\n",
    "#hypers: hyperparameters\n",
    "#optimizer_code: 0 for SGD, 1 for Adagrad, 2 for RMSProp\n",
    "#p_star: estimated optimal value\n",
    "#W_true: true weights\n",
    "'''\n",
    "\n",
    "def SGD(X: np.ndarray, Y: np.ndarray, cost_function_code = 1, hypers = {}, optimizer_code = 0, p_star = 0, W_true = None, Initializer = None):\n",
    "    hypers = defaultdict(int, hypers) #Convert hypers to defaultdict\n",
    "\n",
    "    #Get necessary hyperparameters\n",
    "    uses_bias = hypers['bias']                  #determine whether the bias term is needed\n",
    "    lmbd = hypers['lambda']                     #Lambda for ridge regression\n",
    "    lr = hypers['lr']                           #learning rate\n",
    "    epochs = hypers['epochs']                   #number of epochs\n",
    "    batch_size = hypers['batch_size']           #batch size to use for SGD\n",
    "\n",
    "\n",
    "    #Get additional hyperparameters\n",
    "    momentum = hypers['momentum']\n",
    "    dampening = hypers['dampening']\n",
    "    nesterov = hypers['nesterov']\n",
    "    decay_factor = hypers['decay_factor']\n",
    "    \n",
    "    if Initializer is not None:\n",
    "        Initializer = Initializer.flatten(order = 'F')\n",
    "        Initializer = torch.tensor(Initializer, dtype = torch.float32).reshape(1,-1)\n",
    "      \n",
    "    #Initialize Cost Function\n",
    "    if cost_function_code == 0:\n",
    "        cost_function = LeastSquares(X.shape[1], uses_bias)\n",
    "    elif cost_function_code == 1:\n",
    "        cost_function = RidgeRegression(X.shape[1], lmbd, uses_bias,Initializer)\n",
    "    \n",
    "    #Convert X and Y to pytorch tensors\n",
    "    X = torch.tensor(X, dtype = torch.float32)\n",
    "    Y = torch.tensor(Y, dtype = torch.float32)\n",
    "    \n",
    "\n",
    "    \n",
    "    #Initialize Optimizer\n",
    "    if optimizer_code == 0:\n",
    "        optimizer = optim.SGD(cost_function.parameters(), lr = lr, momentum = momentum, dampening = dampening, nesterov = nesterov)\n",
    "    elif optimizer_code == 1:\n",
    "        optimizer = optim.Adagrad(cost_function.parameters(), lr = lr)\n",
    "    elif optimizer_code == 2:\n",
    "        optimizer = optim.RMSprop(cost_function.parameters(), lr = lr, alpha = decay_factor, momentum = momentum)\n",
    "\n",
    "    #if we want learning rate scheduling \n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer,mode = 'min', factor= 0.5,patience=5,threshold=0.01)#milestones=[100,150],gamma = 0.01)\n",
    "    \n",
    "    #Store batch loss values\n",
    "    loss_values = []\n",
    "    \n",
    "\n",
    "    #Store gap to optimality\n",
    "    gap_to_optimality = []\n",
    "\n",
    "    #saving gradient \n",
    "    sub_problem_gradient =  []\n",
    "    \n",
    "    #Store Metric Values\n",
    "\n",
    "    nmse_values = []\n",
    "    corr_values = []\n",
    "    R2_values = []\n",
    "\n",
    "    #for gradient clipping \n",
    "    #clip_value = 100\n",
    "\n",
    "    #Training Loop\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        #Splitting data into minibatches \n",
    "        \n",
    "        print(batch_size)\n",
    "        \n",
    "        #Data_Loader\n",
    "        dataset = TensorDataset(X,Y)\n",
    "        dataloader = DataLoader(dataset, batch_size = batch_size,  shuffle=True )\n",
    "    \n",
    "        num_batches = 0\n",
    "        sum_gradient_norm = 0\n",
    "\n",
    "        for X_sample, Y_sample in dataloader:\n",
    "        \n",
    "            Y_sample = Y_sample.reshape(-1,1)\n",
    "            \n",
    "            # Zero gradients\n",
    "            #optimizer.zero_grad()\n",
    "\n",
    "            \n",
    "            # Zero the gradients\n",
    "            for param in cost_function.parameters():\n",
    "                if param.grad is not None:\n",
    "                    param.grad.zero_()\n",
    "\n",
    "            # Compute stochastic loss\n",
    "            stochastic_loss = cost_function.evaluate(X_sample, Y_sample, 'sum')\n",
    "\n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Backward pass to compute stochastic gradient\n",
    "            stochastic_loss.backward()\n",
    "            \n",
    "            #if we want gradient clipping \n",
    "            #torch.nn.utils.clip_grad_norm_(cost_function.parameters(), clip_value)\n",
    "\n",
    "            # Update parameters\n",
    "            optimizer.step()           \n",
    "\n",
    "            #storing the batch wise gradients \n",
    "            for param in cost_function.parameters():\n",
    "               print(f\"Gradient Norm_Of_Each_Mini_Batch: {torch.norm(param.grad)}\")\n",
    "               stochastic_gradient  = torch.norm(param.grad)\n",
    "               sum_gradient_norm += stochastic_gradient\n",
    "        \n",
    "            num_batches += 1\n",
    "\n",
    "        #Print and Store batch loss values\n",
    "        batch_loss = cost_function.evaluate(X, Y, 'sum')\n",
    "        loss_values.append(batch_loss.item())\n",
    "        gap_to_optimality.append(batch_loss.item() - p_star)\n",
    "        \n",
    "        #learning rate reduction \n",
    "        #scheduler.step(batch_loss.item())\n",
    "\n",
    "        #Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Backward pass to compute batch gradient\n",
    "        batch_loss.backward()\n",
    "\n",
    "        for param in cost_function.parameters():\n",
    "                print(f\"Gradient Norm_Batch: {torch.norm(param.grad)}\")\n",
    "                gradient_after_epoch = torch.norm(param.grad)\n",
    "                sub_problem_gradient.append(gradient_after_epoch)\n",
    "\n",
    "\n",
    "        #summing and averaging the gradients \n",
    "        \n",
    "        #epcoh_gradient = sum_gradient_norm/num_batches\n",
    "        #print(f'Gradient Norm:{epcoh_gradient}')\n",
    "        #sub_problem_gradient.append(epcoh_gradient)\n",
    "        \n",
    "        #looking at the learning rate\n",
    "        print(optimizer.param_groups[0]['lr'])\n",
    "    \n",
    "        #Calculate Metrics\n",
    "        weights = cost_function.linear.weight.data.numpy().reshape((-1, 1))\n",
    "        bias = cost_function.linear.bias.item() if uses_bias else 0\n",
    "        X_numpy = X.numpy()\n",
    "        Y_predicted = X_numpy @ weights + bias\n",
    "        Y_numpy = Y.numpy()\n",
    "        \n",
    "\n",
    "        nmse = np.sum(np.square((Y_predicted - Y_numpy))) / np.sum(np.square(Y_numpy))\n",
    "        correlation = np.corrcoef(Y_predicted.flatten(), Y_numpy.flatten())[0, 1]\n",
    "        R2_score = r2_score(Y_numpy, Y_predicted)\n",
    "        \n",
    "        nmse_values.append(nmse)\n",
    "        corr_values.append(correlation)\n",
    "        R2_values.append(R2_score)\n",
    "        \n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {batch_loss:.4f}, Gap to Optimality: {gap_to_optimality[-1]:.4f}, NMSE: {nmse}, Correlation: {correlation}, R2: {R2_score}')\n",
    "\n",
    "\n",
    "    weights = cost_function.linear.weight.data.numpy().reshape((-1, 1)) #Return weights as numpy array\n",
    "\n",
    "    #return weights and bias and loss metrics\n",
    "    if uses_bias:\n",
    "        return weights, cost_function.linear.bias.item(), loss_values, gap_to_optimality, nmse_values, corr_values, R2_values,sub_problem_gradient,loss_values\n",
    "    else:\n",
    "        return weights, 0, loss_values, gap_to_optimality, nmse_values, corr_values, R2_values,sub_problem_gradient,loss_values\n",
    "    \n",
    "\n",
    "#Optimize a Cost Function via Gradient Descent with Exact Line Search\n",
    "#X: Shape n x d where n is the number of samples and d is the number of features\n",
    "#Y: Shape n x 1 where n is the number of samples\n",
    "#cost_function_code: 0 for Normal Least Squares, 1 for Ridge Regression\n",
    "#hypers: hyperparameters\n",
    "#p_star: estimated optimal value\n",
    "#W_true: true weights\n",
    "def GD2(X: np.ndarray, Y: np.ndarray, cost_function_code = 1, hypers = {}, p_star = 0, W_true = None):\n",
    "    hypers = defaultdict(int, hypers) #Convert hypers to defaultdict\n",
    "    \n",
    "    #Get necessary hyperparameters\n",
    "    uses_bias = hypers['bias'] #determine whether the bias term is needed\n",
    "    lmbd = hypers['lambda'] #Lambda for ridge regression\n",
    "    epochs = hypers['epochs'] #number of epochs\n",
    "    \n",
    "    #Initialize Cost Function\n",
    "    if cost_function_code == 0:\n",
    "        cost_function = LeastSquares(X.shape[1], uses_bias)\n",
    "    elif cost_function_code == 1:\n",
    "        cost_function = RidgeRegression(X.shape[1], lmbd, uses_bias)\n",
    "    \n",
    "    #Convert X and Y to pytorch tensors\n",
    "    X_tensor = torch.tensor(X, dtype = torch.float32)\n",
    "    Y_tensor = torch.tensor(Y, dtype = torch.float32)\n",
    "    \n",
    "    #If W_true is None, set it to a zero vector\n",
    "    #if not isinstance(W_true, np.ndarray):\n",
    "    #    W_true = np.zeros(shape = (X.shape[1], 1))\n",
    "        \n",
    "    #Store batch loss values\n",
    "    loss_values = []\n",
    "    \n",
    "    #Store gap to optimality\n",
    "    gap_to_optimality = []\n",
    "    \n",
    "    #Store Metric Values \n",
    "    #nee_values = []\n",
    "    nmse_values = []\n",
    "    corr_values = []\n",
    "    R2_values = []\n",
    "\n",
    "    #Training Loop\n",
    "    for epoch in range(epochs):\n",
    "        # Zero the gradients\n",
    "        for param in cost_function.parameters():\n",
    "            if param.grad is not None:\n",
    "                param.grad.zero_()\n",
    "            \n",
    "        # Compute loss\n",
    "        loss = cost_function.evaluate(X_tensor, Y_tensor, 'sum')\n",
    "\n",
    "        # Backward pass to compute gradient\n",
    "        loss.backward()\n",
    "\n",
    "        # Update parameters\n",
    "        t = exact_line_search_RR(X, Y, lmbd, cost_function, uses_bias)\n",
    "        print(f\"Value of t is: {t}\")\n",
    "        \n",
    "        # Manually update the weights and biases\n",
    "        with torch.no_grad():\n",
    "            for param in cost_function.parameters():\n",
    "                param -= t * param.grad\n",
    "        \n",
    "        #Print and Store loss values\n",
    "        loss_value = cost_function.evaluate(X_tensor, Y_tensor, 'sum').item()\n",
    "        loss_values.append(loss_value)\n",
    "        gap_to_optimality.append(loss_value - p_star)\n",
    "        \n",
    "        #Calculate Metrics\n",
    "        weights = cost_function.linear.weight.data.numpy().reshape((-1, 1))\n",
    "        bias = cost_function.linear.bias.item() if uses_bias else 0\n",
    "        X_numpy = X_tensor.numpy()\n",
    "        Y_predicted = X_numpy @ weights + bias\n",
    "        Y_numpy = Y_tensor.numpy()\n",
    "        \n",
    "        #nee = ((np.linalg.norm(weights - W_true)) ** 2) /  ((np.linalg.norm(W_true)) ** 2)\n",
    "        nmse = np.sum(np.square((Y_predicted - Y_numpy))) / np.sum(np.square(Y_numpy))\n",
    "        correlation = np.corrcoef(Y_predicted.flatten(), Y_numpy.flatten())[0, 1]\n",
    "        R2_score = r2_score(Y_numpy, Y_predicted)\n",
    "        \n",
    "        #nee_values.append(nee)\n",
    "        nmse_values.append(nmse)\n",
    "        corr_values.append(correlation)\n",
    "        R2_values.append(R2_score)\n",
    "                \n",
    "        #print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss_value:.4f}, Gap to Optimality: {gap_to_optimality[-1]:.4f}, NMSE: {nmse}, Correlation: {correlation}, R2: {R2_score}')\n",
    "        \n",
    "        # Stopping Criteria\n",
    "        criteria_satisfied = True\n",
    "        for name, param in cost_function.named_parameters():\n",
    "            if param.grad is not None:\n",
    "                print(f\"Gradient Norm for {name}: {torch.norm(param.grad)}\")\n",
    "                criteria_satisfied = criteria_satisfied and (torch.norm(param.grad) <= 0.001)\n",
    "            else:\n",
    "                print(f\"No gradient Norm for {name}\")\n",
    "        \n",
    "        if criteria_satisfied:\n",
    "            break\n",
    "\n",
    "    weights = cost_function.linear.weight.data.numpy().reshape((-1, 1)) #Return weights as numpy array\n",
    "\n",
    "    #return weights and bias and loss metrics\n",
    "    if uses_bias:\n",
    "        return weights, cost_function.linear.bias.item(), loss_values, gap_to_optimality, nmse_values, corr_values, R2_values\n",
    "    else:\n",
    "        return weights, 0, loss_values, gap_to_optimality, nmse_values, corr_values, R2_values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_sgd(X_train: np.ndarray, Y_train: np.ndarray, X_test: np.ndarray, Y_test: np.ndarray, lambda1, hypers,Y_train_mean,lsr_tensor_SGD,B_tensored = None,intercept = False, Initializer = None):\n",
    "  hypers['weight_decay'] = lambda1\n",
    "\n",
    "  \n",
    "  #Define LSR Tensor Hyperparameters\n",
    "  ranks = hypers['ranks']\n",
    "  separation_rank = hypers['separation_rank']\n",
    "  LSR_tensor_dot_shape = tuple(X_train.shape)[1:]\n",
    "  need_intercept = intercept\n",
    "\n",
    "  #Construct LSR Tensor\n",
    "  lsr_tensor = lsr_tensor_SGD\n",
    "  lsr_tensor, objective_function_values, gradient_values,iterate_differences,epoch_level_gradients,epoch_level_function,tensor_iteration,factor_core_iterate = lsr_bcd_regression(lsr_tensor, X_train, Y_train, hypers,intercept = need_intercept, Initializer = Initializer)\n",
    "  expanded_lsr = lsr_tensor.expand_to_tensor()\n",
    "  expanded_lsr = np.reshape(expanded_lsr, X_train[0].shape, order = 'F')\n",
    "  print(expanded_lsr.shape)\n",
    "  Y_test_predicted = inner_product(np.transpose(X_test, (0, 2, 1)), expanded_lsr.flatten(order ='F')) + lsr_tensor.b + Y_train_mean\n",
    "\n",
    "  print('---------------------------Testing with Best Lambda------------------------------')\n",
    "  #print(f\"Y_test_predicted: {Y_test_predicted.flatten()}, Y_test: {Y_test.flatten()}\")\n",
    "  test_nmse_loss = np.sum(np.square((Y_test_predicted.flatten() - Y_test.flatten()))) / np.sum(np.square(Y_test.flatten()))\n",
    "  if B_tensored is not None:\n",
    "    normalized_estimation_error = ((np.linalg.norm(expanded_lsr - B_tensored)) ** 2) /  ((np.linalg.norm(B_tensored)) ** 2)\n",
    "  test_R2_loss = R2(Y_test.flatten(), Y_test_predicted.flatten())\n",
    "  test_correlation = np.corrcoef(Y_test_predicted.flatten(), Y_test.flatten())[0, 1]\n",
    "\n",
    "  print(\"Y Test Predicted: \", Y_test_predicted.flatten())\n",
    "  print(\"Y Test Actual: \", Y_test.flatten())\n",
    "\n",
    "  if B_tensored is not None:\n",
    "    return normalized_estimation_error, test_nmse_loss, test_R2_loss, test_correlation, objective_function_values,gradient_values,iterate_differences,epoch_level_gradients,epoch_level_function,tensor_iteration,factor_core_iterate\n",
    "  else:\n",
    "    normalized_estimation_error = np.inf\n",
    "    return normalized_estimation_error, test_nmse_loss, test_R2_loss, test_correlation, objective_function_values,gradient_values,iterate_differences,epoch_level_gradients,epoch_level_function,tensor_iteration,factor_core_iterate\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting the regression implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Setting up global parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_dimensions = np.array([32, 32])\n",
    "tensor_mode_ranks = np.array([4, 4])\n",
    "separation_rank = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample mean for each feature (across samples): [4.99871322 4.98420917 5.00873521 ... 5.01840176 4.98600789 5.01023623]\n",
      "Sample variance for each feature (across samples): None\n",
      "Response Average: 83.56008733093063\n"
     ]
    }
   ],
   "source": [
    "n_train = 650\n",
    "n_test = 100\n",
    "\n",
    "#Subset X_train and Y_train\n",
    "X_train = X_train_Full[0:(n_train),:,:]\n",
    "Y_train = Y_train_Full[0:(n_train)]\n",
    "\n",
    "#Subset X_test and Y_test\n",
    "X_test = X_test_Full[0:(n_test),:,:]\n",
    "Y_test = Y_test_Full[0:(n_test)]\n",
    "\n",
    "\n",
    "#Preprocessing\n",
    "\n",
    "# Reshape the 3D array to a 2D array where each row represents a sample\n",
    "# The shape of the original 3D array is (n_samples, n_features_per_sample, n_dimensions)\n",
    "# We reshape it to (n_samples, n_features_per_sample * n_dimensions)\n",
    "\n",
    "\n",
    "X_train_2D = X_train.reshape(n_train, -1)\n",
    "X_test_2D = X_test.reshape(n_test,-1)\n",
    "\n",
    "\n",
    "# Initialize StandardScaler\n",
    "scaler = StandardScaler(with_std = False) #standard scalar only\n",
    "\n",
    "# Fit scaler on train data and transform train data\n",
    "X_train_scaled = scaler.fit_transform(X_train_2D)\n",
    "# Transform test data using the scaler fitted on train data\n",
    "X_test_scaled = scaler.transform(X_test_2D)\n",
    "\n",
    "# Reshape the scaled data back to 3D\n",
    "X_train = X_train_scaled.reshape(n_train, tensor_dimensions[0],tensor_dimensions[1])\n",
    "X_test  = X_test_scaled.reshape(n_test, tensor_dimensions[0],tensor_dimensions[1])\n",
    "\n",
    "#average response value\n",
    "Y_train_mean = np.mean(Y_train)\n",
    "#Mean centering y_train and y_test\n",
    "Y_train = Y_train - Y_train_mean\n",
    "\n",
    "\n",
    "print(\"Sample mean for each feature (across samples):\",scaler.mean_)\n",
    "print(\"Sample variance for each feature (across samples):\",scaler.var_)\n",
    "print('Response Average:',Y_train_mean)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Intializing the tensor object "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intercept is not initialized\n"
     ]
    }
   ],
   "source": [
    "# intializing the tensor object \n",
    "\n",
    "hypers = {'max_iter': 50, 'threshold': 1e-8, 'ranks': tuple(tensor_mode_ranks), 'separation_rank': separation_rank}\n",
    "\n",
    "ranks = hypers['ranks']\n",
    "separation_rank = hypers['separation_rank']\n",
    "LSR_tensor_dot_shape = tuple(X_train.shape)[1:]\n",
    "need_intercept = False\n",
    "\n",
    "#initializing the tensor object\n",
    "lsr_tensor = LSR_tensor_dot(shape = LSR_tensor_dot_shape, ranks = ranks, separation_rank = separation_rank, intercept = need_intercept)\n",
    "\n",
    "#regularization parameter\n",
    "lambda1 = 2.5\n",
    "\n",
    "\n",
    "#saving the initializer\n",
    "formatted_time = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "\n",
    "if platform.system() == 'Windows':\n",
    "    formatted_time = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    tensor_dimensions_str = \"_\".join(map(str, tensor_dimensions))\n",
    "    tensor_mode_ranks_str = \"_\".join(map(str, tensor_mode_ranks))\n",
    "    pkl_file = rf\"D:\\Tensor Based ML for Neuro Imaging\\INSPIRE_CAHBHIR\\Python Scripts\\LSR-Tensor-Ridge-Regression_All_Data\\Platform_For_Experimenmts\\With_New_Dataset\\Core_Tensor_Isolated_Experiments\\Initializers_ExecutionTime_intercept_5_{formatted_time}_tensor_dimensions_{tensor_dimensions}_tensor_mode_ranks_{tensor_mode_ranks}_separation_rank_{separation_rank}.pkl\"    \n",
    "elif platform.system() == 'Darwin':\n",
    "    pkl_file = f\"/Users/lakrama/Neuro Project Codes/LSR-Tensor-Ridge-Regression_All_Data/Platforms for Experments/Data/Experiments_Isolating_Sub_Problems/Initializers_ExecutionTime_intercept_5_{formatted_time}, tensor_dimensions:{tensor_dimensions}, tensor_mode_= ranks:{tensor_mode_ranks}, separation_rank:{separation_rank}.pkl\"\n",
    "\n",
    "with open(pkl_file, \"wb\") as file:\n",
    "    dill.dump((lsr_tensor), file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the initializer\n",
    "\n",
    "# intializing the tensor object\n",
    "\n",
    "sys.path.append(rf'D:\\Tensor Based ML for Neuro Imaging\\INSPIRE_CAHBHIR\\Python Scripts\\LSR-Tensor-Ridge-Regression\\Platforms_for_Experments\\CodeFiles') \n",
    "from LSR_Tensor_2D_v1 import LSR_tensor_dot\n",
    "\n",
    "\n",
    "#Opening the previous initializer\n",
    "if platform.system() == 'Windows':\n",
    "    pkl_file = rf\"D:\\Tensor Based ML for Neuro Imaging\\INSPIRE_CAHBHIR\\Python Scripts\\LSR-Tensor-Ridge-Regression_All_Data\\Platform_For_Experimenmts\\With_New_Dataset\\Core_Tensor_Isolated_Experiments\\Initializers_ExecutionTime_intercept_5_2024-07-07_18-45-06_tensor_dimensions_[32 32]_tensor_mode_ranks_[4 4]_separation_rank_2.pkl\"\n",
    "elif platform.system() == 'Darwin':\n",
    "    pkl_file = \"/Users/lakrama/Neuro Project Codes/LSR-Tensor-Ridge-Regression_All_Data/Platforms for Experments/Data/Experiments_Isolating_Sub_Problems/Initializers_ExecutionTime_intercept_5_2024-06-23 21:06:37, tensor_dimensions:[32 32], tensor_mode_= ranks:[4 4], separation_rank:2.pkl\"\n",
    "\n",
    "\n",
    "\n",
    "file= open(pkl_file, 'rb')\n",
    "lsr_tensor_initializer = pickle.load(file)\n",
    "file.close()\n",
    "\n",
    "lsr_tensor = copy.deepcopy(lsr_tensor_initializer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Trainning and Testing SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------------------Learning Rate 1e-06 -----------------------------------------\n",
      "Objective Function Value: 3408906.126530059\n",
      "--------------------------------------------------------------BCD iteration 0 --------------------------------------------------------------\n",
      "---------------------------------------------Sep 0 Factor 0 -------------------------------------------------\n",
      "Iteration: 0, Separation Rank: 0, Factor Matrix: 0, Objective Function Value: 4178689.296386482\n",
      "---------------------------------------------Sep 0 Factor 1 -------------------------------------------------\n",
      "Iteration: 0, Separation Rank: 0, Factor Matrix: 1, Objective Function Value: 3488977.6201287964\n",
      "---------------------------------------------Sep 1 Factor 0 -------------------------------------------------\n",
      "Iteration: 0, Separation Rank: 1, Factor Matrix: 0, Objective Function Value: 3092821.460810512\n",
      "---------------------------------------------Sep 1 Factor 1 -------------------------------------------------\n",
      "Iteration: 0, Separation Rank: 1, Factor Matrix: 1, Objective Function Value: 4026588.625912\n",
      "---------------------------------------------Core-------------------------------------------------\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 332434.4375\n",
      "Gradient Norm_Of_Each_Mini_Batch: 201629.609375\n",
      "Gradient Norm_Of_Each_Mini_Batch: 79377.109375\n",
      "Gradient Norm_Batch: 324869.46875\n",
      "1e-06\n",
      "Epoch [1/200], Loss: 82846.4062, Gap to Optimality: 82846.4062, NMSE: 0.24016845226287842, Correlation: 0.9049914093451826, R2: 0.7598315334064399\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 134209.421875\n",
      "Gradient Norm_Of_Each_Mini_Batch: 94577.75\n",
      "Gradient Norm_Of_Each_Mini_Batch: 35809.578125\n",
      "Gradient Norm_Batch: 152796.09375\n",
      "1e-06\n",
      "Epoch [2/200], Loss: 21395.2559, Gap to Optimality: 21395.2559, NMSE: 0.06201999634504318, Correlation: 0.9789384465381699, R2: 0.9379800045612944\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 64310.23046875\n",
      "Gradient Norm_Of_Each_Mini_Batch: 45233.609375\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18798.359375\n",
      "Gradient Norm_Batch: 80248.609375\n",
      "1e-06\n",
      "Epoch [3/200], Loss: 6830.6943, Gap to Optimality: 6830.6943, NMSE: 0.019796296954154968, Correlation: 0.9927705522290059, R2: 0.9802037021335961\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 31808.1328125\n",
      "Gradient Norm_Of_Each_Mini_Batch: 26841.81640625\n",
      "Gradient Norm_Of_Each_Mini_Batch: 11073.9208984375\n",
      "Gradient Norm_Batch: 45760.4296875\n",
      "1e-06\n",
      "Epoch [4/200], Loss: 2581.0540, Gap to Optimality: 2581.0540, NMSE: 0.0074760159477591515, Correlation: 0.9969908438786269, R2: 0.9925239840552553\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16403.87109375\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18898.9140625\n",
      "Gradient Norm_Of_Each_Mini_Batch: 6008.1005859375\n",
      "Gradient Norm_Batch: 28062.412109375\n",
      "1e-06\n",
      "Epoch [5/200], Loss: 1124.3687, Gap to Optimality: 1124.3687, NMSE: 0.0032527742441743612, Correlation: 0.9986022487058701, R2: 0.9967472257470819\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 11297.578125\n",
      "Gradient Norm_Of_Each_Mini_Batch: 10547.58984375\n",
      "Gradient Norm_Of_Each_Mini_Batch: 4302.88232421875\n",
      "Gradient Norm_Batch: 18404.25\n",
      "1e-06\n",
      "Epoch [6/200], Loss: 552.6013, Gap to Optimality: 552.6013, NMSE: 0.0015950424131006002, Correlation: 0.9992793593541573, R2: 0.9984049575541101\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 6824.42041015625\n",
      "Gradient Norm_Of_Each_Mini_Batch: 6934.45361328125\n",
      "Gradient Norm_Of_Each_Mini_Batch: 3607.676513671875\n",
      "Gradient Norm_Batch: 12776.7705078125\n",
      "1e-06\n",
      "Epoch [7/200], Loss: 296.9846, Gap to Optimality: 296.9846, NMSE: 0.0008539032423868775, Correlation: 0.9996001226803001, R2: 0.9991460967923408\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 4902.015625\n",
      "Gradient Norm_Of_Each_Mini_Batch: 4731.9541015625\n",
      "Gradient Norm_Of_Each_Mini_Batch: 2560.138671875\n",
      "Gradient Norm_Batch: 9277.16796875\n",
      "1e-06\n",
      "Epoch [8/200], Loss: 169.7354, Gap to Optimality: 169.7354, NMSE: 0.00048494493239559233, Correlation: 0.9997685157090977, R2: 0.9995150551102417\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 3991.960693359375\n",
      "Gradient Norm_Of_Each_Mini_Batch: 3697.156494140625\n",
      "Gradient Norm_Of_Each_Mini_Batch: 1447.89697265625\n",
      "Gradient Norm_Batch: 6927.74462890625\n",
      "1e-06\n",
      "Epoch [9/200], Loss: 100.8101, Gap to Optimality: 100.8101, NMSE: 0.00028508916147984564, Correlation: 0.9998619886808675, R2: 0.9997149108688193\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 3003.423095703125\n",
      "Gradient Norm_Of_Each_Mini_Batch: 2452.035888671875\n",
      "Gradient Norm_Of_Each_Mini_Batch: 1226.4534912109375\n",
      "Gradient Norm_Batch: 5289.48828125\n",
      "1e-06\n",
      "Epoch [10/200], Loss: 61.8978, Gap to Optimality: 61.8978, NMSE: 0.00017225518240593374, Correlation: 0.999915820731457, R2: 0.999827744807834\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 2383.9033203125\n",
      "Gradient Norm_Of_Each_Mini_Batch: 1834.8145751953125\n",
      "Gradient Norm_Of_Each_Mini_Batch: 921.2598266601562\n",
      "Gradient Norm_Batch: 4089.44775390625\n",
      "1e-06\n",
      "Epoch [11/200], Loss: 38.9574, Gap to Optimality: 38.9574, NMSE: 0.0001057335248333402, Correlation: 0.9999480289512565, R2: 0.9998942664771201\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 1536.6322021484375\n",
      "Gradient Norm_Of_Each_Mini_Batch: 1660.749267578125\n",
      "Gradient Norm_Of_Each_Mini_Batch: 852.3753051757812\n",
      "Gradient Norm_Batch: 3189.517822265625\n",
      "1e-06\n",
      "Epoch [12/200], Loss: 25.1473, Gap to Optimality: 25.1473, NMSE: 6.568575190613046e-05, Correlation: 0.9999675759979958, R2: 0.9999343142486558\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 1109.576904296875\n",
      "Gradient Norm_Of_Each_Mini_Batch: 1315.92431640625\n",
      "Gradient Norm_Of_Each_Mini_Batch: 701.1921997070312\n",
      "Gradient Norm_Batch: 2506.454833984375\n",
      "1e-06\n",
      "Epoch [13/200], Loss: 16.7139, Gap to Optimality: 16.7139, NMSE: 4.1229210182791576e-05, Correlation: 0.9999796125833813, R2: 0.9999587707885836\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 1058.453125\n",
      "Gradient Norm_Of_Each_Mini_Batch: 984.3911743164062\n",
      "Gradient Norm_Of_Each_Mini_Batch: 462.8506774902344\n",
      "Gradient Norm_Batch: 1976.9615478515625\n",
      "1e-06\n",
      "Epoch [14/200], Loss: 11.4720, Gap to Optimality: 11.4720, NMSE: 2.6027357307611965e-05, Correlation: 0.999987119742327, R2: 0.9999739726424219\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 836.7367553710938\n",
      "Gradient Norm_Of_Each_Mini_Batch: 735.6665649414062\n",
      "Gradient Norm_Of_Each_Mini_Batch: 414.074462890625\n",
      "Gradient Norm_Batch: 1565.7615966796875\n",
      "1e-06\n",
      "Epoch [15/200], Loss: 8.2057, Gap to Optimality: 8.2057, NMSE: 1.655366395425517e-05, Correlation: 0.9999917925587413, R2: 0.999983446336551\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 656.984619140625\n",
      "Gradient Norm_Of_Each_Mini_Batch: 608.2366943359375\n",
      "Gradient Norm_Of_Each_Mini_Batch: 271.83258056640625\n",
      "Gradient Norm_Batch: 1244.267333984375\n",
      "1e-06\n",
      "Epoch [16/200], Loss: 6.1539, Gap to Optimality: 6.1539, NMSE: 1.0602506335999351e-05, Correlation: 0.9999947359637502, R2: 0.9999893974942201\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 511.1134338378906\n",
      "Gradient Norm_Of_Each_Mini_Batch: 475.3266296386719\n",
      "Gradient Norm_Of_Each_Mini_Batch: 242.91798400878906\n",
      "Gradient Norm_Batch: 990.9994506835938\n",
      "1e-06\n",
      "Epoch [17/200], Loss: 4.8564, Gap to Optimality: 4.8564, NMSE: 6.8387344072107226e-06, Correlation: 0.9999966011840327, R2: 0.9999931612656365\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 425.8938293457031\n",
      "Gradient Norm_Of_Each_Mini_Batch: 393.43902587890625\n",
      "Gradient Norm_Of_Each_Mini_Batch: 168.02481079101562\n",
      "Gradient Norm_Batch: 791.0848999023438\n",
      "1e-06\n",
      "Epoch [18/200], Loss: 4.0316, Gap to Optimality: 4.0316, NMSE: 4.445977083378239e-06, Correlation: 0.9999977923691604, R2: 0.9999955540226543\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 326.9909362792969\n",
      "Gradient Norm_Of_Each_Mini_Batch: 300.7876892089844\n",
      "Gradient Norm_Of_Each_Mini_Batch: 139.5605010986328\n",
      "Gradient Norm_Batch: 632.5234375\n",
      "1e-06\n",
      "Epoch [19/200], Loss: 3.5064, Gap to Optimality: 3.5064, NMSE: 2.92204822471831e-06, Correlation: 0.9999985479308267, R2: 0.9999970779516983\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 288.2706604003906\n",
      "Gradient Norm_Of_Each_Mini_Batch: 228.82957458496094\n",
      "Gradient Norm_Of_Each_Mini_Batch: 122.88371276855469\n",
      "Gradient Norm_Batch: 506.4241638183594\n",
      "1e-06\n",
      "Epoch [20/200], Loss: 3.1699, Gap to Optimality: 3.1699, NMSE: 1.945716121554142e-06, Correlation: 0.9999990323261392, R2: 0.9999980542839976\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 207.29013061523438\n",
      "Gradient Norm_Of_Each_Mini_Batch: 203.2108917236328\n",
      "Gradient Norm_Of_Each_Mini_Batch: 97.71937561035156\n",
      "Gradient Norm_Batch: 405.82818603515625\n",
      "1e-06\n",
      "Epoch [21/200], Loss: 2.9541, Gap to Optimality: 2.9541, NMSE: 1.319252078246791e-06, Correlation: 0.9999993437596196, R2: 0.999998680747889\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 177.1592254638672\n",
      "Gradient Norm_Of_Each_Mini_Batch: 164.7191619873047\n",
      "Gradient Norm_Of_Each_Mini_Batch: 74.95648956298828\n",
      "Gradient Norm_Batch: 325.5738220214844\n",
      "1e-06\n",
      "Epoch [22/200], Loss: 2.8154, Gap to Optimality: 2.8154, NMSE: 9.16602289180446e-07, Correlation: 0.9999995439142788, R2: 0.9999990833976697\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 135.64312744140625\n",
      "Gradient Norm_Of_Each_Mini_Batch: 132.08998107910156\n",
      "Gradient Norm_Of_Each_Mini_Batch: 64.34027099609375\n",
      "Gradient Norm_Batch: 261.396240234375\n",
      "1e-06\n",
      "Epoch [23/200], Loss: 2.7261, Gap to Optimality: 2.7261, NMSE: 6.574014150828589e-07, Correlation: 0.9999996726883835, R2: 0.9999993425985995\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 97.03936004638672\n",
      "Gradient Norm_Of_Each_Mini_Batch: 129.45867919921875\n",
      "Gradient Norm_Of_Each_Mini_Batch: 57.23530960083008\n",
      "Gradient Norm_Batch: 209.94558715820312\n",
      "1e-06\n",
      "Epoch [24/200], Loss: 2.6684, Gap to Optimality: 2.6684, NMSE: 4.897560756944586e-07, Correlation: 0.9999997560164512, R2: 0.9999995102438999\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 99.5931396484375\n",
      "Gradient Norm_Of_Each_Mini_Batch: 80.76124572753906\n",
      "Gradient Norm_Of_Each_Mini_Batch: 45.36061096191406\n",
      "Gradient Norm_Batch: 168.96087646484375\n",
      "1e-06\n",
      "Epoch [25/200], Loss: 2.6312, Gap to Optimality: 2.6312, NMSE: 3.8188119333426584e-07, Correlation: 0.9999998098945899, R2: 0.9999996181187651\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 76.25157928466797\n",
      "Gradient Norm_Of_Each_Mini_Batch: 69.07341003417969\n",
      "Gradient Norm_Of_Each_Mini_Batch: 37.254676818847656\n",
      "Gradient Norm_Batch: 136.4015655517578\n",
      "1e-06\n",
      "Epoch [26/200], Loss: 2.6073, Gap to Optimality: 2.6073, NMSE: 3.124430634215969e-07, Correlation: 0.9999998445400012, R2: 0.9999996875569506\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 60.0968017578125\n",
      "Gradient Norm_Of_Each_Mini_Batch: 52.250267028808594\n",
      "Gradient Norm_Of_Each_Mini_Batch: 38.52473068237305\n",
      "Gradient Norm_Batch: 110.43995666503906\n",
      "1e-06\n",
      "Epoch [27/200], Loss: 2.5919, Gap to Optimality: 2.5919, NMSE: 2.675390646800224e-07, Correlation: 0.9999998670359185, R2: 0.9999997324609339\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 50.61703109741211\n",
      "Gradient Norm_Of_Each_Mini_Batch: 49.78494644165039\n",
      "Gradient Norm_Of_Each_Mini_Batch: 33.55855941772461\n",
      "Gradient Norm_Batch: 88.96249389648438\n",
      "1e-06\n",
      "Epoch [28/200], Loss: 2.5817, Gap to Optimality: 2.5817, NMSE: 2.3797387882495968e-07, Correlation: 0.9999998815600634, R2: 0.9999997620261435\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 35.06475067138672\n",
      "Gradient Norm_Of_Each_Mini_Batch: 42.36198425292969\n",
      "Gradient Norm_Of_Each_Mini_Batch: 33.02644348144531\n",
      "Gradient Norm_Batch: 72.21752166748047\n",
      "1e-06\n",
      "Epoch [29/200], Loss: 2.5752, Gap to Optimality: 2.5752, NMSE: 2.1901271907154296e-07, Correlation: 0.9999998910227857, R2: 0.9999997809872851\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 32.955814361572266\n",
      "Gradient Norm_Of_Each_Mini_Batch: 36.211097717285156\n",
      "Gradient Norm_Of_Each_Mini_Batch: 27.17124366760254\n",
      "Gradient Norm_Batch: 58.69268035888672\n",
      "1e-06\n",
      "Epoch [30/200], Loss: 2.5710, Gap to Optimality: 2.5710, NMSE: 2.066438327119613e-07, Correlation: 0.9999998971522502, R2: 0.9999997933561805\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 26.87513542175293\n",
      "Gradient Norm_Of_Each_Mini_Batch: 34.74961471557617\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.287242889404297\n",
      "Gradient Norm_Batch: 47.55189895629883\n",
      "1e-06\n",
      "Epoch [31/200], Loss: 2.5682, Gap to Optimality: 2.5682, NMSE: 1.984664805831926e-07, Correlation: 0.9999999011357358, R2: 0.9999998015335118\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 25.122934341430664\n",
      "Gradient Norm_Of_Each_Mini_Batch: 24.617292404174805\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.127724647521973\n",
      "Gradient Norm_Batch: 38.69596862792969\n",
      "1e-06\n",
      "Epoch [32/200], Loss: 2.5664, Gap to Optimality: 2.5664, NMSE: 1.9324627942296502e-07, Correlation: 0.9999999036720073, R2: 0.9999998067537117\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.670583724975586\n",
      "Gradient Norm_Of_Each_Mini_Batch: 28.956844329833984\n",
      "Gradient Norm_Of_Each_Mini_Batch: 13.45928955078125\n",
      "Gradient Norm_Batch: 31.709285736083984\n",
      "1e-06\n",
      "Epoch [33/200], Loss: 2.5652, Gap to Optimality: 2.5652, NMSE: 1.8981138794060826e-07, Correlation: 0.9999999053628318, R2: 0.9999998101886078\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 33.10800552368164\n",
      "Gradient Norm_Of_Each_Mini_Batch: 32.97743606567383\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.263513565063477\n",
      "Gradient Norm_Batch: 25.958776473999023\n",
      "1e-06\n",
      "Epoch [34/200], Loss: 2.5645, Gap to Optimality: 2.5645, NMSE: 1.8752459141069266e-07, Correlation: 0.9999999064037963, R2: 0.9999998124753873\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.16267204284668\n",
      "Gradient Norm_Of_Each_Mini_Batch: 25.29601287841797\n",
      "Gradient Norm_Of_Each_Mini_Batch: 13.71703052520752\n",
      "Gradient Norm_Batch: 22.490877151489258\n",
      "1e-06\n",
      "Epoch [35/200], Loss: 2.5640, Gap to Optimality: 2.5640, NMSE: 1.8616297836615558e-07, Correlation: 0.9999999071416029, R2: 0.9999998138370298\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 24.116201400756836\n",
      "Gradient Norm_Of_Each_Mini_Batch: 27.571271896362305\n",
      "Gradient Norm_Of_Each_Mini_Batch: 24.50190544128418\n",
      "Gradient Norm_Batch: 18.739099502563477\n",
      "1e-06\n",
      "Epoch [36/200], Loss: 2.5636, Gap to Optimality: 2.5636, NMSE: 1.8512186272801046e-07, Correlation: 0.9999999076440333, R2: 0.9999998148781453\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.865942001342773\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.39500617980957\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.78683090209961\n",
      "Gradient Norm_Batch: 15.79136848449707\n",
      "1e-06\n",
      "Epoch [37/200], Loss: 2.5634, Gap to Optimality: 2.5634, NMSE: 1.844877459689087e-07, Correlation: 0.9999999079637972, R2: 0.9999998155122649\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.11478042602539\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.024967193603516\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.49240779876709\n",
      "Gradient Norm_Batch: 14.088398933410645\n",
      "1e-06\n",
      "Epoch [38/200], Loss: 2.5633, Gap to Optimality: 2.5633, NMSE: 1.8409299684662983e-07, Correlation: 0.9999999081673607, R2: 0.9999998159070269\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 12.475401878356934\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.491451263427734\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.873653411865234\n",
      "Gradient Norm_Batch: 13.023041725158691\n",
      "1e-06\n",
      "Epoch [39/200], Loss: 2.5632, Gap to Optimality: 2.5632, NMSE: 1.838368319795336e-07, Correlation: 0.9999999083100396, R2: 0.9999998161631732\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.962892532348633\n",
      "Gradient Norm_Of_Each_Mini_Batch: 27.14417266845703\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.991857528686523\n",
      "Gradient Norm_Batch: 11.780838012695312\n",
      "1e-06\n",
      "Epoch [40/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.836498739749004e-07, Correlation: 0.9999999083614274, R2: 0.999999816350137\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.124914169311523\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.4404354095459\n",
      "Gradient Norm_Of_Each_Mini_Batch: 12.031670570373535\n",
      "Gradient Norm_Batch: 12.713921546936035\n",
      "1e-06\n",
      "Epoch [41/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.8365419407473382e-07, Correlation: 0.9999999084123479, R2: 0.999999816345798\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 27.21830177307129\n",
      "Gradient Norm_Of_Each_Mini_Batch: 27.83776092529297\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.33625602722168\n",
      "Gradient Norm_Batch: 11.677346229553223\n",
      "1e-06\n",
      "Epoch [42/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.8353551922700717e-07, Correlation: 0.9999999084462563, R2: 0.9999998164644879\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.055980682373047\n",
      "Gradient Norm_Of_Each_Mini_Batch: 27.35433578491211\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.230392456054688\n",
      "Gradient Norm_Batch: 9.796012878417969\n",
      "1e-06\n",
      "Epoch [43/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8338216989377543e-07, Correlation: 0.9999999084972617, R2: 0.9999998166178222\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.26431655883789\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.195615768432617\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.6945161819458\n",
      "Gradient Norm_Batch: 11.118388175964355\n",
      "1e-06\n",
      "Epoch [44/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.834390843669098e-07, Correlation: 0.9999999085297038, R2: 0.9999998165609164\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.0192928314209\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.896589279174805\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.476049423217773\n",
      "Gradient Norm_Batch: 13.342292785644531\n",
      "1e-06\n",
      "Epoch [45/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.8357535225277388e-07, Correlation: 0.9999999085499802, R2: 0.9999998164246474\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.128320693969727\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.15004539489746\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.1508731842041\n",
      "Gradient Norm_Batch: 12.758853912353516\n",
      "1e-06\n",
      "Epoch [46/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.8353533448589587e-07, Correlation: 0.9999999085333492, R2: 0.9999998164646654\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.948217391967773\n",
      "Gradient Norm_Of_Each_Mini_Batch: 10.584397315979004\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.28926658630371\n",
      "Gradient Norm_Batch: 12.484614372253418\n",
      "1e-06\n",
      "Epoch [47/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.8351306607655715e-07, Correlation: 0.9999999085448485, R2: 0.9999998164869341\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.804929733276367\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.1377010345459\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.797626495361328\n",
      "Gradient Norm_Batch: 13.354737281799316\n",
      "1e-06\n",
      "Epoch [48/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.83554988097967e-07, Correlation: 0.9999999085622062, R2: 0.9999998164450197\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.428773880004883\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.689603805541992\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.36806297302246\n",
      "Gradient Norm_Batch: 12.024299621582031\n",
      "1e-06\n",
      "Epoch [49/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.8345370733641175e-07, Correlation: 0.9999999085651732, R2: 0.99999981654631\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.54901695251465\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.934720993041992\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.196613311767578\n",
      "Gradient Norm_Batch: 13.67406940460205\n",
      "1e-06\n",
      "Epoch [50/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.8355159170369006e-07, Correlation: 0.9999999085562027, R2: 0.999999816448386\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.52300453186035\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.590518951416016\n",
      "Gradient Norm_Of_Each_Mini_Batch: 13.691763877868652\n",
      "Gradient Norm_Batch: 12.725757598876953\n",
      "1e-06\n",
      "Epoch [51/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.834953451407273e-07, Correlation: 0.9999999085215999, R2: 0.9999998165046509\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.332807540893555\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.009675979614258\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.7994966506958\n",
      "Gradient Norm_Batch: 10.69803524017334\n",
      "1e-06\n",
      "Epoch [52/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8339558494062658e-07, Correlation: 0.999999908544534, R2: 0.9999998166044081\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.429155349731445\n",
      "Gradient Norm_Of_Each_Mini_Batch: 25.132509231567383\n",
      "Gradient Norm_Of_Each_Mini_Batch: 13.689653396606445\n",
      "Gradient Norm_Batch: 12.762594223022461\n",
      "1e-06\n",
      "Epoch [53/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.8346044328154676e-07, Correlation: 0.9999999085383712, R2: 0.9999998165395652\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.807781219482422\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.68714714050293\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.312274932861328\n",
      "Gradient Norm_Batch: 9.842558860778809\n",
      "1e-06\n",
      "Epoch [54/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833238059134601e-07, Correlation: 0.9999999085545906, R2: 0.9999998166761923\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.186330795288086\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.644954681396484\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.440793991088867\n",
      "Gradient Norm_Batch: 10.129828453063965\n",
      "1e-06\n",
      "Epoch [55/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833088560942997e-07, Correlation: 0.9999999085499514, R2: 0.9999998166911338\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.665242195129395\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.41656494140625\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.864428520202637\n",
      "Gradient Norm_Batch: 12.021662712097168\n",
      "1e-06\n",
      "Epoch [56/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.834462750593957e-07, Correlation: 0.9999999085703152, R2: 0.9999998165537176\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.288219451904297\n",
      "Gradient Norm_Of_Each_Mini_Batch: 27.300853729248047\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.681434631347656\n",
      "Gradient Norm_Batch: 9.730366706848145\n",
      "1e-06\n",
      "Epoch [57/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8331826368012116e-07, Correlation: 0.99999990854553, R2: 0.9999998166817438\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.822051048278809\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.154894828796387\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.009148597717285\n",
      "Gradient Norm_Batch: 10.135952949523926\n",
      "1e-06\n",
      "Epoch [58/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8334939966280217e-07, Correlation: 0.9999999085579452, R2: 0.9999998166506087\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.425514221191406\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.043885231018066\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.43088150024414\n",
      "Gradient Norm_Batch: 10.943673133850098\n",
      "1e-06\n",
      "Epoch [59/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833775655768477e-07, Correlation: 0.9999999085199291, R2: 0.9999998166224311\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.710466384887695\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.398773193359375\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.055110931396484\n",
      "Gradient Norm_Batch: 11.334114074707031\n",
      "1e-06\n",
      "Epoch [60/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8343556007494044e-07, Correlation: 0.9999999085438129, R2: 0.9999998165644488\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.867302894592285\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.915727615356445\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.173795700073242\n",
      "Gradient Norm_Batch: 11.699731826782227\n",
      "1e-06\n",
      "Epoch [61/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.8345947694342613e-07, Correlation: 0.9999999085425492, R2: 0.9999998165405332\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.484752655029297\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.510469436645508\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.440813064575195\n",
      "Gradient Norm_Batch: 12.771930694580078\n",
      "1e-06\n",
      "Epoch [62/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.8351622088630393e-07, Correlation: 0.9999999085552332, R2: 0.9999998164837892\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.40532112121582\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.14273452758789\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.703140258789062\n",
      "Gradient Norm_Batch: 12.517322540283203\n",
      "1e-06\n",
      "Epoch [63/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.8349699359987426e-07, Correlation: 0.9999999085551159, R2: 0.9999998165030073\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.64387321472168\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.822111129760742\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.58134651184082\n",
      "Gradient Norm_Batch: 9.729351043701172\n",
      "1e-06\n",
      "Epoch [64/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8333555829030956e-07, Correlation: 0.9999999085324766, R2: 0.9999998166644551\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.442959785461426\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.385501861572266\n",
      "Gradient Norm_Of_Each_Mini_Batch: 24.375368118286133\n",
      "Gradient Norm_Batch: 11.125535011291504\n",
      "1e-06\n",
      "Epoch [65/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8341137320021517e-07, Correlation: 0.9999999085565306, R2: 0.9999998165886163\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.739259719848633\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.979734420776367\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.740612030029297\n",
      "Gradient Norm_Batch: 11.16295337677002\n",
      "1e-06\n",
      "Epoch [66/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.83417768084837e-07, Correlation: 0.9999999085586739, R2: 0.9999998165822335\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.01602554321289\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.280845642089844\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.8471040725708\n",
      "Gradient Norm_Batch: 12.34129810333252\n",
      "1e-06\n",
      "Epoch [67/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.8348396224610042e-07, Correlation: 0.9999999085640859, R2: 0.9999998165160463\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.54204559326172\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.29281997680664\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.12590217590332\n",
      "Gradient Norm_Batch: 13.429341316223145\n",
      "1e-06\n",
      "Epoch [68/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.8353155439854163e-07, Correlation: 0.999999908571352, R2: 0.9999998164684332\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.293630599975586\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.033035278320312\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.191566467285156\n",
      "Gradient Norm_Batch: 10.80988597869873\n",
      "1e-06\n",
      "Epoch [69/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833714833310296e-07, Correlation: 0.9999999085634702, R2: 0.9999998166285222\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.54354476928711\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.654348373413086\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.103519439697266\n",
      "Gradient Norm_Batch: 12.02342700958252\n",
      "1e-06\n",
      "Epoch [70/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8344348973187152e-07, Correlation: 0.9999999085642792, R2: 0.9999998165565104\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.5894775390625\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.421655654907227\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.498249053955078\n",
      "Gradient Norm_Batch: 12.739056587219238\n",
      "1e-06\n",
      "Epoch [71/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.8347530783557886e-07, Correlation: 0.9999999085801012, R2: 0.9999998165247044\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.283838272094727\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.029083251953125\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.938881874084473\n",
      "Gradient Norm_Batch: 12.026068687438965\n",
      "1e-06\n",
      "Epoch [72/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.834627170183012e-07, Correlation: 0.9999999085648588, R2: 0.999999816537294\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.701932907104492\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.703561782836914\n",
      "Gradient Norm_Of_Each_Mini_Batch: 13.998979568481445\n",
      "Gradient Norm_Batch: 12.748703956604004\n",
      "1e-06\n",
      "Epoch [73/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.8346221963838616e-07, Correlation: 0.9999999085784312, R2: 0.9999998165377797\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.36183738708496\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.888118743896484\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.15790367126465\n",
      "Gradient Norm_Batch: 12.441211700439453\n",
      "1e-06\n",
      "Epoch [74/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.8346250385548046e-07, Correlation: 0.99999990856869, R2: 0.9999998165375021\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.418285369873047\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.328990936279297\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.872529983520508\n",
      "Gradient Norm_Batch: 9.339828491210938\n",
      "1e-06\n",
      "Epoch [75/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8329724582599738e-07, Correlation: 0.9999999085553088, R2: 0.9999998167027682\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.67434310913086\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.730106353759766\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.028491973876953\n",
      "Gradient Norm_Batch: 11.639222145080566\n",
      "1e-06\n",
      "Epoch [76/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8342059604492533e-07, Correlation: 0.9999999085648791, R2: 0.999999816579404\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.389968872070312\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.780012130737305\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.096643447875977\n",
      "Gradient Norm_Batch: 10.695436477661133\n",
      "1e-06\n",
      "Epoch [77/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8336967855248076e-07, Correlation: 0.9999999085580387, R2: 0.999999816630297\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.559703826904297\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.328489303588867\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.14497947692871\n",
      "Gradient Norm_Batch: 13.443254470825195\n",
      "1e-06\n",
      "Epoch [78/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.8353127018144733e-07, Correlation: 0.9999999085681214, R2: 0.999999816468736\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.358142852783203\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.665672302246094\n",
      "Gradient Norm_Of_Each_Mini_Batch: 13.957086563110352\n",
      "Gradient Norm_Batch: 12.524682998657227\n",
      "1e-06\n",
      "Epoch [79/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.834896465879865e-07, Correlation: 0.9999999085624401, R2: 0.9999998165103472\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.46332550048828\n",
      "Gradient Norm_Of_Each_Mini_Batch: 27.540578842163086\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.096996307373047\n",
      "Gradient Norm_Batch: 12.967721939086914\n",
      "1e-06\n",
      "Epoch [80/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.835075948974918e-07, Correlation: 0.9999999085627778, R2: 0.9999998164923981\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.88465690612793\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.570700645446777\n",
      "Gradient Norm_Of_Each_Mini_Batch: 11.225502014160156\n",
      "Gradient Norm_Batch: 9.72493839263916\n",
      "1e-06\n",
      "Epoch [81/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8333426510253048e-07, Correlation: 0.999999908550614, R2: 0.9999998166657297\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.082015991210938\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.050403594970703\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.338497161865234\n",
      "Gradient Norm_Batch: 9.846171379089355\n",
      "1e-06\n",
      "Epoch [82/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833389688954412e-07, Correlation: 0.9999999085488788, R2: 0.9999998166610455\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.712749481201172\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.544816970825195\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.517915725708008\n",
      "Gradient Norm_Batch: 11.949657440185547\n",
      "1e-06\n",
      "Epoch [83/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.8349906838466268e-07, Correlation: 0.9999999085377368, R2: 0.9999998165009271\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.475921630859375\n",
      "Gradient Norm_Of_Each_Mini_Batch: 33.94255828857422\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.790550231933594\n",
      "Gradient Norm_Batch: 14.315984725952148\n",
      "1e-06\n",
      "Epoch [84/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.836036034319477e-07, Correlation: 0.9999999085398712, R2: 0.9999998163963882\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.319318771362305\n",
      "Gradient Norm_Of_Each_Mini_Batch: 24.98406982421875\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.145097732543945\n",
      "Gradient Norm_Batch: 9.817646980285645\n",
      "1e-06\n",
      "Epoch [85/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833499823078455e-07, Correlation: 0.9999999085445861, R2: 0.9999998166500257\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.903172492980957\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.009098052978516\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.741348266601562\n",
      "Gradient Norm_Batch: 9.699557304382324\n",
      "1e-06\n",
      "Epoch [86/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833451221955329e-07, Correlation: 0.9999999085455324, R2: 0.9999998166548855\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.577037811279297\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.764869689941406\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.473726272583008\n",
      "Gradient Norm_Batch: 11.387307167053223\n",
      "1e-06\n",
      "Epoch [87/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8341940233312926e-07, Correlation: 0.9999999085601133, R2: 0.9999998165805873\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 12.3861665725708\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.859773635864258\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.313678741455078\n",
      "Gradient Norm_Batch: 9.865365028381348\n",
      "1e-06\n",
      "Epoch [88/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833319487332119e-07, Correlation: 0.9999999085588384, R2: 0.9999998166680523\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.354251861572266\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.755657196044922\n",
      "Gradient Norm_Of_Each_Mini_Batch: 13.118046760559082\n",
      "Gradient Norm_Batch: 8.316024780273438\n",
      "1e-06\n",
      "Epoch [89/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8323966344269138e-07, Correlation: 0.9999999085418685, R2: 0.9999998167603562\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.339365005493164\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.19095802307129\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.262062072753906\n",
      "Gradient Norm_Batch: 11.829166412353516\n",
      "1e-06\n",
      "Epoch [90/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8342147711791768e-07, Correlation: 0.9999999085403306, R2: 0.9999998165785144\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.28949546813965\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.317364692687988\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.592248916625977\n",
      "Gradient Norm_Batch: 11.384730339050293\n",
      "1e-06\n",
      "Epoch [91/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8339098062369885e-07, Correlation: 0.9999999085601908, R2: 0.9999998166090234\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.08208656311035\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.271848678588867\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.633842468261719\n",
      "Gradient Norm_Batch: 10.29280948638916\n",
      "1e-06\n",
      "Epoch [92/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8331897422285692e-07, Correlation: 0.9999999085669803, R2: 0.9999998166810375\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.58808708190918\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.77143669128418\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.825115203857422\n",
      "Gradient Norm_Batch: 12.863916397094727\n",
      "1e-06\n",
      "Epoch [93/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.8348872288243e-07, Correlation: 0.9999999085789216, R2: 0.9999998165112787\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.048357009887695\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.758581161499023\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.11971092224121\n",
      "Gradient Norm_Batch: 10.226053237915039\n",
      "1e-06\n",
      "Epoch [94/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8330705131575087e-07, Correlation: 0.9999999085513808, R2: 0.9999998166929532\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.357057571411133\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.87782096862793\n",
      "Gradient Norm_Of_Each_Mini_Batch: 11.000407218933105\n",
      "Gradient Norm_Batch: 9.01894474029541\n",
      "1e-06\n",
      "Epoch [95/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8326872464058397e-07, Correlation: 0.9999999085382876, R2: 0.999999816731279\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.028974533081055\n",
      "Gradient Norm_Of_Each_Mini_Batch: 29.863548278808594\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.239500045776367\n",
      "Gradient Norm_Batch: 8.402786254882812\n",
      "1e-06\n",
      "Epoch [96/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8323719075397094e-07, Correlation: 0.9999999085379322, R2: 0.9999998167628119\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.85556411743164\n",
      "Gradient Norm_Of_Each_Mini_Batch: 29.026851654052734\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.134449005126953\n",
      "Gradient Norm_Batch: 11.546677589416504\n",
      "1e-06\n",
      "Epoch [97/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8341089003115485e-07, Correlation: 0.9999999085578543, R2: 0.9999998165891038\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.17304229736328\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.78331184387207\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.267987251281738\n",
      "Gradient Norm_Batch: 11.840991973876953\n",
      "1e-06\n",
      "Epoch [98/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8342164764817426e-07, Correlation: 0.9999999085558495, R2: 0.9999998165783526\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.20831871032715\n",
      "Gradient Norm_Of_Each_Mini_Batch: 26.509796142578125\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.1936092376709\n",
      "Gradient Norm_Batch: 12.049205780029297\n",
      "1e-06\n",
      "Epoch [99/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8344481134136004e-07, Correlation: 0.9999999085723544, R2: 0.9999998165551929\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 13.818754196166992\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.08493995666504\n",
      "Gradient Norm_Of_Each_Mini_Batch: 11.278389930725098\n",
      "Gradient Norm_Batch: 10.763376235961914\n",
      "1e-06\n",
      "Epoch [100/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8337141227675602e-07, Correlation: 0.9999999085726048, R2: 0.9999998166285874\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 25.638118743896484\n",
      "Gradient Norm_Of_Each_Mini_Batch: 24.51883316040039\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.622249603271484\n",
      "Gradient Norm_Batch: 12.132233619689941\n",
      "1e-06\n",
      "Epoch [101/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.8345109253914416e-07, Correlation: 0.9999999085470193, R2: 0.9999998165489086\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.627012252807617\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.91642189025879\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.858842849731445\n",
      "Gradient Norm_Batch: 11.529473304748535\n",
      "1e-06\n",
      "Epoch [102/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8341408747346577e-07, Correlation: 0.999999908561526, R2: 0.9999998165859035\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.305578231811523\n",
      "Gradient Norm_Of_Each_Mini_Batch: 25.37289810180664\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.31060791015625\n",
      "Gradient Norm_Batch: 9.921634674072266\n",
      "1e-06\n",
      "Epoch [103/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8331154194584087e-07, Correlation: 0.9999999085628591, R2: 0.9999998166884535\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.2331600189209\n",
      "Gradient Norm_Of_Each_Mini_Batch: 27.17866325378418\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.69449234008789\n",
      "Gradient Norm_Batch: 11.99988079071045\n",
      "1e-06\n",
      "Epoch [104/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.834346932128028e-07, Correlation: 0.9999999085680189, R2: 0.9999998165652935\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 24.446571350097656\n",
      "Gradient Norm_Of_Each_Mini_Batch: 24.586753845214844\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.642850875854492\n",
      "Gradient Norm_Batch: 8.95253849029541\n",
      "1e-06\n",
      "Epoch [105/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.832629692444243e-07, Correlation: 0.9999999085462675, R2: 0.9999998167370269\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.96251106262207\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.243462562561035\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.920698165893555\n",
      "Gradient Norm_Batch: 9.206753730773926\n",
      "1e-06\n",
      "Epoch [106/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.832904388265888e-07, Correlation: 0.9999999085396848, R2: 0.9999998167095567\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.322677612304688\n",
      "Gradient Norm_Of_Each_Mini_Batch: 24.33819007873535\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.97058868408203\n",
      "Gradient Norm_Batch: 10.099013328552246\n",
      "1e-06\n",
      "Epoch [107/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8334586115997809e-07, Correlation: 0.9999999085488426, R2: 0.9999998166541251\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.735082626342773\n",
      "Gradient Norm_Of_Each_Mini_Batch: 24.409961700439453\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.8558406829834\n",
      "Gradient Norm_Batch: 11.929758071899414\n",
      "1e-06\n",
      "Epoch [108/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.834563789770982e-07, Correlation: 0.9999999085534731, R2: 0.9999998165436095\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.39076805114746\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.59356117248535\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.978564262390137\n",
      "Gradient Norm_Batch: 12.35463809967041\n",
      "1e-06\n",
      "Epoch [109/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.834866480976416e-07, Correlation: 0.9999999085600252, R2: 0.9999998165133591\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.812273025512695\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.195470809936523\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.376361846923828\n",
      "Gradient Norm_Batch: 11.01325511932373\n",
      "1e-06\n",
      "Epoch [110/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8339348173412873e-07, Correlation: 0.9999999085450977, R2: 0.9999998166065117\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.602734565734863\n",
      "Gradient Norm_Of_Each_Mini_Batch: 24.047515869140625\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.715913772583008\n",
      "Gradient Norm_Batch: 10.773653030395508\n",
      "1e-06\n",
      "Epoch [111/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8336497475957003e-07, Correlation: 0.9999999085687175, R2: 0.9999998166350075\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.567527770996094\n",
      "Gradient Norm_Of_Each_Mini_Batch: 25.27703285217285\n",
      "Gradient Norm_Of_Each_Mini_Batch: 13.31750202178955\n",
      "Gradient Norm_Batch: 12.117094039916992\n",
      "1e-06\n",
      "Epoch [112/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.8345915009376768e-07, Correlation: 0.9999999085647187, R2: 0.9999998165408525\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.696317672729492\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.213207244873047\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.587417602539062\n",
      "Gradient Norm_Batch: 12.695547103881836\n",
      "1e-06\n",
      "Epoch [113/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.8351515507220029e-07, Correlation: 0.9999999085501293, R2: 0.9999998164848446\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.128772735595703\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.263168334960938\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.010623931884766\n",
      "Gradient Norm_Batch: 13.289290428161621\n",
      "1e-06\n",
      "Epoch [114/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.8356250564011134e-07, Correlation: 0.9999999085589841, R2: 0.999999816437506\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.485258102416992\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.330846786499023\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.404104232788086\n",
      "Gradient Norm_Batch: 13.358307838439941\n",
      "1e-06\n",
      "Epoch [115/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.835364713542731e-07, Correlation: 0.9999999085701385, R2: 0.9999998164635209\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.587578773498535\n",
      "Gradient Norm_Of_Each_Mini_Batch: 24.329957962036133\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.032270431518555\n",
      "Gradient Norm_Batch: 11.676409721374512\n",
      "1e-06\n",
      "Epoch [116/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8342724672493205e-07, Correlation: 0.9999999085686999, R2: 0.9999998165727679\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 11.124074935913086\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.761411666870117\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.426185607910156\n",
      "Gradient Norm_Batch: 12.913411140441895\n",
      "1e-06\n",
      "Epoch [117/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.8349250296978425e-07, Correlation: 0.9999999085745114, R2: 0.9999998165075075\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.93191146850586\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.00052833557129\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.374168395996094\n",
      "Gradient Norm_Batch: 13.218182563781738\n",
      "1e-06\n",
      "Epoch [118/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.834794574051557e-07, Correlation: 0.9999999085552501, R2: 0.9999998165205496\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.50227928161621\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.55829620361328\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.374184608459473\n",
      "Gradient Norm_Batch: 13.318374633789062\n",
      "1e-06\n",
      "Epoch [119/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.8349948049944942e-07, Correlation: 0.99999990855641, R2: 0.9999998165005196\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 13.984423637390137\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.6619930267334\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.357013702392578\n",
      "Gradient Norm_Batch: 9.931772232055664\n",
      "1e-06\n",
      "Epoch [120/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833447527133103e-07, Correlation: 0.9999999085474399, R2: 0.9999998166552259\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.469776153564453\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.4871768951416\n",
      "Gradient Norm_Of_Each_Mini_Batch: 10.934881210327148\n",
      "Gradient Norm_Batch: 11.598402976989746\n",
      "1e-06\n",
      "Epoch [121/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8343530427955557e-07, Correlation: 0.9999999085621728, R2: 0.9999998165646902\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.81888771057129\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.934797286987305\n",
      "Gradient Norm_Of_Each_Mini_Batch: 13.105926513671875\n",
      "Gradient Norm_Batch: 9.385150909423828\n",
      "1e-06\n",
      "Epoch [122/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8328076123452774e-07, Correlation: 0.9999999085707254, R2: 0.9999998167192412\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.329065322875977\n",
      "Gradient Norm_Of_Each_Mini_Batch: 28.224407196044922\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.628339767456055\n",
      "Gradient Norm_Batch: 9.921165466308594\n",
      "1e-06\n",
      "Epoch [123/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8331469675558765e-07, Correlation: 0.9999999085528618, R2: 0.9999998166852904\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.20543670654297\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.050113677978516\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.358884811401367\n",
      "Gradient Norm_Batch: 11.635610580444336\n",
      "1e-06\n",
      "Epoch [124/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833928848782307e-07, Correlation: 0.9999999085719821, R2: 0.9999998166070979\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.479722023010254\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.699644088745117\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.58449363708496\n",
      "Gradient Norm_Batch: 10.23537826538086\n",
      "1e-06\n",
      "Epoch [125/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8331905948798521e-07, Correlation: 0.9999999085786275, R2: 0.9999998166809267\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 13.740492820739746\n",
      "Gradient Norm_Of_Each_Mini_Batch: 9.343396186828613\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.709857940673828\n",
      "Gradient Norm_Batch: 8.50851821899414\n",
      "1e-06\n",
      "Epoch [126/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.832301563808869e-07, Correlation: 0.9999999085703768, R2: 0.9999998167698585\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.046175003051758\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.71234703063965\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.29180908203125\n",
      "Gradient Norm_Batch: 12.324850082397461\n",
      "1e-06\n",
      "Epoch [127/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8342650776048686e-07, Correlation: 0.9999999085760829, R2: 0.9999998165734907\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.84551239013672\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.22305679321289\n",
      "Gradient Norm_Of_Each_Mini_Batch: 11.42793083190918\n",
      "Gradient Norm_Batch: 14.939251899719238\n",
      "1e-06\n",
      "Epoch [128/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.836130394394786e-07, Correlation: 0.9999999085790096, R2: 0.9999998163869608\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.379323959350586\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.876171112060547\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.721052169799805\n",
      "Gradient Norm_Batch: 10.924736976623535\n",
      "1e-06\n",
      "Epoch [129/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833755192137687e-07, Correlation: 0.9999999085498629, R2: 0.9999998166244731\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.95759391784668\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.643918991088867\n",
      "Gradient Norm_Of_Each_Mini_Batch: 24.586862564086914\n",
      "Gradient Norm_Batch: 13.036539077758789\n",
      "1e-06\n",
      "Epoch [130/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.8349371089243505e-07, Correlation: 0.9999999085757533, R2: 0.9999998165063145\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.583160400390625\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.606407165527344\n",
      "Gradient Norm_Of_Each_Mini_Batch: 13.796428680419922\n",
      "Gradient Norm_Batch: 13.0213041305542\n",
      "1e-06\n",
      "Epoch [131/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.83534979214528e-07, Correlation: 0.9999999085598178, R2: 0.999999816465021\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.751371383666992\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.200721740722656\n",
      "Gradient Norm_Of_Each_Mini_Batch: 13.48578929901123\n",
      "Gradient Norm_Batch: 14.252323150634766\n",
      "1e-06\n",
      "Epoch [132/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.8357793862833205e-07, Correlation: 0.9999999085354726, R2: 0.9999998164220618\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.613372802734375\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.537498474121094\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.99746322631836\n",
      "Gradient Norm_Batch: 13.987309455871582\n",
      "1e-06\n",
      "Epoch [133/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.836115472997335e-07, Correlation: 0.9999999085556508, R2: 0.999999816388454\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.92617416381836\n",
      "Gradient Norm_Of_Each_Mini_Batch: 29.099651336669922\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.0065975189209\n",
      "Gradient Norm_Batch: 13.067094802856445\n",
      "1e-06\n",
      "Epoch [134/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.8355886766130425e-07, Correlation: 0.9999999085434613, R2: 0.999999816441125\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 26.458087921142578\n",
      "Gradient Norm_Of_Each_Mini_Batch: 31.4088077545166\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.851699829101562\n",
      "Gradient Norm_Batch: 15.434395790100098\n",
      "1e-06\n",
      "Epoch [135/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.836869785165618e-07, Correlation: 0.9999999085649081, R2: 0.9999998163130427\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.60065269470215\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.4450740814209\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.48059844970703\n",
      "Gradient Norm_Batch: 12.555368423461914\n",
      "1e-06\n",
      "Epoch [136/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.8351082076151215e-07, Correlation: 0.9999999085540298, R2: 0.9999998164891751\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.9637451171875\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.707738876342773\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.695566177368164\n",
      "Gradient Norm_Batch: 11.740762710571289\n",
      "1e-06\n",
      "Epoch [137/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8345130570196488e-07, Correlation: 0.9999999085522338, R2: 0.9999998165487046\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 24.67416000366211\n",
      "Gradient Norm_Of_Each_Mini_Batch: 27.108192443847656\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.74167251586914\n",
      "Gradient Norm_Batch: 11.186890602111816\n",
      "1e-06\n",
      "Epoch [138/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8339798657507345e-07, Correlation: 0.9999999085256672, R2: 0.9999998166020129\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.410030364990234\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.810853958129883\n",
      "Gradient Norm_Of_Each_Mini_Batch: 12.435800552368164\n",
      "Gradient Norm_Batch: 9.537797927856445\n",
      "1e-06\n",
      "Epoch [139/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833071792134433e-07, Correlation: 0.9999999085433657, R2: 0.9999998166928254\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.9632568359375\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.372621536254883\n",
      "Gradient Norm_Of_Each_Mini_Batch: 24.02293586730957\n",
      "Gradient Norm_Batch: 15.430707931518555\n",
      "1e-06\n",
      "Epoch [140/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.8361386366905208e-07, Correlation: 0.9999999085396901, R2: 0.999999816386134\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 24.451902389526367\n",
      "Gradient Norm_Of_Each_Mini_Batch: 26.960317611694336\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.394296646118164\n",
      "Gradient Norm_Batch: 15.650191307067871\n",
      "1e-06\n",
      "Epoch [141/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.836881011740843e-07, Correlation: 0.9999999085712254, R2: 0.9999998163119033\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.090593338012695\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.465755462646484\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.728764533996582\n",
      "Gradient Norm_Batch: 10.964807510375977\n",
      "1e-06\n",
      "Epoch [142/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833610241419592e-07, Correlation: 0.9999999085707034, R2: 0.9999998166389747\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.530696868896484\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.284242630004883\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.928900718688965\n",
      "Gradient Norm_Batch: 12.058243751525879\n",
      "1e-06\n",
      "Epoch [143/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8342251451031188e-07, Correlation: 0.9999999085706875, R2: 0.9999998165774948\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.920793533325195\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.000396728515625\n",
      "Gradient Norm_Of_Each_Mini_Batch: 12.017558097839355\n",
      "Gradient Norm_Batch: 11.955894470214844\n",
      "1e-06\n",
      "Epoch [144/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.834230971553552e-07, Correlation: 0.9999999085716635, R2: 0.9999998165768916\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.530668258666992\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.734804153442383\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.177961349487305\n",
      "Gradient Norm_Batch: 15.75707721710205\n",
      "1e-06\n",
      "Epoch [145/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.8367609300184995e-07, Correlation: 0.9999999085706046, R2: 0.9999998163238876\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.383974075317383\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.26006317138672\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.338907241821289\n",
      "Gradient Norm_Batch: 11.890414237976074\n",
      "1e-06\n",
      "Epoch [146/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.8345781427342445e-07, Correlation: 0.9999999085553527, R2: 0.9999998165422023\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.997506141662598\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.818008422851562\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.35045623779297\n",
      "Gradient Norm_Batch: 10.27341079711914\n",
      "1e-06\n",
      "Epoch [147/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833539187146016e-07, Correlation: 0.9999999085351864, R2: 0.9999998166460787\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.56719970703125\n",
      "Gradient Norm_Of_Each_Mini_Batch: 25.028640747070312\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.05628204345703\n",
      "Gradient Norm_Batch: 7.918633460998535\n",
      "1e-06\n",
      "Epoch [148/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8322421624361596e-07, Correlation: 0.9999999085363156, R2: 0.9999998167757754\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.748144149780273\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.00238609313965\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.17879295349121\n",
      "Gradient Norm_Batch: 7.342883110046387\n",
      "1e-06\n",
      "Epoch [149/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8319161654289928e-07, Correlation: 0.999999908529176, R2: 0.9999998168083752\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.26785659790039\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.663801193237305\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.540803909301758\n",
      "Gradient Norm_Batch: 11.486566543579102\n",
      "1e-06\n",
      "Epoch [150/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8339375174036832e-07, Correlation: 0.9999999085509396, R2: 0.9999998166062771\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 24.217391967773438\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.964035034179688\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.44333839416504\n",
      "Gradient Norm_Batch: 10.47614574432373\n",
      "1e-06\n",
      "Epoch [151/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8328964301872475e-07, Correlation: 0.9999999085317225, R2: 0.999999816710343\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.073692321777344\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.992002487182617\n",
      "Gradient Norm_Of_Each_Mini_Batch: 13.734715461730957\n",
      "Gradient Norm_Batch: 9.319051742553711\n",
      "1e-06\n",
      "Epoch [152/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.832635234677582e-07, Correlation: 0.9999999085624711, R2: 0.9999998167364698\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.922513961791992\n",
      "Gradient Norm_Of_Each_Mini_Batch: 24.453262329101562\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.85376739501953\n",
      "Gradient Norm_Batch: 12.373946189880371\n",
      "1e-06\n",
      "Epoch [153/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.8346686658787803e-07, Correlation: 0.999999908572204, R2: 0.9999998165331424\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 26.701480865478516\n",
      "Gradient Norm_Of_Each_Mini_Batch: 31.09200668334961\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.507340431213379\n",
      "Gradient Norm_Batch: 14.11805248260498\n",
      "1e-06\n",
      "Epoch [154/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.8356567466071283e-07, Correlation: 0.9999999085729661, R2: 0.9999998164343278\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 13.115307807922363\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.7932071685791\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.731559753417969\n",
      "Gradient Norm_Batch: 12.387335777282715\n",
      "1e-06\n",
      "Epoch [155/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.834754357332713e-07, Correlation: 0.9999999085724379, R2: 0.9999998165245579\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.59120750427246\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.97350311279297\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.879154205322266\n",
      "Gradient Norm_Batch: 10.860677719116211\n",
      "1e-06\n",
      "Epoch [156/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833722365063295e-07, Correlation: 0.9999999085656859, R2: 0.9999998166277584\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.409530639648438\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.395784378051758\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.35639762878418\n",
      "Gradient Norm_Batch: 7.923036098480225\n",
      "1e-06\n",
      "Epoch [157/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.832117249023213e-07, Correlation: 0.999999908564738, R2: 0.9999998167882737\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.70278549194336\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.23256492614746\n",
      "Gradient Norm_Of_Each_Mini_Batch: 12.576423645019531\n",
      "Gradient Norm_Batch: 9.149033546447754\n",
      "1e-06\n",
      "Epoch [158/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8328185547034082e-07, Correlation: 0.9999999085633365, R2: 0.999999816718125\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.372499465942383\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.339046478271484\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.520488739013672\n",
      "Gradient Norm_Batch: 12.799436569213867\n",
      "1e-06\n",
      "Epoch [159/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.8345980379308457e-07, Correlation: 0.9999999085632091, R2: 0.9999998165401871\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.887474060058594\n",
      "Gradient Norm_Of_Each_Mini_Batch: 11.757987976074219\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.72514533996582\n",
      "Gradient Norm_Batch: 10.575948715209961\n",
      "1e-06\n",
      "Epoch [160/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8336808693675266e-07, Correlation: 0.9999999085613477, R2: 0.9999998166319161\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.66712760925293\n",
      "Gradient Norm_Of_Each_Mini_Batch: 25.66614532470703\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.95012855529785\n",
      "Gradient Norm_Batch: 11.285624504089355\n",
      "1e-06\n",
      "Epoch [161/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833881242419011e-07, Correlation: 0.9999999085602218, R2: 0.9999998166118785\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.23513412475586\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.10525894165039\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.989356994628906\n",
      "Gradient Norm_Batch: 9.875848770141602\n",
      "1e-06\n",
      "Epoch [162/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833101777037882e-07, Correlation: 0.9999999085614166, R2: 0.9999998166898291\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.99964714050293\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.095373153686523\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.956391334533691\n",
      "Gradient Norm_Batch: 9.572555541992188\n",
      "1e-06\n",
      "Epoch [163/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8329592421650887e-07, Correlation: 0.9999999085700529, R2: 0.9999998167040743\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.722394943237305\n",
      "Gradient Norm_Of_Each_Mini_Batch: 24.666044235229492\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.984996795654297\n",
      "Gradient Norm_Batch: 11.89120864868164\n",
      "1e-06\n",
      "Epoch [164/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.83430941547158e-07, Correlation: 0.9999999085624152, R2: 0.9999998165690501\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.973711967468262\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.67071533203125\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.698148727416992\n",
      "Gradient Norm_Batch: 13.76863956451416\n",
      "1e-06\n",
      "Epoch [165/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.835442304809476e-07, Correlation: 0.999999908562796, R2: 0.9999998164557672\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.17555046081543\n",
      "Gradient Norm_Of_Each_Mini_Batch: 13.840141296386719\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.524943351745605\n",
      "Gradient Norm_Batch: 12.490592956542969\n",
      "1e-06\n",
      "Epoch [166/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.8349025765473925e-07, Correlation: 0.999999908569617, R2: 0.9999998165097692\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.722299575805664\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.15504264831543\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.512551307678223\n",
      "Gradient Norm_Batch: 12.169746398925781\n",
      "1e-06\n",
      "Epoch [167/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.8349331298850302e-07, Correlation: 0.9999999085559373, R2: 0.9999998165066668\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 13.646475791931152\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.251628875732422\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.445649147033691\n",
      "Gradient Norm_Batch: 11.636122703552246\n",
      "1e-06\n",
      "Epoch [168/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8344273655657162e-07, Correlation: 0.9999999085621585, R2: 0.9999998165572549\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.47502899169922\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.34071159362793\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.451135635375977\n",
      "Gradient Norm_Batch: 10.801311492919922\n",
      "1e-06\n",
      "Epoch [169/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833537197626356e-07, Correlation: 0.999999908561197, R2: 0.999999816646276\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.08378791809082\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.618738174438477\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.223026275634766\n",
      "Gradient Norm_Batch: 9.457717895507812\n",
      "1e-06\n",
      "Epoch [170/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8327241946280992e-07, Correlation: 0.9999999085469414, R2: 0.9999998167275785\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.338590621948242\n",
      "Gradient Norm_Of_Each_Mini_Batch: 28.471200942993164\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.350984573364258\n",
      "Gradient Norm_Batch: 10.928791046142578\n",
      "1e-06\n",
      "Epoch [171/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8339900975661294e-07, Correlation: 0.999999908561807, R2: 0.9999998166009711\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.188241958618164\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.272472381591797\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.64065933227539\n",
      "Gradient Norm_Batch: 12.083597183227539\n",
      "1e-06\n",
      "Epoch [172/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.834735741113036e-07, Correlation: 0.9999999085481227, R2: 0.9999998165264151\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.48750114440918\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.094406127929688\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.824153900146484\n",
      "Gradient Norm_Batch: 10.9791898727417\n",
      "1e-06\n",
      "Epoch [173/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8340323038046336e-07, Correlation: 0.9999999085497017, R2: 0.9999998165967738\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.590568542480469\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.75983428955078\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.55733871459961\n",
      "Gradient Norm_Batch: 10.645730972290039\n",
      "1e-06\n",
      "Epoch [174/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8337301810333884e-07, Correlation: 0.999999908557819, R2: 0.9999998166269787\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.869688034057617\n",
      "Gradient Norm_Of_Each_Mini_Batch: 27.152982711791992\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.626604080200195\n",
      "Gradient Norm_Batch: 9.092049598693848\n",
      "1e-06\n",
      "Epoch [175/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8328496764752344e-07, Correlation: 0.9999999085474034, R2: 0.9999998167150299\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.78692626953125\n",
      "Gradient Norm_Of_Each_Mini_Batch: 25.354015350341797\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.187917709350586\n",
      "Gradient Norm_Batch: 10.92646312713623\n",
      "1e-06\n",
      "Epoch [176/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833727907296634e-07, Correlation: 0.999999908574233, R2: 0.9999998166272129\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.285045623779297\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.830408096313477\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.437308311462402\n",
      "Gradient Norm_Batch: 10.499340057373047\n",
      "1e-06\n",
      "Epoch [177/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833641931625607e-07, Correlation: 0.9999999085520408, R2: 0.9999998166358136\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 13.86697769165039\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.37710189819336\n",
      "Gradient Norm_Of_Each_Mini_Batch: 13.705904960632324\n",
      "Gradient Norm_Batch: 9.393693923950195\n",
      "1e-06\n",
      "Epoch [178/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8329563999941456e-07, Correlation: 0.9999999085660352, R2: 0.9999998167043506\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.313495635986328\n",
      "Gradient Norm_Of_Each_Mini_Batch: 24.532236099243164\n",
      "Gradient Norm_Of_Each_Mini_Batch: 13.249183654785156\n",
      "Gradient Norm_Batch: 9.6143159866333\n",
      "1e-06\n",
      "Epoch [179/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833038112408758e-07, Correlation: 0.9999999085612306, R2: 0.999999816696191\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.148326873779297\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.150774002075195\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.08942413330078\n",
      "Gradient Norm_Batch: 10.668021202087402\n",
      "1e-06\n",
      "Epoch [180/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8335127549562458e-07, Correlation: 0.9999999085640824, R2: 0.9999998166487252\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 8.469644546508789\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.530463218688965\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.174882888793945\n",
      "Gradient Norm_Batch: 9.244423866271973\n",
      "1e-06\n",
      "Epoch [181/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8328702822145715e-07, Correlation: 0.9999999085641605, R2: 0.9999998167129591\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.810840606689453\n",
      "Gradient Norm_Of_Each_Mini_Batch: 26.96257209777832\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.93524932861328\n",
      "Gradient Norm_Batch: 11.14671516418457\n",
      "1e-06\n",
      "Epoch [182/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8336842799726583e-07, Correlation: 0.9999999085596958, R2: 0.9999998166315852\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.96454429626465\n",
      "Gradient Norm_Of_Each_Mini_Batch: 24.333208084106445\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.246158599853516\n",
      "Gradient Norm_Batch: 10.491025924682617\n",
      "1e-06\n",
      "Epoch [183/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8336380946948339e-07, Correlation: 0.9999999085597996, R2: 0.9999998166362045\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.701759338378906\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.998929977416992\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.90936851501465\n",
      "Gradient Norm_Batch: 10.794476509094238\n",
      "1e-06\n",
      "Epoch [184/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833628999747816e-07, Correlation: 0.9999999085557361, R2: 0.9999998166371135\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.898002624511719\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.860904693603516\n",
      "Gradient Norm_Of_Each_Mini_Batch: 13.126790046691895\n",
      "Gradient Norm_Batch: 9.629591941833496\n",
      "1e-06\n",
      "Epoch [185/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8332347906380164e-07, Correlation: 0.9999999085584084, R2: 0.9999998166765087\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 30.955249786376953\n",
      "Gradient Norm_Of_Each_Mini_Batch: 31.075965881347656\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.0681095123291\n",
      "Gradient Norm_Batch: 11.68966007232666\n",
      "1e-06\n",
      "Epoch [186/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8336734797230747e-07, Correlation: 0.9999999085127809, R2: 0.9999998166326571\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.54081916809082\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.423660278320312\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.71871566772461\n",
      "Gradient Norm_Batch: 10.844741821289062\n",
      "1e-06\n",
      "Epoch [187/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8337094331855042e-07, Correlation: 0.9999999085464532, R2: 0.999999816629062\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.384037017822266\n",
      "Gradient Norm_Of_Each_Mini_Batch: 26.311574935913086\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.635221481323242\n",
      "Gradient Norm_Batch: 11.093061447143555\n",
      "1e-06\n",
      "Epoch [188/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8339584073601145e-07, Correlation: 0.9999999085614381, R2: 0.9999998166041606\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.868896484375\n",
      "Gradient Norm_Of_Each_Mini_Batch: 25.341726303100586\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.82088279724121\n",
      "Gradient Norm_Batch: 12.39342212677002\n",
      "1e-06\n",
      "Epoch [189/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.8344714192153333e-07, Correlation: 0.9999999085577446, R2: 0.9999998165528704\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.820072174072266\n",
      "Gradient Norm_Of_Each_Mini_Batch: 24.393508911132812\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.20909881591797\n",
      "Gradient Norm_Batch: 11.568923950195312\n",
      "1e-06\n",
      "Epoch [190/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8335819618187088e-07, Correlation: 0.9999999085305898, R2: 0.9999998166418175\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.830638885498047\n",
      "Gradient Norm_Of_Each_Mini_Batch: 26.846736907958984\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.046724319458008\n",
      "Gradient Norm_Batch: 10.799945831298828\n",
      "1e-06\n",
      "Epoch [191/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8337564711146115e-07, Correlation: 0.9999999085655104, R2: 0.9999998166243562\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.720197677612305\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.59610366821289\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.88601303100586\n",
      "Gradient Norm_Batch: 10.739742279052734\n",
      "1e-06\n",
      "Epoch [192/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8337294704906526e-07, Correlation: 0.9999999085593703, R2: 0.9999998166270524\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.89630699157715\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.050912857055664\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.971789360046387\n",
      "Gradient Norm_Batch: 13.519183158874512\n",
      "1e-06\n",
      "Epoch [193/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.835174003872453e-07, Correlation: 0.9999999085669394, R2: 0.9999998164826094\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.156536102294922\n",
      "Gradient Norm_Of_Each_Mini_Batch: 25.149864196777344\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.602144241333008\n",
      "Gradient Norm_Batch: 10.116050720214844\n",
      "1e-06\n",
      "Epoch [194/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833384999372356e-07, Correlation: 0.9999999085556653, R2: 0.9999998166615118\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 27.570411682128906\n",
      "Gradient Norm_Of_Each_Mini_Batch: 24.585186004638672\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.206157684326172\n",
      "Gradient Norm_Batch: 12.49827766418457\n",
      "1e-06\n",
      "Epoch [195/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833981286836206e-07, Correlation: 0.9999999085212721, R2: 0.9999998166018812\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 28.684782028198242\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.588666915893555\n",
      "Gradient Norm_Of_Each_Mini_Batch: 24.268077850341797\n",
      "Gradient Norm_Batch: 17.501272201538086\n",
      "1e-06\n",
      "Epoch [196/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.8376292132415983e-07, Correlation: 0.999999908532365, R2: 0.9999998162370693\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.12782859802246\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.21356964111328\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.866230010986328\n",
      "Gradient Norm_Batch: 14.883712768554688\n",
      "1e-06\n",
      "Epoch [197/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.8362085540957196e-07, Correlation: 0.9999999085541605, R2: 0.9999998163791373\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.366508483886719\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.331815719604492\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.459388732910156\n",
      "Gradient Norm_Batch: 12.327767372131348\n",
      "1e-06\n",
      "Epoch [198/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.8349538777329144e-07, Correlation: 0.9999999085579849, R2: 0.9999998165046087\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.708656311035156\n",
      "Gradient Norm_Of_Each_Mini_Batch: 28.381317138671875\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.809398651123047\n",
      "Gradient Norm_Batch: 11.071109771728516\n",
      "1e-06\n",
      "Epoch [199/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.834205676232159e-07, Correlation: 0.9999999085499193, R2: 0.9999998165794223\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.19709014892578\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.73155403137207\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.387880325317383\n",
      "Gradient Norm_Batch: 9.583050727844238\n",
      "1e-06\n",
      "Epoch [200/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8332013951294357e-07, Correlation: 0.9999999085379035, R2: 0.9999998166798744\n",
      "Final gradient of the subproblem Core : 9.583050727844238\n",
      "(32, 32)\n",
      "---------------------------Testing with Best Lambda------------------------------\n",
      "Y Test Predicted:  [ 67.54902322  64.18963918 100.90615745 106.79425699  66.52897828\n",
      "  96.79613025 112.76409593  87.0813828   58.88521269  60.74044368\n",
      "  65.75336752  64.15718826  82.80165081  68.56401064 142.4588049\n",
      "  98.02005142  98.35283993  55.41619624  96.74703852  87.26471175\n",
      "  47.97822377  91.13725471  73.56511655  77.46459511  46.96429191\n",
      "  79.37991622  84.71034467  45.49457156  87.5668339   73.47983885\n",
      "  86.90817976  88.26733719 124.65323303  81.42695849  87.54737693\n",
      "  82.79792967  98.73508563  69.03790449  80.63823763  93.08319914\n",
      "  68.40997259  87.2803606   62.04192555  57.34036325  56.02099718\n",
      "  89.30929935 110.5433431  134.61109842  43.26806693  74.48151496\n",
      "  96.23312348  87.89129628  79.23415793  90.72745019 125.88538363\n",
      " 160.37041715  60.51189198  86.88824628  89.5741344   85.49688003\n",
      "  79.97199602  95.54258091  57.09925436  50.36925943  70.86101412\n",
      " 131.23405596  94.51180555  83.16055957  62.92638357  84.10761216\n",
      "  76.25914115  96.48136332  77.08434971  65.7926323   71.77911362\n",
      "  65.14976093  61.02623447  68.75435794  66.41034448 104.95721059\n",
      "  79.74038092  91.80708826 110.44684056  62.24790064  57.61569791\n",
      "  98.12930838  67.83599014  72.12842832 127.85071112  63.30871414\n",
      " 103.69746617 150.0799342   67.41977192  92.3886293  123.67921623\n",
      "  46.91100289  64.66052697 108.2204462  101.13978721 113.37867224]\n",
      "Y Test Actual:  [ 67.54932471  64.19367989 100.89907103 106.80033802  66.5296898\n",
      "  96.80085769 112.75716496  87.08379176  58.86645104  60.75871896\n",
      "  65.76651181  64.14603363  82.80477245  68.55912818 142.45923938\n",
      "  98.00053671  98.36489423  55.40988723  96.73410604  87.25275894\n",
      "  47.97392502  91.14302569  73.57729565  77.46144622  46.97758628\n",
      "  79.38435102  84.7040345   45.4879342   87.55672081  73.49886432\n",
      "  86.90598491  88.25175908 124.63253198  81.43265772  87.55287141\n",
      "  82.8080788   98.72768177  69.0468243   80.62856336  93.08545506\n",
      "  68.40807412  87.26847367  62.04717769  57.33990441  56.01334774\n",
      "  89.31934273 110.55219975 134.61307846  43.26935217  74.47617341\n",
      "  96.21843559  87.89937403  79.21862917  90.71786386 125.8824493\n",
      " 160.38439066  60.50387741  86.87508783  89.5739495   85.50880057\n",
      "  79.97185552  95.54965657  57.09355445  50.35928763  70.86185745\n",
      " 131.24078613  94.50837386  83.15863626  62.93422433  84.09831139\n",
      "  76.26182003  96.48126457  77.07300387  65.77838739  71.76861932\n",
      "  65.14937109  61.0440168   68.74400314  66.41967439 104.95340089\n",
      "  79.75508697  91.80406117 110.44770514  62.24491669  57.60974417\n",
      "  98.13365195  67.82489044  72.14320497 127.85744229  63.30804812\n",
      " 103.70015697 150.07489827  67.42743244  92.3906977  123.66379363\n",
      "  46.92967844  64.66629945 108.22935093 101.14612067 113.38031778]\n",
      "Error Report on Testing _ With best Lambda\n",
      "SGD Learning Rate: 1e-06\n",
      "SGD_Alpha chosen for model:  2.5\n",
      "SGD_Test Normalized Estimation Error:  8.65611542786013e-09\n",
      "SGD_Test NMSE Loss:  1.13553485262833e-08\n",
      "SGD_Test R2 Loss:  0.9999998425650548\n",
      "SGD_Test Correlation:  0.9999999214834854\n",
      "Objective Function Values 1110.0198382297976\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final gradient: 9.583050727844238\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------------------Learning Rate 9e-07 -----------------------------------------\n",
      "Objective Function Value: 3408906.126530059\n",
      "--------------------------------------------------------------BCD iteration 0 --------------------------------------------------------------\n",
      "---------------------------------------------Sep 0 Factor 0 -------------------------------------------------\n",
      "Iteration: 0, Separation Rank: 0, Factor Matrix: 0, Objective Function Value: 4178689.296386482\n",
      "---------------------------------------------Sep 0 Factor 1 -------------------------------------------------\n",
      "Iteration: 0, Separation Rank: 0, Factor Matrix: 1, Objective Function Value: 3488977.6201287964\n",
      "---------------------------------------------Sep 1 Factor 0 -------------------------------------------------\n",
      "Iteration: 0, Separation Rank: 1, Factor Matrix: 0, Objective Function Value: 3092821.460810512\n",
      "---------------------------------------------Sep 1 Factor 1 -------------------------------------------------\n",
      "Iteration: 0, Separation Rank: 1, Factor Matrix: 1, Objective Function Value: 4026588.625912\n",
      "---------------------------------------------Core-------------------------------------------------\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 265386.9375\n",
      "Gradient Norm_Of_Each_Mini_Batch: 195831.3125\n",
      "Gradient Norm_Of_Each_Mini_Batch: 99301.2734375\n",
      "Gradient Norm_Batch: 336249.75\n",
      "9e-07\n",
      "Epoch [1/200], Loss: 94001.2109, Gap to Optimality: 94001.2109, NMSE: 0.2725064754486084, Correlation: 0.8895755106769405, R2: 0.7274935501380988\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 121843.09375\n",
      "Gradient Norm_Of_Each_Mini_Batch: 136822.59375\n",
      "Gradient Norm_Of_Each_Mini_Batch: 29879.7578125\n",
      "Gradient Norm_Batch: 176837.71875\n",
      "9e-07\n",
      "Epoch [2/200], Loss: 29354.2363, Gap to Optimality: 29354.2363, NMSE: 0.08509340137243271, Correlation: 0.9696903930687378, R2: 0.9149066019733806\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 60611.6875\n",
      "Gradient Norm_Of_Each_Mini_Batch: 66176.5\n",
      "Gradient Norm_Of_Each_Mini_Batch: 27113.009765625\n",
      "Gradient Norm_Batch: 100908.6484375\n",
      "9e-07\n",
      "Epoch [3/200], Loss: 10728.8789, Gap to Optimality: 10728.8789, NMSE: 0.031097428873181343, Correlation: 0.9883142706329646, R2: 0.9689025695010738\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 38952.82421875\n",
      "Gradient Norm_Of_Each_Mini_Batch: 36349.65625\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14460.220703125\n",
      "Gradient Norm_Batch: 61527.4609375\n",
      "9e-07\n",
      "Epoch [4/200], Loss: 4392.8403, Gap to Optimality: 4392.8403, NMSE: 0.012728633359074593, Correlation: 0.9949255310858275, R2: 0.9872713661698805\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23449.48046875\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22360.826171875\n",
      "Gradient Norm_Of_Each_Mini_Batch: 9763.240234375\n",
      "Gradient Norm_Batch: 39414.3203125\n",
      "9e-07\n",
      "Epoch [5/200], Loss: 1956.4454, Gap to Optimality: 1956.4454, NMSE: 0.005665148142725229, Correlation: 0.9976159172091092, R2: 0.9943348521041955\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14275.56640625\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14607.7138671875\n",
      "Gradient Norm_Of_Each_Mini_Batch: 7445.9423828125\n",
      "Gradient Norm_Batch: 26265.33203125\n",
      "9e-07\n",
      "Epoch [6/200], Loss: 929.4484, Gap to Optimality: 929.4484, NMSE: 0.0026876546908169985, Correlation: 0.998825445595406, R2: 0.9973123454155197\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 9685.5634765625\n",
      "Gradient Norm_Of_Each_Mini_Batch: 8720.853515625\n",
      "Gradient Norm_Of_Each_Mini_Batch: 5378.7763671875\n",
      "Gradient Norm_Batch: 18038.5390625\n",
      "9e-07\n",
      "Epoch [7/200], Loss: 464.7739, Gap to Optimality: 464.7739, NMSE: 0.0013404205674305558, Correlation: 0.9993975984465803, R2: 0.9986595793832962\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 6793.6328125\n",
      "Gradient Norm_Of_Each_Mini_Batch: 7272.216796875\n",
      "Gradient Norm_Of_Each_Mini_Batch: 2742.80615234375\n",
      "Gradient Norm_Batch: 12648.1357421875\n",
      "9e-07\n",
      "Epoch [8/200], Loss: 240.9210, Gap to Optimality: 240.9210, NMSE: 0.0006913784891366959, Correlation: 0.9996826760496365, R2: 0.9993086214929349\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 5088.6435546875\n",
      "Gradient Norm_Of_Each_Mini_Batch: 4348.2353515625\n",
      "Gradient Norm_Of_Each_Mini_Batch: 2600.450439453125\n",
      "Gradient Norm_Batch: 9040.89453125\n",
      "9e-07\n",
      "Epoch [9/200], Loss: 129.5031, Gap to Optimality: 129.5031, NMSE: 0.00036831776378676295, Correlation: 0.9998284477689658, R2: 0.9996316822222061\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 3954.059326171875\n",
      "Gradient Norm_Of_Each_Mini_Batch: 3043.964599609375\n",
      "Gradient Norm_Of_Each_Mini_Batch: 1697.3819580078125\n",
      "Gradient Norm_Batch: 6561.21728515625\n",
      "9e-07\n",
      "Epoch [10/200], Loss: 71.8937, Gap to Optimality: 71.8937, NMSE: 0.0002012683980865404, Correlation: 0.9999054281661921, R2: 0.9997987315979658\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 2542.4033203125\n",
      "Gradient Norm_Of_Each_Mini_Batch: 2237.74462890625\n",
      "Gradient Norm_Of_Each_Mini_Batch: 1388.50537109375\n",
      "Gradient Norm_Batch: 4827.48486328125\n",
      "9e-07\n",
      "Epoch [11/200], Loss: 41.3723, Gap to Optimality: 41.3723, NMSE: 0.00011275902215857059, Correlation: 0.9999465209109336, R2: 0.9998872409782593\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 2029.77392578125\n",
      "Gradient Norm_Of_Each_Mini_Batch: 1741.4373779296875\n",
      "Gradient Norm_Of_Each_Mini_Batch: 860.2485961914062\n",
      "Gradient Norm_Batch: 3587.48583984375\n",
      "9e-07\n",
      "Epoch [12/200], Loss: 24.6791, Gap to Optimality: 24.6791, NMSE: 6.434621900552884e-05, Correlation: 0.9999692506677924, R2: 0.9999356537755075\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 1333.4881591796875\n",
      "Gradient Norm_Of_Each_Mini_Batch: 1505.5540771484375\n",
      "Gradient Norm_Of_Each_Mini_Batch: 620.7293090820312\n",
      "Gradient Norm_Batch: 2691.10107421875\n",
      "9e-07\n",
      "Epoch [13/200], Loss: 15.3851, Gap to Optimality: 15.3851, NMSE: 3.7388970667961985e-05, Correlation: 0.9999820024424548, R2: 0.9999626110309177\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 1038.0557861328125\n",
      "Gradient Norm_Of_Each_Mini_Batch: 998.9136962890625\n",
      "Gradient Norm_Of_Each_Mini_Batch: 565.1219482421875\n",
      "Gradient Norm_Batch: 2038.733154296875\n",
      "9e-07\n",
      "Epoch [14/200], Loss: 10.1328, Gap to Optimality: 10.1328, NMSE: 2.2153113604872487e-05, Correlation: 0.9999892821231438, R2: 0.999977846886086\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 939.9505004882812\n",
      "Gradient Norm_Of_Each_Mini_Batch: 730.2334594726562\n",
      "Gradient Norm_Of_Each_Mini_Batch: 301.3152160644531\n",
      "Gradient Norm_Batch: 1556.12890625\n",
      "9e-07\n",
      "Epoch [15/200], Loss: 7.0928, Gap to Optimality: 7.0928, NMSE: 1.3333283277461305e-05, Correlation: 0.9999935255687291, R2: 0.9999866667163945\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 532.419677734375\n",
      "Gradient Norm_Of_Each_Mini_Batch: 688.0113525390625\n",
      "Gradient Norm_Of_Each_Mini_Batch: 317.72283935546875\n",
      "Gradient Norm_Batch: 1197.2513427734375\n",
      "9e-07\n",
      "Epoch [16/200], Loss: 5.3136, Gap to Optimality: 5.3136, NMSE: 8.170282853825483e-06, Correlation: 0.9999960099001999, R2: 0.9999918297172207\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 456.8489074707031\n",
      "Gradient Norm_Of_Each_Mini_Batch: 485.8396301269531\n",
      "Gradient Norm_Of_Each_Mini_Batch: 247.67355346679688\n",
      "Gradient Norm_Batch: 927.6449584960938\n",
      "9e-07\n",
      "Epoch [17/200], Loss: 4.2547, Gap to Optimality: 4.2547, NMSE: 5.097374923934694e-06, Correlation: 0.9999975119050638, R2: 0.9999949026246912\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 363.55023193359375\n",
      "Gradient Norm_Of_Each_Mini_Batch: 365.0032043457031\n",
      "Gradient Norm_Of_Each_Mini_Batch: 187.17593383789062\n",
      "Gradient Norm_Batch: 723.8358764648438\n",
      "9e-07\n",
      "Epoch [18/200], Loss: 3.6164, Gap to Optimality: 3.6164, NMSE: 3.2441216717415955e-06, Correlation: 0.9999984107951354, R2: 0.9999967558787173\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 316.704833984375\n",
      "Gradient Norm_Of_Each_Mini_Batch: 284.00286865234375\n",
      "Gradient Norm_Of_Each_Mini_Batch: 112.65956115722656\n",
      "Gradient Norm_Batch: 568.5546264648438\n",
      "9e-07\n",
      "Epoch [19/200], Loss: 3.2259, Gap to Optimality: 3.2259, NMSE: 2.1104715415276587e-06, Correlation: 0.9999989644726106, R2: 0.9999978895286677\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 234.89199829101562\n",
      "Gradient Norm_Of_Each_Mini_Batch: 240.02670288085938\n",
      "Gradient Norm_Of_Each_Mini_Batch: 90.63103485107422\n",
      "Gradient Norm_Batch: 449.0802307128906\n",
      "9e-07\n",
      "Epoch [20/200], Loss: 2.9840, Gap to Optimality: 2.9840, NMSE: 1.4077638752496568e-06, Correlation: 0.999999307626584, R2: 0.9999985922362271\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 186.3519287109375\n",
      "Gradient Norm_Of_Each_Mini_Batch: 174.79666137695312\n",
      "Gradient Norm_Of_Each_Mini_Batch: 69.39700317382812\n",
      "Gradient Norm_Batch: 356.82818603515625\n",
      "9e-07\n",
      "Epoch [21/200], Loss: 2.8331, Gap to Optimality: 2.8331, NMSE: 9.691277682577493e-07, Correlation: 0.9999995223042305, R2: 0.9999990308722794\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 154.9757843017578\n",
      "Gradient Norm_Of_Each_Mini_Batch: 134.62864685058594\n",
      "Gradient Norm_Of_Each_Mini_Batch: 63.13637924194336\n",
      "Gradient Norm_Batch: 284.93792724609375\n",
      "9e-07\n",
      "Epoch [22/200], Loss: 2.7375, Gap to Optimality: 2.7375, NMSE: 6.912384264978755e-07, Correlation: 0.9999996588341588, R2: 0.9999993087616007\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 124.80171966552734\n",
      "Gradient Norm_Of_Each_Mini_Batch: 111.37737274169922\n",
      "Gradient Norm_Of_Each_Mini_Batch: 51.65530776977539\n",
      "Gradient Norm_Batch: 228.4165496826172\n",
      "9e-07\n",
      "Epoch [23/200], Loss: 2.6764, Gap to Optimality: 2.6764, NMSE: 5.136781737746787e-07, Correlation: 0.9999997459572683, R2: 0.999999486321814\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 87.8266830444336\n",
      "Gradient Norm_Of_Each_Mini_Batch: 103.81565856933594\n",
      "Gradient Norm_Of_Each_Mini_Batch: 48.3940315246582\n",
      "Gradient Norm_Batch: 183.7955322265625\n",
      "9e-07\n",
      "Epoch [24/200], Loss: 2.6371, Gap to Optimality: 2.6371, NMSE: 3.9924390193846193e-07, Correlation: 0.9999998024185267, R2: 0.999999600756081\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 83.45394134521484\n",
      "Gradient Norm_Of_Each_Mini_Batch: 72.88109588623047\n",
      "Gradient Norm_Of_Each_Mini_Batch: 34.670623779296875\n",
      "Gradient Norm_Batch: 148.21376037597656\n",
      "9e-07\n",
      "Epoch [25/200], Loss: 2.6116, Gap to Optimality: 2.6116, NMSE: 3.250845850288897e-07, Correlation: 0.9999998386890846, R2: 0.9999996749154192\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 55.57429122924805\n",
      "Gradient Norm_Of_Each_Mini_Batch: 62.4303092956543\n",
      "Gradient Norm_Of_Each_Mini_Batch: 31.88620948791504\n",
      "Gradient Norm_Batch: 119.90946197509766\n",
      "9e-07\n",
      "Epoch [26/200], Loss: 2.5950, Gap to Optimality: 2.5950, NMSE: 2.768042293155304e-07, Correlation: 0.9999998624441573, R2: 0.9999997231957596\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 53.90315246582031\n",
      "Gradient Norm_Of_Each_Mini_Batch: 52.57756423950195\n",
      "Gradient Norm_Of_Each_Mini_Batch: 27.24031639099121\n",
      "Gradient Norm_Batch: 97.71876525878906\n",
      "9e-07\n",
      "Epoch [27/200], Loss: 2.5843, Gap to Optimality: 2.5843, NMSE: 2.4545011001464445e-07, Correlation: 0.999999878062585, R2: 0.9999997545499134\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 46.016666412353516\n",
      "Gradient Norm_Of_Each_Mini_Batch: 45.780006408691406\n",
      "Gradient Norm_Of_Each_Mini_Batch: 24.917072296142578\n",
      "Gradient Norm_Batch: 79.8612289428711\n",
      "9e-07\n",
      "Epoch [28/200], Loss: 2.5772, Gap to Optimality: 2.5772, NMSE: 2.247554249379391e-07, Correlation: 0.9999998882940763, R2: 0.9999997752445606\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 33.770633697509766\n",
      "Gradient Norm_Of_Each_Mini_Batch: 39.3731803894043\n",
      "Gradient Norm_Of_Each_Mini_Batch: 24.787057876586914\n",
      "Gradient Norm_Batch: 65.36711883544922\n",
      "9e-07\n",
      "Epoch [29/200], Loss: 2.5725, Gap to Optimality: 2.5725, NMSE: 2.109740222522305e-07, Correlation: 0.9999998950680213, R2: 0.9999997890259711\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 27.482622146606445\n",
      "Gradient Norm_Of_Each_Mini_Batch: 29.887544631958008\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.66556739807129\n",
      "Gradient Norm_Batch: 54.02750778198242\n",
      "9e-07\n",
      "Epoch [30/200], Loss: 2.5693, Gap to Optimality: 2.5693, NMSE: 2.0190388738683396e-07, Correlation: 0.9999998995693505, R2: 0.9999997980961163\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 28.246538162231445\n",
      "Gradient Norm_Of_Each_Mini_Batch: 26.89078140258789\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.82491683959961\n",
      "Gradient Norm_Batch: 44.51472854614258\n",
      "9e-07\n",
      "Epoch [31/200], Loss: 2.5673, Gap to Optimality: 2.5673, NMSE: 1.9581653987188474e-07, Correlation: 0.9999999025156586, R2: 0.9999998041834828\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.576940536499023\n",
      "Gradient Norm_Of_Each_Mini_Batch: 25.08871841430664\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.125696182250977\n",
      "Gradient Norm_Batch: 36.7294807434082\n",
      "9e-07\n",
      "Epoch [32/200], Loss: 2.5659, Gap to Optimality: 2.5659, NMSE: 1.916960741255025e-07, Correlation: 0.9999999044937056, R2: 0.9999998083039264\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.877178192138672\n",
      "Gradient Norm_Of_Each_Mini_Batch: 26.604537963867188\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.416866302490234\n",
      "Gradient Norm_Batch: 30.98103141784668\n",
      "9e-07\n",
      "Epoch [33/200], Loss: 2.5649, Gap to Optimality: 2.5649, NMSE: 1.8903241993939446e-07, Correlation: 0.999999905836681, R2: 0.9999998109675713\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.2291202545166\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.556741714477539\n",
      "Gradient Norm_Of_Each_Mini_Batch: 27.671220779418945\n",
      "Gradient Norm_Batch: 25.22893714904785\n",
      "9e-07\n",
      "Epoch [34/200], Loss: 2.5643, Gap to Optimality: 2.5643, NMSE: 1.8703995863234013e-07, Correlation: 0.9999999067184054, R2: 0.9999998129600439\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.752546310424805\n",
      "Gradient Norm_Of_Each_Mini_Batch: 26.21932029724121\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.160701751708984\n",
      "Gradient Norm_Batch: 21.540771484375\n",
      "9e-07\n",
      "Epoch [35/200], Loss: 2.5639, Gap to Optimality: 2.5639, NMSE: 1.8581502558845386e-07, Correlation: 0.9999999073352763, R2: 0.9999998141849821\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.592592239379883\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.63459014892578\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.207265853881836\n",
      "Gradient Norm_Batch: 17.704008102416992\n",
      "9e-07\n",
      "Epoch [36/200], Loss: 2.5636, Gap to Optimality: 2.5636, NMSE: 1.849057724712111e-07, Correlation: 0.9999999077318761, R2: 0.9999998150942164\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.961730003356934\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.038331985473633\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.306814193725586\n",
      "Gradient Norm_Batch: 16.46051597595215\n",
      "9e-07\n",
      "Epoch [37/200], Loss: 2.5634, Gap to Optimality: 2.5634, NMSE: 1.844953061436172e-07, Correlation: 0.9999999079969398, R2: 0.9999998155047104\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.327224731445312\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.54978370666504\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.767812728881836\n",
      "Gradient Norm_Batch: 15.088432312011719\n",
      "9e-07\n",
      "Epoch [38/200], Loss: 2.5633, Gap to Optimality: 2.5633, NMSE: 1.8416726277337148e-07, Correlation: 0.9999999081795306, R2: 0.9999998158327502\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.125065803527832\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.261024475097656\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.259578704833984\n",
      "Gradient Norm_Batch: 12.990716934204102\n",
      "9e-07\n",
      "Epoch [39/200], Loss: 2.5632, Gap to Optimality: 2.5632, NMSE: 1.8384280053851398e-07, Correlation: 0.9999999082972045, R2: 0.9999998161572095\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.49712371826172\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.093582153320312\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.68164348602295\n",
      "Gradient Norm_Batch: 12.41238784790039\n",
      "9e-07\n",
      "Epoch [40/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.836914691466518e-07, Correlation: 0.9999999083856633, R2: 0.9999998163085141\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.32810401916504\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.886499404907227\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.34452247619629\n",
      "Gradient Norm_Batch: 12.597793579101562\n",
      "9e-07\n",
      "Epoch [41/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.8363695630796428e-07, Correlation: 0.9999999084497142, R2: 0.9999998163630598\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 13.719645500183105\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.85894775390625\n",
      "Gradient Norm_Of_Each_Mini_Batch: 11.67247486114502\n",
      "Gradient Norm_Batch: 11.52612018585205\n",
      "9e-07\n",
      "Epoch [42/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.8352166364365985e-07, Correlation: 0.9999999084848427, R2: 0.9999998164783566\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.74075698852539\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.79066276550293\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.453262329101562\n",
      "Gradient Norm_Batch: 12.434059143066406\n",
      "9e-07\n",
      "Epoch [43/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.8355059694386e-07, Correlation: 0.999999908506759, R2: 0.9999998164494031\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 12.16256046295166\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.042993545532227\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.469438552856445\n",
      "Gradient Norm_Batch: 10.324125289916992\n",
      "9e-07\n",
      "Epoch [44/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8339474650019838e-07, Correlation: 0.9999999085271375, R2: 0.9999998166052805\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.374526977539062\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.341447830200195\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.894323348999023\n",
      "Gradient Norm_Batch: 10.37244987487793\n",
      "9e-07\n",
      "Epoch [45/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8337357232667273e-07, Correlation: 0.9999999085376944, R2: 0.9999998166264232\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.42913055419922\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.704126358032227\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.555404663085938\n",
      "Gradient Norm_Batch: 9.077170372009277\n",
      "9e-07\n",
      "Epoch [46/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8329059514599066e-07, Correlation: 0.9999999085237496, R2: 0.9999998167094001\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 25.477027893066406\n",
      "Gradient Norm_Of_Each_Mini_Batch: 24.079849243164062\n",
      "Gradient Norm_Of_Each_Mini_Batch: 12.43491268157959\n",
      "Gradient Norm_Batch: 11.048303604125977\n",
      "9e-07\n",
      "Epoch [47/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8340465146593488e-07, Correlation: 0.9999999085503902, R2: 0.999999816595357\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.24549102783203\n",
      "Gradient Norm_Of_Each_Mini_Batch: 25.276884078979492\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.22603988647461\n",
      "Gradient Norm_Batch: 10.604304313659668\n",
      "9e-07\n",
      "Epoch [48/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833741976042802e-07, Correlation: 0.9999999085520627, R2: 0.9999998166257918\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 24.497413635253906\n",
      "Gradient Norm_Of_Each_Mini_Batch: 25.61219596862793\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.487100601196289\n",
      "Gradient Norm_Batch: 12.419316291809082\n",
      "9e-07\n",
      "Epoch [49/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.8348944763602049e-07, Correlation: 0.999999908553113, R2: 0.9999998165105488\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.7683048248291\n",
      "Gradient Norm_Of_Each_Mini_Batch: 27.983749389648438\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.13875961303711\n",
      "Gradient Norm_Batch: 11.190670013427734\n",
      "9e-07\n",
      "Epoch [50/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.834062572925177e-07, Correlation: 0.9999999085546105, R2: 0.9999998165937515\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.25075340270996\n",
      "Gradient Norm_Of_Each_Mini_Batch: 24.135921478271484\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.85392189025879\n",
      "Gradient Norm_Batch: 12.35972785949707\n",
      "9e-07\n",
      "Epoch [51/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.8348906394294318e-07, Correlation: 0.9999999085529542, R2: 0.9999998165109301\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.458158493041992\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.176109313964844\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.160123825073242\n",
      "Gradient Norm_Batch: 11.45041561126709\n",
      "9e-07\n",
      "Epoch [52/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.834184075732992e-07, Correlation: 0.9999999085489558, R2: 0.999999816581596\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.27718162536621\n",
      "Gradient Norm_Of_Each_Mini_Batch: 25.29216766357422\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.02619171142578\n",
      "Gradient Norm_Batch: 11.470041275024414\n",
      "9e-07\n",
      "Epoch [53/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8342443297569844e-07, Correlation: 0.9999999085531148, R2: 0.9999998165755688\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.852319717407227\n",
      "Gradient Norm_Of_Each_Mini_Batch: 27.358938217163086\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.565309524536133\n",
      "Gradient Norm_Batch: 10.624402046203613\n",
      "9e-07\n",
      "Epoch [54/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8338342044899036e-07, Correlation: 0.9999999085525223, R2: 0.9999998166165786\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.64771842956543\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.883825302124023\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.76990509033203\n",
      "Gradient Norm_Batch: 8.955595970153809\n",
      "9e-07\n",
      "Epoch [55/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.832808038670919e-07, Correlation: 0.9999999085564332, R2: 0.9999998167192014\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.549800872802734\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.17580223083496\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.641796112060547\n",
      "Gradient Norm_Batch: 11.083745956420898\n",
      "9e-07\n",
      "Epoch [56/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8335957463477826e-07, Correlation: 0.9999999085721826, R2: 0.999999816640421\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 27.43979263305664\n",
      "Gradient Norm_Of_Each_Mini_Batch: 25.217010498046875\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.827852249145508\n",
      "Gradient Norm_Batch: 14.040215492248535\n",
      "9e-07\n",
      "Epoch [57/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.8356804787345027e-07, Correlation: 0.9999999085717386, R2: 0.9999998164319522\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.709579467773438\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.27581787109375\n",
      "Gradient Norm_Of_Each_Mini_Batch: 11.51195240020752\n",
      "Gradient Norm_Batch: 11.684898376464844\n",
      "9e-07\n",
      "Epoch [58/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8342960572681477e-07, Correlation: 0.9999999085708926, R2: 0.9999998165704054\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.814062118530273\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.72260856628418\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.039390563964844\n",
      "Gradient Norm_Batch: 11.083584785461426\n",
      "9e-07\n",
      "Epoch [59/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833781340110363e-07, Correlation: 0.9999999085656253, R2: 0.99999981662187\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.27704429626465\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.092452049255371\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.929850578308105\n",
      "Gradient Norm_Batch: 8.543597221374512\n",
      "9e-07\n",
      "Epoch [60/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.832395781775631e-07, Correlation: 0.9999999085689578, R2: 0.9999998167604212\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.51849365234375\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.996097564697266\n",
      "Gradient Norm_Of_Each_Mini_Batch: 25.238845825195312\n",
      "Gradient Norm_Batch: 7.767498016357422\n",
      "9e-07\n",
      "Epoch [61/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8319431660529517e-07, Correlation: 0.9999999085410919, R2: 0.9999998168056801\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.28388214111328\n",
      "Gradient Norm_Of_Each_Mini_Batch: 24.39850616455078\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.22309112548828\n",
      "Gradient Norm_Batch: 10.415444374084473\n",
      "9e-07\n",
      "Epoch [62/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8334972651246062e-07, Correlation: 0.999999908560896, R2: 0.9999998166502747\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.955365180969238\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.229352951049805\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.087074279785156\n",
      "Gradient Norm_Batch: 11.690956115722656\n",
      "9e-07\n",
      "Epoch [63/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8343585850288946e-07, Correlation: 0.9999999085653132, R2: 0.9999998165641285\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 24.714462280273438\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.302852630615234\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.323163986206055\n",
      "Gradient Norm_Batch: 10.840991020202637\n",
      "9e-07\n",
      "Epoch [64/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8336716323119617e-07, Correlation: 0.9999999085471214, R2: 0.9999998166328423\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.718902587890625\n",
      "Gradient Norm_Of_Each_Mini_Batch: 26.952770233154297\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.435397148132324\n",
      "Gradient Norm_Batch: 9.165725708007812\n",
      "9e-07\n",
      "Epoch [65/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833027738484816e-07, Correlation: 0.9999999085409751, R2: 0.9999998166972134\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 11.891677856445312\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.239389419555664\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.978489875793457\n",
      "Gradient Norm_Batch: 11.084539413452148\n",
      "9e-07\n",
      "Epoch [66/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8339333962558158e-07, Correlation: 0.9999999085563542, R2: 0.9999998166066612\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 13.793500900268555\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.776622772216797\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.727886199951172\n",
      "Gradient Norm_Batch: 10.193222045898438\n",
      "9e-07\n",
      "Epoch [67/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8334021945065615e-07, Correlation: 0.9999999085625783, R2: 0.9999998166597572\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.268741607666016\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.584867477416992\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.24562644958496\n",
      "Gradient Norm_Batch: 7.626896858215332\n",
      "9e-07\n",
      "Epoch [68/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.831889164805034e-07, Correlation: 0.999999908558699, R2: 0.9999998168110674\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.5590763092041\n",
      "Gradient Norm_Of_Each_Mini_Batch: 28.699066162109375\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.64369010925293\n",
      "Gradient Norm_Batch: 9.570073127746582\n",
      "9e-07\n",
      "Epoch [69/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8328142914469936e-07, Correlation: 0.9999999085652437, R2: 0.999999816718554\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 26.904502868652344\n",
      "Gradient Norm_Of_Each_Mini_Batch: 30.406360626220703\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.34922218322754\n",
      "Gradient Norm_Batch: 11.310815811157227\n",
      "9e-07\n",
      "Epoch [70/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8338278096052818e-07, Correlation: 0.9999999085270825, R2: 0.9999998166172311\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.804271697998047\n",
      "Gradient Norm_Of_Each_Mini_Batch: 24.778079986572266\n",
      "Gradient Norm_Of_Each_Mini_Batch: 10.257218360900879\n",
      "Gradient Norm_Batch: 11.75994873046875\n",
      "9e-07\n",
      "Epoch [71/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8344128704939067e-07, Correlation: 0.9999999085585538, R2: 0.9999998165587096\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 9.981943130493164\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.0069522857666\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.799943923950195\n",
      "Gradient Norm_Batch: 11.37397575378418\n",
      "9e-07\n",
      "Epoch [72/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8342007024330087e-07, Correlation: 0.9999999085558122, R2: 0.9999998165799246\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.775936126708984\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.486364364624023\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.95786190032959\n",
      "Gradient Norm_Batch: 11.578557014465332\n",
      "9e-07\n",
      "Epoch [73/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8341791019338416e-07, Correlation: 0.9999999085455002, R2: 0.999999816582098\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.315895080566406\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.338056564331055\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.770081520080566\n",
      "Gradient Norm_Batch: 11.678885459899902\n",
      "9e-07\n",
      "Epoch [74/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.834260814348454e-07, Correlation: 0.9999999085734765, R2: 0.9999998165739173\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.13968276977539\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.386999130249023\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.888145446777344\n",
      "Gradient Norm_Batch: 9.002391815185547\n",
      "9e-07\n",
      "Epoch [75/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8326876727314811e-07, Correlation: 0.9999999085621049, R2: 0.9999998167312346\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.032381057739258\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.723417282104492\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.846711158752441\n",
      "Gradient Norm_Batch: 11.825101852416992\n",
      "9e-07\n",
      "Epoch [76/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8341120266995858e-07, Correlation: 0.99999990856886, R2: 0.9999998165887896\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.764400482177734\n",
      "Gradient Norm_Of_Each_Mini_Batch: 26.139238357543945\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.918720245361328\n",
      "Gradient Norm_Batch: 12.060416221618652\n",
      "9e-07\n",
      "Epoch [77/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8344923091717646e-07, Correlation: 0.9999999085671948, R2: 0.9999998165507622\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.383174896240234\n",
      "Gradient Norm_Of_Each_Mini_Batch: 25.100177764892578\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.167158126831055\n",
      "Gradient Norm_Batch: 11.518651962280273\n",
      "9e-07\n",
      "Epoch [78/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8341300744850741e-07, Correlation: 0.9999999085458511, R2: 0.9999998165869721\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.30857276916504\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.95819091796875\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.309078216552734\n",
      "Gradient Norm_Batch: 11.75340747833252\n",
      "9e-07\n",
      "Epoch [79/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8344614716170327e-07, Correlation: 0.9999999085579409, R2: 0.9999998165538547\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.35042953491211\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.084993362426758\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.357521057128906\n",
      "Gradient Norm_Batch: 10.998920440673828\n",
      "9e-07\n",
      "Epoch [80/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8337161122872203e-07, Correlation: 0.999999908571754, R2: 0.9999998166283995\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.082996368408203\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.67641830444336\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.477304458618164\n",
      "Gradient Norm_Batch: 13.48619556427002\n",
      "9e-07\n",
      "Epoch [81/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.835139613604042e-07, Correlation: 0.9999999085754531, R2: 0.9999998164860535\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.44980239868164\n",
      "Gradient Norm_Of_Each_Mini_Batch: 26.15415382385254\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.100129127502441\n",
      "Gradient Norm_Batch: 10.613482475280762\n",
      "9e-07\n",
      "Epoch [82/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8337306073590298e-07, Correlation: 0.9999999085595475, R2: 0.999999816626948\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.863481521606445\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.1281681060791\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.298127174377441\n",
      "Gradient Norm_Batch: 11.717669486999512\n",
      "9e-07\n",
      "Epoch [83/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8343463636938395e-07, Correlation: 0.9999999085591534, R2: 0.9999998165653716\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 12.57614517211914\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.95720863342285\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.010793685913086\n",
      "Gradient Norm_Batch: 11.407561302185059\n",
      "9e-07\n",
      "Epoch [84/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8341333429816586e-07, Correlation: 0.9999999085694518, R2: 0.9999998165866614\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.935022354125977\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.844907760620117\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.92354965209961\n",
      "Gradient Norm_Batch: 11.858600616455078\n",
      "9e-07\n",
      "Epoch [85/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8344647401136172e-07, Correlation: 0.9999999085659933, R2: 0.9999998165535299\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.44522476196289\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.68303108215332\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.241727828979492\n",
      "Gradient Norm_Batch: 11.736056327819824\n",
      "9e-07\n",
      "Epoch [86/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8341602014970704e-07, Correlation: 0.9999999085667742, R2: 0.999999816583984\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.85242462158203\n",
      "Gradient Norm_Of_Each_Mini_Batch: 29.01837730407715\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.895771026611328\n",
      "Gradient Norm_Batch: 13.849233627319336\n",
      "9e-07\n",
      "Epoch [87/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.8357479802944e-07, Correlation: 0.9999999085724268, R2: 0.9999998164252263\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.552072525024414\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.135866165161133\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.638961791992188\n",
      "Gradient Norm_Batch: 11.304434776306152\n",
      "9e-07\n",
      "Epoch [88/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.834216050156101e-07, Correlation: 0.9999999085533865, R2: 0.999999816578388\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.7133731842041\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.077857971191406\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.974859237670898\n",
      "Gradient Norm_Batch: 10.930214881896973\n",
      "9e-07\n",
      "Epoch [89/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8338290885822062e-07, Correlation: 0.9999999085690924, R2: 0.9999998166170891\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.497364044189453\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.446908950805664\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.37260627746582\n",
      "Gradient Norm_Batch: 10.255624771118164\n",
      "9e-07\n",
      "Epoch [90/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833390541605695e-07, Correlation: 0.9999999085492426, R2: 0.9999998166609552\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.10852813720703\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.505891799926758\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.67325782775879\n",
      "Gradient Norm_Batch: 10.368348121643066\n",
      "9e-07\n",
      "Epoch [91/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8331672890781192e-07, Correlation: 0.9999999085792557, R2: 0.9999998166832847\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.176321983337402\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.137800216674805\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.680228233337402\n",
      "Gradient Norm_Batch: 12.048617362976074\n",
      "9e-07\n",
      "Epoch [92/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8342164764817426e-07, Correlation: 0.9999999085804377, R2: 0.9999998165783497\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.603208541870117\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.10309410095215\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.55111312866211\n",
      "Gradient Norm_Batch: 10.040044784545898\n",
      "9e-07\n",
      "Epoch [93/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8333116713620257e-07, Correlation: 0.9999999085631829, R2: 0.9999998166688296\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.35967254638672\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.86390495300293\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.571657180786133\n",
      "Gradient Norm_Batch: 10.363908767700195\n",
      "9e-07\n",
      "Epoch [94/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833375478099697e-07, Correlation: 0.999999908569731, R2: 0.999999816662456\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.99695587158203\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.911325454711914\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.35517692565918\n",
      "Gradient Norm_Batch: 10.1190185546875\n",
      "9e-07\n",
      "Epoch [95/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8333659568270377e-07, Correlation: 0.9999999085435288, R2: 0.9999998166633943\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.067428588867188\n",
      "Gradient Norm_Of_Each_Mini_Batch: 26.636369705200195\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.868791580200195\n",
      "Gradient Norm_Batch: 9.679378509521484\n",
      "9e-07\n",
      "Epoch [96/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8330788975617907e-07, Correlation: 0.9999999085457147, R2: 0.9999998166921199\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.08869171142578\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.729036331176758\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.619248390197754\n",
      "Gradient Norm_Batch: 9.202279090881348\n",
      "9e-07\n",
      "Epoch [97/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8329609474676545e-07, Correlation: 0.9999999085573951, R2: 0.9999998167039053\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.069622039794922\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.55874252319336\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.532087326049805\n",
      "Gradient Norm_Batch: 11.980711936950684\n",
      "9e-07\n",
      "Epoch [98/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8343793328767788e-07, Correlation: 0.9999999085735558, R2: 0.9999998165620618\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.4459285736084\n",
      "Gradient Norm_Of_Each_Mini_Batch: 24.861421585083008\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.502180099487305\n",
      "Gradient Norm_Batch: 11.494636535644531\n",
      "9e-07\n",
      "Epoch [99/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8341287955081498e-07, Correlation: 0.9999999085666892, R2: 0.999999816587117\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.970706939697266\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.42414665222168\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.367786407470703\n",
      "Gradient Norm_Batch: 11.465782165527344\n",
      "9e-07\n",
      "Epoch [100/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8339899554575823e-07, Correlation: 0.9999999085563909, R2: 0.9999998166010091\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.708066940307617\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.25546646118164\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.682464599609375\n",
      "Gradient Norm_Batch: 10.741204261779785\n",
      "9e-07\n",
      "Epoch [101/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8332721651859174e-07, Correlation: 0.9999999085463331, R2: 0.9999998166727577\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.122756958007812\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.85041618347168\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.24836540222168\n",
      "Gradient Norm_Batch: 10.53141975402832\n",
      "9e-07\n",
      "Epoch [102/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8335029494664923e-07, Correlation: 0.9999999085466957, R2: 0.9999998166497241\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.440771102905273\n",
      "Gradient Norm_Of_Each_Mini_Batch: 24.641311645507812\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.634449005126953\n",
      "Gradient Norm_Batch: 9.343276023864746\n",
      "9e-07\n",
      "Epoch [103/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8328761086650047e-07, Correlation: 0.9999999085550251, R2: 0.9999998167123938\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 12.311777114868164\n",
      "Gradient Norm_Of_Each_Mini_Batch: 13.480846405029297\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.806488037109375\n",
      "Gradient Norm_Batch: 8.571507453918457\n",
      "9e-07\n",
      "Epoch [104/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8326521455946931e-07, Correlation: 0.9999999085472804, R2: 0.9999998167347833\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.09073257446289\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.363908767700195\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.500316619873047\n",
      "Gradient Norm_Batch: 10.305063247680664\n",
      "9e-07\n",
      "Epoch [105/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8337851770411362e-07, Correlation: 0.9999999085437797, R2: 0.9999998166214701\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 25.233842849731445\n",
      "Gradient Norm_Of_Each_Mini_Batch: 26.668582916259766\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.000537872314453\n",
      "Gradient Norm_Batch: 8.181379318237305\n",
      "9e-07\n",
      "Epoch [106/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8324907102851284e-07, Correlation: 0.9999999085292222, R2: 0.999999816750904\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.81391143798828\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.20397186279297\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.47466278076172\n",
      "Gradient Norm_Batch: 8.461380958557129\n",
      "9e-07\n",
      "Epoch [107/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8325965811527567e-07, Correlation: 0.9999999085330867, R2: 0.999999816740355\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.30558204650879\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.470266342163086\n",
      "Gradient Norm_Of_Each_Mini_Batch: 12.591634750366211\n",
      "Gradient Norm_Batch: 10.242769241333008\n",
      "9e-07\n",
      "Epoch [108/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833459748468158e-07, Correlation: 0.9999999085621315, R2: 0.9999998166540383\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.196779251098633\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.853303909301758\n",
      "Gradient Norm_Of_Each_Mini_Batch: 13.670523643493652\n",
      "Gradient Norm_Batch: 10.162732124328613\n",
      "9e-07\n",
      "Epoch [109/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833471827694666e-07, Correlation: 0.9999999085509761, R2: 0.9999998166528158\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.113479614257812\n",
      "Gradient Norm_Of_Each_Mini_Batch: 26.09808349609375\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.197704315185547\n",
      "Gradient Norm_Batch: 11.426565170288086\n",
      "9e-07\n",
      "Epoch [110/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8339235907660623e-07, Correlation: 0.9999999085663319, R2: 0.999999816607647\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.14113998413086\n",
      "Gradient Norm_Of_Each_Mini_Batch: 24.183252334594727\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.129682540893555\n",
      "Gradient Norm_Batch: 8.219103813171387\n",
      "9e-07\n",
      "Epoch [111/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8324195139030053e-07, Correlation: 0.9999999085538864, R2: 0.9999998167580602\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.659591674804688\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.836191177368164\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.272193908691406\n",
      "Gradient Norm_Batch: 10.764811515808105\n",
      "9e-07\n",
      "Epoch [112/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833544729379355e-07, Correlation: 0.9999999085682146, R2: 0.9999998166455251\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.278776168823242\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.404518127441406\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.856599807739258\n",
      "Gradient Norm_Batch: 11.573076248168945\n",
      "9e-07\n",
      "Epoch [113/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.834129079725244e-07, Correlation: 0.9999999085721761, R2: 0.9999998165870808\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.030284881591797\n",
      "Gradient Norm_Of_Each_Mini_Batch: 26.068010330200195\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.528219223022461\n",
      "Gradient Norm_Batch: 11.25164794921875\n",
      "9e-07\n",
      "Epoch [114/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8338565155318065e-07, Correlation: 0.9999999085757271, R2: 0.999999816614355\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.78261947631836\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.569297790527344\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.53849983215332\n",
      "Gradient Norm_Batch: 11.47952938079834\n",
      "9e-07\n",
      "Epoch [115/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.834004592637939e-07, Correlation: 0.9999999085759239, R2: 0.9999998165995371\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.21969223022461\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.264225006103516\n",
      "Gradient Norm_Of_Each_Mini_Batch: 9.225726127624512\n",
      "Gradient Norm_Batch: 12.063958168029785\n",
      "9e-07\n",
      "Epoch [116/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8344324814734136e-07, Correlation: 0.999999908579121, R2: 0.9999998165567371\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.56792640686035\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.770845413208008\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.19647216796875\n",
      "Gradient Norm_Batch: 10.769064903259277\n",
      "9e-07\n",
      "Epoch [117/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8336679374897358e-07, Correlation: 0.9999999085652548, R2: 0.9999998166332182\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.778745651245117\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.162439346313477\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.109406471252441\n",
      "Gradient Norm_Batch: 10.492630958557129\n",
      "9e-07\n",
      "Epoch [118/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8334385742946324e-07, Correlation: 0.9999999085616676, R2: 0.9999998166561463\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.080289840698242\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.05303955078125\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.444316864013672\n",
      "Gradient Norm_Batch: 10.02445125579834\n",
      "9e-07\n",
      "Epoch [119/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.83324459612777e-07, Correlation: 0.9999999085659749, R2: 0.9999998166755439\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.499982833862305\n",
      "Gradient Norm_Of_Each_Mini_Batch: 28.271146774291992\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.256254196166992\n",
      "Gradient Norm_Batch: 11.868395805358887\n",
      "9e-07\n",
      "Epoch [120/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8342372243296268e-07, Correlation: 0.9999999085757028, R2: 0.9999998165762872\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.041048049926758\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.255516052246094\n",
      "Gradient Norm_Of_Each_Mini_Batch: 12.666672706604004\n",
      "Gradient Norm_Batch: 12.202441215515137\n",
      "9e-07\n",
      "Epoch [121/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8344182706186984e-07, Correlation: 0.999999908564137, R2: 0.9999998165581782\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.377519607543945\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.044143676757812\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.470542907714844\n",
      "Gradient Norm_Batch: 11.231758117675781\n",
      "9e-07\n",
      "Epoch [122/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8338562313147122e-07, Correlation: 0.9999999085537193, R2: 0.9999998166143768\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 13.614338874816895\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.00050926208496\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.91990089416504\n",
      "Gradient Norm_Batch: 9.872783660888672\n",
      "9e-07\n",
      "Epoch [123/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8334104368022963e-07, Correlation: 0.9999999085519683, R2: 0.9999998166589661\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.90731430053711\n",
      "Gradient Norm_Of_Each_Mini_Batch: 24.33586311340332\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.936727523803711\n",
      "Gradient Norm_Batch: 10.731101989746094\n",
      "9e-07\n",
      "Epoch [124/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8338873530865385e-07, Correlation: 0.9999999085536001, R2: 0.9999998166112715\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 13.881056785583496\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.309154510498047\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.69306755065918\n",
      "Gradient Norm_Batch: 9.955094337463379\n",
      "9e-07\n",
      "Epoch [125/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8332600859594095e-07, Correlation: 0.999999908566613, R2: 0.9999998166740033\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.585640907287598\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.48755645751953\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.552986145019531\n",
      "Gradient Norm_Batch: 9.241581916809082\n",
      "9e-07\n",
      "Epoch [126/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8329974693642725e-07, Correlation: 0.9999999085477601, R2: 0.9999998167002385\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.74140739440918\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.052364349365234\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.42152976989746\n",
      "Gradient Norm_Batch: 10.095428466796875\n",
      "9e-07\n",
      "Epoch [127/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8335117601964157e-07, Correlation: 0.9999999085493682, R2: 0.9999998166488259\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.461469650268555\n",
      "Gradient Norm_Of_Each_Mini_Batch: 28.469371795654297\n",
      "Gradient Norm_Of_Each_Mini_Batch: 13.836038589477539\n",
      "Gradient Norm_Batch: 12.542922973632812\n",
      "9e-07\n",
      "Epoch [128/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.8346709396155347e-07, Correlation: 0.9999999085650175, R2: 0.9999998165328973\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.722576141357422\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.55131721496582\n",
      "Gradient Norm_Of_Each_Mini_Batch: 12.71969223022461\n",
      "Gradient Norm_Batch: 12.871637344360352\n",
      "9e-07\n",
      "Epoch [129/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.834855396509738e-07, Correlation: 0.9999999085759603, R2: 0.9999998165144535\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.999893188476562\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.468708038330078\n",
      "Gradient Norm_Of_Each_Mini_Batch: 24.105260848999023\n",
      "Gradient Norm_Batch: 10.46536922454834\n",
      "9e-07\n",
      "Epoch [130/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8336417895170598e-07, Correlation: 0.9999999085592367, R2: 0.9999998166358319\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.279083251953125\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.772266387939453\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.603015899658203\n",
      "Gradient Norm_Batch: 13.364459991455078\n",
      "9e-07\n",
      "Epoch [131/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.835407772432518e-07, Correlation: 0.9999999085670277, R2: 0.9999998164592293\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 27.10612678527832\n",
      "Gradient Norm_Of_Each_Mini_Batch: 28.46937370300293\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.383956909179688\n",
      "Gradient Norm_Batch: 15.296671867370605\n",
      "9e-07\n",
      "Epoch [132/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.8366841914030374e-07, Correlation: 0.9999999085779372, R2: 0.9999998163315754\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.549348831176758\n",
      "Gradient Norm_Of_Each_Mini_Batch: 12.89250659942627\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.35963249206543\n",
      "Gradient Norm_Batch: 12.977134704589844\n",
      "9e-07\n",
      "Epoch [133/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.835045964071469e-07, Correlation: 0.9999999085636407, R2: 0.9999998164954057\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.552274703979492\n",
      "Gradient Norm_Of_Each_Mini_Batch: 24.755876541137695\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.919322967529297\n",
      "Gradient Norm_Batch: 10.952859878540039\n",
      "9e-07\n",
      "Epoch [134/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8339788709909044e-07, Correlation: 0.9999999085649665, R2: 0.9999998166020982\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.242450714111328\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.610994338989258\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.484969139099121\n",
      "Gradient Norm_Batch: 10.579998970031738\n",
      "9e-07\n",
      "Epoch [135/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8337156859615789e-07, Correlation: 0.9999999085604107, R2: 0.9999998166284401\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.208084106445312\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.618322372436523\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.088558197021484\n",
      "Gradient Norm_Batch: 10.020367622375488\n",
      "9e-07\n",
      "Epoch [136/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8334506535211403e-07, Correlation: 0.9999999085578475, R2: 0.9999998166549308\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.93492317199707\n",
      "Gradient Norm_Of_Each_Mini_Batch: 27.52060890197754\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.152433395385742\n",
      "Gradient Norm_Batch: 10.889772415161133\n",
      "9e-07\n",
      "Epoch [137/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8340089980029006e-07, Correlation: 0.9999999085463842, R2: 0.9999998165991073\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.86471176147461\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.810523986816406\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.02843189239502\n",
      "Gradient Norm_Batch: 11.57794189453125\n",
      "9e-07\n",
      "Epoch [138/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8341603436056175e-07, Correlation: 0.9999999085612135, R2: 0.9999998165839707\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.377012252807617\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.991100311279297\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.502217292785645\n",
      "Gradient Norm_Batch: 11.32593822479248\n",
      "9e-07\n",
      "Epoch [139/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8341353325013188e-07, Correlation: 0.9999999085662861, R2: 0.9999998165864634\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.82005500793457\n",
      "Gradient Norm_Of_Each_Mini_Batch: 27.41453742980957\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.12615394592285\n",
      "Gradient Norm_Batch: 9.63356876373291\n",
      "9e-07\n",
      "Epoch [140/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8331037665575423e-07, Correlation: 0.9999999085652616, R2: 0.9999998166896266\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.345333099365234\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.28531837463379\n",
      "Gradient Norm_Of_Each_Mini_Batch: 24.480764389038086\n",
      "Gradient Norm_Batch: 12.383077621459961\n",
      "9e-07\n",
      "Epoch [141/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8345284047427413e-07, Correlation: 0.9999999085723149, R2: 0.999999816547152\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 24.505725860595703\n",
      "Gradient Norm_Of_Each_Mini_Batch: 30.17698097229004\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.142114639282227\n",
      "Gradient Norm_Batch: 11.995065689086914\n",
      "9e-07\n",
      "Epoch [142/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8344493923905247e-07, Correlation: 0.9999999085589898, R2: 0.9999998165550489\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.17095947265625\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.601579666137695\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.283584594726562\n",
      "Gradient Norm_Batch: 11.208288192749023\n",
      "9e-07\n",
      "Epoch [143/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8340648466619314e-07, Correlation: 0.9999999085649616, R2: 0.9999998165934909\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 13.726061820983887\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.26204490661621\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.80797576904297\n",
      "Gradient Norm_Batch: 11.331890106201172\n",
      "9e-07\n",
      "Epoch [144/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8339937923883554e-07, Correlation: 0.9999999085675378, R2: 0.9999998166006137\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.36200714111328\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.46784782409668\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.485980987548828\n",
      "Gradient Norm_Batch: 12.44265079498291\n",
      "9e-07\n",
      "Epoch [145/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.8344886143495387e-07, Correlation: 0.9999999085621704, R2: 0.9999998165511214\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.70073699951172\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.69671630859375\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.933073997497559\n",
      "Gradient Norm_Batch: 12.159614562988281\n",
      "9e-07\n",
      "Epoch [146/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.8345872376812622e-07, Correlation: 0.9999999085679044, R2: 0.9999998165412735\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.57705307006836\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.141429901123047\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.208386421203613\n",
      "Gradient Norm_Batch: 12.711750984191895\n",
      "9e-07\n",
      "Epoch [147/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.8351150288253848e-07, Correlation: 0.9999999085686992, R2: 0.9999998164885088\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 12.769428253173828\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.907258987426758\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.99289894104004\n",
      "Gradient Norm_Batch: 10.488466262817383\n",
      "9e-07\n",
      "Epoch [148/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833607399248649e-07, Correlation: 0.9999999085667542, R2: 0.9999998166392636\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.092164993286133\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.692649841308594\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.628573417663574\n",
      "Gradient Norm_Batch: 11.28353500366211\n",
      "9e-07\n",
      "Epoch [149/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8339351015583816e-07, Correlation: 0.9999999085620787, R2: 0.999999816606494\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.9060001373291\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.5866756439209\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.057567596435547\n",
      "Gradient Norm_Batch: 9.043729782104492\n",
      "9e-07\n",
      "Epoch [150/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.832673461876766e-07, Correlation: 0.9999999085633298, R2: 0.9999998167326576\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 26.803083419799805\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.151466369628906\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.838109016418457\n",
      "Gradient Norm_Batch: 10.052695274353027\n",
      "9e-07\n",
      "Epoch [151/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8330572970626235e-07, Correlation: 0.9999999085567817, R2: 0.9999998166942642\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.965287208557129\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.049016952514648\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.086349487304688\n",
      "Gradient Norm_Batch: 10.96458911895752\n",
      "9e-07\n",
      "Epoch [152/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8338108986881707e-07, Correlation: 0.9999999085661196, R2: 0.9999998166189151\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 26.937623977661133\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.968984603881836\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.592119216918945\n",
      "Gradient Norm_Batch: 10.235790252685547\n",
      "9e-07\n",
      "Epoch [153/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.83318022095591e-07, Correlation: 0.9999999085220619, R2: 0.9999998166819674\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.89287757873535\n",
      "Gradient Norm_Of_Each_Mini_Batch: 25.33906364440918\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.094539642333984\n",
      "Gradient Norm_Batch: 9.096256256103516\n",
      "9e-07\n",
      "Epoch [154/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8327628481529246e-07, Correlation: 0.9999999085609719, R2: 0.999999816723699\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.077877044677734\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.407812118530273\n",
      "Gradient Norm_Of_Each_Mini_Batch: 13.198468208312988\n",
      "Gradient Norm_Batch: 10.410171508789062\n",
      "9e-07\n",
      "Epoch [155/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8334462481561786e-07, Correlation: 0.999999908569723, R2: 0.9999998166553891\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 12.209895133972168\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.59269905090332\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.313399314880371\n",
      "Gradient Norm_Batch: 9.604208946228027\n",
      "9e-07\n",
      "Epoch [156/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8329953377360653e-07, Correlation: 0.9999999085669024, R2: 0.9999998167004681\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.291370391845703\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.10211181640625\n",
      "Gradient Norm_Of_Each_Mini_Batch: 13.12305736541748\n",
      "Gradient Norm_Batch: 9.927079200744629\n",
      "9e-07\n",
      "Epoch [157/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833148388641348e-07, Correlation: 0.9999999085666004, R2: 0.9999998166851571\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.316713333129883\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.943113327026367\n",
      "Gradient Norm_Of_Each_Mini_Batch: 13.067647933959961\n",
      "Gradient Norm_Batch: 10.765966415405273\n",
      "9e-07\n",
      "Epoch [158/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8336780271965836e-07, Correlation: 0.999999908569934, R2: 0.9999998166321882\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.162477493286133\n",
      "Gradient Norm_Of_Each_Mini_Batch: 28.680776596069336\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.226299285888672\n",
      "Gradient Norm_Batch: 9.941513061523438\n",
      "9e-07\n",
      "Epoch [159/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8333497564526624e-07, Correlation: 0.9999999085583229, R2: 0.9999998166650172\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 26.298912048339844\n",
      "Gradient Norm_Of_Each_Mini_Batch: 30.162456512451172\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.03611946105957\n",
      "Gradient Norm_Batch: 11.313332557678223\n",
      "9e-07\n",
      "Epoch [160/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.83393268571308e-07, Correlation: 0.999999908525369, R2: 0.9999998166067164\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.272443771362305\n",
      "Gradient Norm_Of_Each_Mini_Batch: 24.269805908203125\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.401073455810547\n",
      "Gradient Norm_Batch: 11.96955394744873\n",
      "9e-07\n",
      "Epoch [161/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.8347606101087877e-07, Correlation: 0.9999999085577773, R2: 0.9999998165239313\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.4732666015625\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.535070419311523\n",
      "Gradient Norm_Of_Each_Mini_Batch: 12.770604133605957\n",
      "Gradient Norm_Batch: 12.171279907226562\n",
      "9e-07\n",
      "Epoch [162/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.834833796010571e-07, Correlation: 0.9999999085617197, R2: 0.9999998165166257\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 26.588590621948242\n",
      "Gradient Norm_Of_Each_Mini_Batch: 30.032346725463867\n",
      "Gradient Norm_Of_Each_Mini_Batch: 29.424861907958984\n",
      "Gradient Norm_Batch: 13.921025276184082\n",
      "9e-07\n",
      "Epoch [163/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.835725669252497e-07, Correlation: 0.9999999085668619, R2: 0.9999998164274352\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.785625457763672\n",
      "Gradient Norm_Of_Each_Mini_Batch: 25.0893611907959\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.707508087158203\n",
      "Gradient Norm_Batch: 10.371308326721191\n",
      "9e-07\n",
      "Epoch [164/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833582103927256e-07, Correlation: 0.9999999085591704, R2: 0.9999998166417966\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.349576950073242\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.55060386657715\n",
      "Gradient Norm_Of_Each_Mini_Batch: 11.493300437927246\n",
      "Gradient Norm_Batch: 11.268147468566895\n",
      "9e-07\n",
      "Epoch [165/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8341491170303925e-07, Correlation: 0.9999999085585146, R2: 0.9999998165850821\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 13.636693954467773\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.823264122009277\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.1332950592041\n",
      "Gradient Norm_Batch: 11.062700271606445\n",
      "9e-07\n",
      "Epoch [166/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.83397759201398e-07, Correlation: 0.9999999085495312, R2: 0.9999998166022436\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 26.511695861816406\n",
      "Gradient Norm_Of_Each_Mini_Batch: 28.61301040649414\n",
      "Gradient Norm_Of_Each_Mini_Batch: 13.58035659790039\n",
      "Gradient Norm_Batch: 10.409997940063477\n",
      "9e-07\n",
      "Epoch [167/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8335413187742233e-07, Correlation: 0.9999999085448635, R2: 0.9999998166458504\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.423660278320312\n",
      "Gradient Norm_Of_Each_Mini_Batch: 25.710298538208008\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.256267547607422\n",
      "Gradient Norm_Batch: 12.363067626953125\n",
      "9e-07\n",
      "Epoch [168/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.8348913499721675e-07, Correlation: 0.9999999085619259, R2: 0.9999998165108586\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.677938461303711\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.95209312438965\n",
      "Gradient Norm_Of_Each_Mini_Batch: 13.896313667297363\n",
      "Gradient Norm_Batch: 10.520000457763672\n",
      "9e-07\n",
      "Epoch [169/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833632552461495e-07, Correlation: 0.9999999085663854, R2: 0.9999998166367372\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.102606773376465\n",
      "Gradient Norm_Of_Each_Mini_Batch: 11.89664077758789\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.327178955078125\n",
      "Gradient Norm_Batch: 10.890050888061523\n",
      "9e-07\n",
      "Epoch [170/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833750360447084e-07, Correlation: 0.9999999085611339, R2: 0.9999998166249495\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.886035919189453\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.29859161376953\n",
      "Gradient Norm_Of_Each_Mini_Batch: 24.13136863708496\n",
      "Gradient Norm_Batch: 11.997876167297363\n",
      "9e-07\n",
      "Epoch [171/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8343098417972215e-07, Correlation: 0.9999999085737253, R2: 0.9999998165690114\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 12.6306734085083\n",
      "Gradient Norm_Of_Each_Mini_Batch: 12.405257225036621\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.114072799682617\n",
      "Gradient Norm_Batch: 10.189550399780273\n",
      "9e-07\n",
      "Epoch [172/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8333884099774878e-07, Correlation: 0.9999999085636978, R2: 0.9999998166611593\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 10.929844856262207\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.071529388427734\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.1416015625\n",
      "Gradient Norm_Batch: 9.70849895477295\n",
      "9e-07\n",
      "Epoch [173/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8331762419165898e-07, Correlation: 0.9999999085653728, R2: 0.9999998166823629\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.165843963623047\n",
      "Gradient Norm_Of_Each_Mini_Batch: 26.372373580932617\n",
      "Gradient Norm_Of_Each_Mini_Batch: 13.40039348602295\n",
      "Gradient Norm_Batch: 10.362421035766602\n",
      "9e-07\n",
      "Epoch [174/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8333922469082609e-07, Correlation: 0.9999999085474168, R2: 0.9999998166607766\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 24.747825622558594\n",
      "Gradient Norm_Of_Each_Mini_Batch: 29.443632125854492\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.83568572998047\n",
      "Gradient Norm_Batch: 12.29991626739502\n",
      "9e-07\n",
      "Epoch [175/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.8345660635077365e-07, Correlation: 0.9999999085463207, R2: 0.9999998165433959\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.37215232849121\n",
      "Gradient Norm_Of_Each_Mini_Batch: 24.101240158081055\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.6171932220459\n",
      "Gradient Norm_Batch: 10.406216621398926\n",
      "9e-07\n",
      "Epoch [176/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8335296658733569e-07, Correlation: 0.9999999085611762, R2: 0.9999998166470391\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.37930679321289\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.750691413879395\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.84075164794922\n",
      "Gradient Norm_Batch: 11.62760066986084\n",
      "9e-07\n",
      "Epoch [177/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.834190612726161e-07, Correlation: 0.9999999085627208, R2: 0.9999998165809345\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.68386459350586\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.758197784423828\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.633106231689453\n",
      "Gradient Norm_Batch: 13.233705520629883\n",
      "9e-07\n",
      "Epoch [178/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.8353205177845666e-07, Correlation: 0.9999999085680538, R2: 0.9999998164679478\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.166133880615234\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.11589241027832\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.055831909179688\n",
      "Gradient Norm_Batch: 9.628238677978516\n",
      "9e-07\n",
      "Epoch [179/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833072644785716e-07, Correlation: 0.9999999085652869, R2: 0.9999998166927343\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.670198440551758\n",
      "Gradient Norm_Of_Each_Mini_Batch: 25.576875686645508\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.528280258178711\n",
      "Gradient Norm_Batch: 11.177857398986816\n",
      "9e-07\n",
      "Epoch [180/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8337988194616628e-07, Correlation: 0.9999999085742073, R2: 0.99999981662013\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.569068908691406\n",
      "Gradient Norm_Of_Each_Mini_Batch: 24.153467178344727\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.42057991027832\n",
      "Gradient Norm_Batch: 9.733154296875\n",
      "9e-07\n",
      "Epoch [181/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833263354455994e-07, Correlation: 0.9999999085578396, R2: 0.9999998166736677\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.904170989990234\n",
      "Gradient Norm_Of_Each_Mini_Batch: 25.14379119873047\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.703121185302734\n",
      "Gradient Norm_Batch: 11.268104553222656\n",
      "9e-07\n",
      "Epoch [182/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8340838892072497e-07, Correlation: 0.9999999085575323, R2: 0.9999998165915909\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.857563018798828\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.511960983276367\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.118976593017578\n",
      "Gradient Norm_Batch: 10.381092071533203\n",
      "9e-07\n",
      "Epoch [183/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8334215212689742e-07, Correlation: 0.9999999085596942, R2: 0.9999998166578463\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.26108741760254\n",
      "Gradient Norm_Of_Each_Mini_Batch: 26.966585159301758\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.58740234375\n",
      "Gradient Norm_Batch: 10.768706321716309\n",
      "9e-07\n",
      "Epoch [184/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8336899643145443e-07, Correlation: 0.9999999085587693, R2: 0.9999998166309922\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.180042266845703\n",
      "Gradient Norm_Of_Each_Mini_Batch: 25.075592041015625\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.551778793334961\n",
      "Gradient Norm_Batch: 10.274557113647461\n",
      "9e-07\n",
      "Epoch [185/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833700764564128e-07, Correlation: 0.9999999085533904, R2: 0.9999998166299303\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.771909713745117\n",
      "Gradient Norm_Of_Each_Mini_Batch: 24.419042587280273\n",
      "Gradient Norm_Of_Each_Mini_Batch: 13.885881423950195\n",
      "Gradient Norm_Batch: 10.916086196899414\n",
      "9e-07\n",
      "Epoch [186/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.834069252026893e-07, Correlation: 0.9999999085599378, R2: 0.9999998165930669\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.880285263061523\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.61845588684082\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.531404495239258\n",
      "Gradient Norm_Batch: 13.441004753112793\n",
      "9e-07\n",
      "Epoch [187/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.8355677866566111e-07, Correlation: 0.9999999085718608, R2: 0.9999998164432171\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.453880310058594\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.537883758544922\n",
      "Gradient Norm_Of_Each_Mini_Batch: 25.13839340209961\n",
      "Gradient Norm_Batch: 14.438008308410645\n",
      "9e-07\n",
      "Epoch [188/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.835879004374874e-07, Correlation: 0.999999908570372, R2: 0.9999998164121038\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.230798721313477\n",
      "Gradient Norm_Of_Each_Mini_Batch: 25.265796661376953\n",
      "Gradient Norm_Of_Each_Mini_Batch: 13.128808975219727\n",
      "Gradient Norm_Batch: 11.400857925415039\n",
      "9e-07\n",
      "Epoch [189/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8340165297558997e-07, Correlation: 0.9999999085694952, R2: 0.9999998165983413\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.073781967163086\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.736783981323242\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.482078552246094\n",
      "Gradient Norm_Batch: 10.123787879943848\n",
      "9e-07\n",
      "Epoch [190/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8332767126594263e-07, Correlation: 0.9999999085560577, R2: 0.9999998166723263\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.609485626220703\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.075226783752441\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.30754280090332\n",
      "Gradient Norm_Batch: 11.246962547302246\n",
      "9e-07\n",
      "Epoch [191/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8338275253881875e-07, Correlation: 0.9999999085352292, R2: 0.9999998166172476\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 10.897878646850586\n",
      "Gradient Norm_Of_Each_Mini_Batch: 13.521902084350586\n",
      "Gradient Norm_Of_Each_Mini_Batch: 13.288294792175293\n",
      "Gradient Norm_Batch: 11.766850471496582\n",
      "9e-07\n",
      "Epoch [192/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8343284580168984e-07, Correlation: 0.9999999085683832, R2: 0.9999998165671565\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 10.586288452148438\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.214609146118164\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.827280044555664\n",
      "Gradient Norm_Batch: 11.048130989074707\n",
      "9e-07\n",
      "Epoch [193/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8339983398618642e-07, Correlation: 0.9999999085685436, R2: 0.9999998166001608\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.644487380981445\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.955434799194336\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.640003204345703\n",
      "Gradient Norm_Batch: 10.082178115844727\n",
      "9e-07\n",
      "Epoch [194/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8335686036152765e-07, Correlation: 0.999999908548827, R2: 0.9999998166431452\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.982784271240234\n",
      "Gradient Norm_Of_Each_Mini_Batch: 26.71103858947754\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.409170150756836\n",
      "Gradient Norm_Batch: 12.168329238891602\n",
      "9e-07\n",
      "Epoch [195/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.8346921137890604e-07, Correlation: 0.9999999085702321, R2: 0.9999998165307797\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 26.21308135986328\n",
      "Gradient Norm_Of_Each_Mini_Batch: 25.866329193115234\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.992855072021484\n",
      "Gradient Norm_Batch: 9.680052757263184\n",
      "9e-07\n",
      "Epoch [196/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8332666229525785e-07, Correlation: 0.9999999085396176, R2: 0.9999998166733337\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.197673797607422\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.729111671447754\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.66756820678711\n",
      "Gradient Norm_Batch: 10.466614723205566\n",
      "9e-07\n",
      "Epoch [197/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8337152596359374e-07, Correlation: 0.9999999085420983, R2: 0.9999998166284815\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.650838851928711\n",
      "Gradient Norm_Of_Each_Mini_Batch: 11.70627212524414\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.67043113708496\n",
      "Gradient Norm_Batch: 9.353753089904785\n",
      "9e-07\n",
      "Epoch [198/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8330806028643565e-07, Correlation: 0.9999999085423074, R2: 0.9999998166919328\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.944456100463867\n",
      "Gradient Norm_Of_Each_Mini_Batch: 30.001178741455078\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.907848358154297\n",
      "Gradient Norm_Batch: 10.298856735229492\n",
      "9e-07\n",
      "Epoch [199/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833564482467409e-07, Correlation: 0.9999999085581442, R2: 0.9999998166435669\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.42991828918457\n",
      "Gradient Norm_Of_Each_Mini_Batch: 26.745525360107422\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.76607894897461\n",
      "Gradient Norm_Batch: 12.14893913269043\n",
      "9e-07\n",
      "Epoch [200/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8345063779179327e-07, Correlation: 0.999999908567219, R2: 0.9999998165493619\n",
      "Final gradient of the subproblem Core : 12.14893913269043\n",
      "(32, 32)\n",
      "---------------------------Testing with Best Lambda------------------------------\n",
      "Y Test Predicted:  [ 67.54946854  64.18957656 100.90641015 106.79444758  66.52892826\n",
      "  96.79608928 112.76400762  87.081141    58.88485278  60.74065593\n",
      "  65.75330078  64.1572385   82.80136366  68.56418091 142.45856319\n",
      "  98.01990481  98.35308693  55.41645858  96.74738869  87.26440711\n",
      "  47.97826816  91.13698363  73.56488721  77.46470563  46.96438318\n",
      "  79.38020194  84.71055811  45.49448301  87.56694979  73.47984012\n",
      "  86.90805628  88.26721961 124.65286692  81.42685009  87.54737752\n",
      "  82.79799402  98.7347892   69.03779242  80.63847932  93.08343462\n",
      "  68.40977323  87.28011651  62.04219109  57.34047017  56.02120747\n",
      "  89.30955685 110.54353568 134.61068916  43.26800694  74.48163866\n",
      "  96.23282319  87.89111294  79.23381616  90.72714894 125.88510188\n",
      " 160.36982801  60.51192684  86.88819914  89.57422021  85.49696041\n",
      "  79.97225312  95.54232313  57.09906787  50.36966539  70.86145747\n",
      " 131.23369931  94.5116279   83.16017849  62.92657844  84.10767921\n",
      "  76.25957973  96.48156405  77.0842178   65.79257704  71.77937155\n",
      "  65.14985738  61.02651959  68.75444342  66.41036937 104.95699952\n",
      "  79.74027261  91.80683634 110.44704024  62.24777339  57.61591475\n",
      "  98.12922322  67.8358753   72.12861357 127.85051534  63.30886738\n",
      " 103.69715344 150.07960003  67.4198555   92.38844859 123.6785509\n",
      "  46.91140357  64.66067819 108.22063355 101.13967001 113.37839113]\n",
      "Y Test Actual:  [ 67.54932471  64.19367989 100.89907103 106.80033802  66.5296898\n",
      "  96.80085769 112.75716496  87.08379176  58.86645104  60.75871896\n",
      "  65.76651181  64.14603363  82.80477245  68.55912818 142.45923938\n",
      "  98.00053671  98.36489423  55.40988723  96.73410604  87.25275894\n",
      "  47.97392502  91.14302569  73.57729565  77.46144622  46.97758628\n",
      "  79.38435102  84.7040345   45.4879342   87.55672081  73.49886432\n",
      "  86.90598491  88.25175908 124.63253198  81.43265772  87.55287141\n",
      "  82.8080788   98.72768177  69.0468243   80.62856336  93.08545506\n",
      "  68.40807412  87.26847367  62.04717769  57.33990441  56.01334774\n",
      "  89.31934273 110.55219975 134.61307846  43.26935217  74.47617341\n",
      "  96.21843559  87.89937403  79.21862917  90.71786386 125.8824493\n",
      " 160.38439066  60.50387741  86.87508783  89.5739495   85.50880057\n",
      "  79.97185552  95.54965657  57.09355445  50.35928763  70.86185745\n",
      " 131.24078613  94.50837386  83.15863626  62.93422433  84.09831139\n",
      "  76.26182003  96.48126457  77.07300387  65.77838739  71.76861932\n",
      "  65.14937109  61.0440168   68.74400314  66.41967439 104.95340089\n",
      "  79.75508697  91.80406117 110.44770514  62.24491669  57.60974417\n",
      "  98.13365195  67.82489044  72.14320497 127.85744229  63.30804812\n",
      " 103.70015697 150.07489827  67.42743244  92.3906977  123.66379363\n",
      "  46.92967844  64.66629945 108.22935093 101.14612067 113.38031778]\n",
      "Error Report on Testing _ With best Lambda\n",
      "SGD Learning Rate: 9e-07\n",
      "SGD_Alpha chosen for model:  2.5\n",
      "SGD_Test Normalized Estimation Error:  8.530621232835021e-09\n",
      "SGD_Test NMSE Loss:  1.123807936183226e-08\n",
      "SGD_Test R2 Loss:  0.9999998441909198\n",
      "SGD_Test Correlation:  0.9999999222467646\n",
      "Objective Function Values 1110.0198683254678\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final gradient: 12.14893913269043\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------------------Learning Rate 8e-07 -----------------------------------------\n",
      "Objective Function Value: 3408906.126530059\n",
      "--------------------------------------------------------------BCD iteration 0 --------------------------------------------------------------\n",
      "---------------------------------------------Sep 0 Factor 0 -------------------------------------------------\n",
      "Iteration: 0, Separation Rank: 0, Factor Matrix: 0, Objective Function Value: 4178689.296386482\n",
      "---------------------------------------------Sep 0 Factor 1 -------------------------------------------------\n",
      "Iteration: 0, Separation Rank: 0, Factor Matrix: 1, Objective Function Value: 3488977.6201287964\n",
      "---------------------------------------------Sep 1 Factor 0 -------------------------------------------------\n",
      "Iteration: 0, Separation Rank: 1, Factor Matrix: 0, Objective Function Value: 3092821.460810512\n",
      "---------------------------------------------Sep 1 Factor 1 -------------------------------------------------\n",
      "Iteration: 0, Separation Rank: 1, Factor Matrix: 1, Objective Function Value: 4026588.625912\n",
      "---------------------------------------------Core-------------------------------------------------\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 241692.78125\n",
      "Gradient Norm_Of_Each_Mini_Batch: 174401.03125\n",
      "Gradient Norm_Of_Each_Mini_Batch: 94823.421875\n",
      "Gradient Norm_Batch: 342193.125\n",
      "8e-07\n",
      "Epoch [1/200], Loss: 107672.2109, Gap to Optimality: 107672.2109, NMSE: 0.3121381998062134, Correlation: 0.8482482700880302, R2: 0.6878618040465483\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 129595.953125\n",
      "Gradient Norm_Of_Each_Mini_Batch: 115481.9296875\n",
      "Gradient Norm_Of_Each_Mini_Batch: 54159.578125\n",
      "Gradient Norm_Batch: 205680.09375\n",
      "8e-07\n",
      "Epoch [2/200], Loss: 43764.8125, Gap to Optimality: 43764.8125, NMSE: 0.12686961889266968, Correlation: 0.9453220459717993, R2: 0.8731303856245068\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 74319.6953125\n",
      "Gradient Norm_Of_Each_Mini_Batch: 78640.890625\n",
      "Gradient Norm_Of_Each_Mini_Batch: 32106.00390625\n",
      "Gradient Norm_Batch: 130268.5859375\n",
      "8e-07\n",
      "Epoch [3/200], Loss: 19749.3184, Gap to Optimality: 19749.3184, NMSE: 0.0572478324174881, Correlation: 0.9753995985743645, R2: 0.9427521645992749\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 45376.3359375\n",
      "Gradient Norm_Of_Each_Mini_Batch: 44412.9921875\n",
      "Gradient Norm_Of_Each_Mini_Batch: 27686.05078125\n",
      "Gradient Norm_Batch: 86916.640625\n",
      "8e-07\n",
      "Epoch [4/200], Loss: 9807.9385, Gap to Optimality: 9807.9385, NMSE: 0.028427185490727425, Correlation: 0.9874273040199057, R2: 0.9715728143086594\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 34126.94921875\n",
      "Gradient Norm_Of_Each_Mini_Batch: 28131.0703125\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17187.884765625\n",
      "Gradient Norm_Batch: 60514.41796875\n",
      "8e-07\n",
      "Epoch [5/200], Loss: 5241.0508, Gap to Optimality: 5241.0508, NMSE: 0.015187390148639679, Correlation: 0.9930805919538844, R2: 0.984812610513094\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22713.990234375\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20020.169921875\n",
      "Gradient Norm_Of_Each_Mini_Batch: 13383.58203125\n",
      "Gradient Norm_Batch: 43694.6015625\n",
      "8e-07\n",
      "Epoch [6/200], Loss: 2969.5916, Gap to Optimality: 2969.5916, NMSE: 0.008602171204984188, Correlation: 0.9959788673102911, R2: 0.9913978294532858\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17482.541015625\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16967.57421875\n",
      "Gradient Norm_Of_Each_Mini_Batch: 6687.43603515625\n",
      "Gradient Norm_Batch: 32483.97265625\n",
      "8e-07\n",
      "Epoch [7/200], Loss: 1755.8936, Gap to Optimality: 1755.8936, NMSE: 0.005083492491394281, Correlation: 0.9975823275820457, R2: 0.994916507820863\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 13122.05859375\n",
      "Gradient Norm_Of_Each_Mini_Batch: 11481.419921875\n",
      "Gradient Norm_Of_Each_Mini_Batch: 6553.31103515625\n",
      "Gradient Norm_Batch: 24737.40625\n",
      "8e-07\n",
      "Epoch [8/200], Loss: 1074.3976, Gap to Optimality: 1074.3976, NMSE: 0.0031077233143150806, Correlation: 0.9985048205628277, R2: 0.9968922769214639\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 10116.513671875\n",
      "Gradient Norm_Of_Each_Mini_Batch: 10011.515625\n",
      "Gradient Norm_Of_Each_Mini_Batch: 3841.6298828125\n",
      "Gradient Norm_Batch: 19145.744140625\n",
      "8e-07\n",
      "Epoch [9/200], Loss: 672.4289, Gap to Optimality: 672.4289, NMSE: 0.0019423390040174127, Correlation: 0.99905839079578, R2: 0.9980576609344234\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 7841.9345703125\n",
      "Gradient Norm_Of_Each_Mini_Batch: 7573.59521484375\n",
      "Gradient Norm_Of_Each_Mini_Batch: 3062.608642578125\n",
      "Gradient Norm_Batch: 15030.7236328125\n",
      "8e-07\n",
      "Epoch [10/200], Loss: 429.2855, Gap to Optimality: 429.2855, NMSE: 0.0012374109355732799, Correlation: 0.9993963110680782, R2: 0.998762589122496\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 6409.138671875\n",
      "Gradient Norm_Of_Each_Mini_Batch: 5782.07666015625\n",
      "Gradient Norm_Of_Each_Mini_Batch: 2588.42626953125\n",
      "Gradient Norm_Batch: 11926.5810546875\n",
      "8e-07\n",
      "Epoch [11/200], Loss: 278.1496, Gap to Optimality: 278.1496, NMSE: 0.0007992289029061794, Correlation: 0.999608443783536, R2: 0.9992007710972738\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 5110.31396484375\n",
      "Gradient Norm_Of_Each_Mini_Batch: 4374.09765625\n",
      "Gradient Norm_Of_Each_Mini_Batch: 2279.0908203125\n",
      "Gradient Norm_Batch: 9539.9208984375\n",
      "8e-07\n",
      "Epoch [12/200], Loss: 182.4615, Gap to Optimality: 182.4615, NMSE: 0.000521799607668072, Correlation: 0.9997432710560238, R2: 0.9994782004272953\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 3865.25732421875\n",
      "Gradient Norm_Of_Each_Mini_Batch: 3520.37646484375\n",
      "Gradient Norm_Of_Each_Mini_Batch: 1770.64990234375\n",
      "Gradient Norm_Batch: 7679.54248046875\n",
      "8e-07\n",
      "Epoch [13/200], Loss: 121.0326, Gap to Optimality: 121.0326, NMSE: 0.0003436960105318576, Correlation: 0.9998304792039293, R2: 0.9996563039989986\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 3529.530029296875\n",
      "Gradient Norm_Of_Each_Mini_Batch: 2642.96142578125\n",
      "Gradient Norm_Of_Each_Mini_Batch: 1458.7413330078125\n",
      "Gradient Norm_Batch: 6208.7080078125\n",
      "8e-07\n",
      "Epoch [14/200], Loss: 80.9964, Gap to Optimality: 80.9964, NMSE: 0.00022761539730709046, Correlation: 0.9998875488869681, R2: 0.9997723846124402\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 2507.64306640625\n",
      "Gradient Norm_Of_Each_Mini_Batch: 2453.411376953125\n",
      "Gradient Norm_Of_Each_Mini_Batch: 1111.4593505859375\n",
      "Gradient Norm_Batch: 5038.72802734375\n",
      "8e-07\n",
      "Epoch [15/200], Loss: 54.7812, Gap to Optimality: 54.7812, NMSE: 0.0001516061747679487, Correlation: 0.9999250279062372, R2: 0.9998483938105931\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 2201.761474609375\n",
      "Gradient Norm_Of_Each_Mini_Batch: 1783.9859619140625\n",
      "Gradient Norm_Of_Each_Mini_Batch: 962.1922607421875\n",
      "Gradient Norm_Batch: 4101.65234375\n",
      "8e-07\n",
      "Epoch [16/200], Loss: 37.4792, Gap to Optimality: 37.4792, NMSE: 0.00010143773397430778, Correlation: 0.9999497192664157, R2: 0.9998985622671631\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 1644.725341796875\n",
      "Gradient Norm_Of_Each_Mini_Batch: 1724.74560546875\n",
      "Gradient Norm_Of_Each_Mini_Batch: 758.9462890625\n",
      "Gradient Norm_Batch: 3345.30078125\n",
      "8e-07\n",
      "Epoch [17/200], Loss: 25.9744, Gap to Optimality: 25.9744, NMSE: 6.807853060308844e-05, Correlation: 0.9999662385559006, R2: 0.9999319214631149\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 1572.3125\n",
      "Gradient Norm_Of_Each_Mini_Batch: 1156.8251953125\n",
      "Gradient Norm_Of_Each_Mini_Batch: 623.1587524414062\n",
      "Gradient Norm_Batch: 2734.26611328125\n",
      "8e-07\n",
      "Epoch [18/200], Loss: 18.3092, Gap to Optimality: 18.3092, NMSE: 4.585144415614195e-05, Correlation: 0.9999772314693972, R2: 0.9999541485595901\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 1140.7779541015625\n",
      "Gradient Norm_Of_Each_Mini_Batch: 994.9052734375\n",
      "Gradient Norm_Of_Each_Mini_Batch: 557.7823486328125\n",
      "Gradient Norm_Batch: 2239.8466796875\n",
      "8e-07\n",
      "Epoch [19/200], Loss: 13.1942, Gap to Optimality: 13.1942, NMSE: 3.101910260738805e-05, Correlation: 0.9999845965197764, R2: 0.9999689808975845\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 837.0908203125\n",
      "Gradient Norm_Of_Each_Mini_Batch: 817.4498901367188\n",
      "Gradient Norm_Of_Each_Mini_Batch: 568.4352416992188\n",
      "Gradient Norm_Batch: 1837.6136474609375\n",
      "8e-07\n",
      "Epoch [20/200], Loss: 9.7594, Gap to Optimality: 9.7594, NMSE: 2.1058433048892766e-05, Correlation: 0.9999895415071559, R2: 0.9999789415664467\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 709.42431640625\n",
      "Gradient Norm_Of_Each_Mini_Batch: 730.573486328125\n",
      "Gradient Norm_Of_Each_Mini_Batch: 364.4210205078125\n",
      "Gradient Norm_Batch: 1509.315185546875\n",
      "8e-07\n",
      "Epoch [21/200], Loss: 7.4439, Gap to Optimality: 7.4439, NMSE: 1.4343189832288772e-05, Correlation: 0.9999928737452773, R2: 0.9999856568108948\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 608.771484375\n",
      "Gradient Norm_Of_Each_Mini_Batch: 614.7181396484375\n",
      "Gradient Norm_Of_Each_Mini_Batch: 343.44390869140625\n",
      "Gradient Norm_Batch: 1240.5726318359375\n",
      "8e-07\n",
      "Epoch [22/200], Loss: 5.8769, Gap to Optimality: 5.8769, NMSE: 9.79840933723608e-06, Correlation: 0.9999951312244115, R2: 0.9999902015907568\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 475.60406494140625\n",
      "Gradient Norm_Of_Each_Mini_Batch: 495.8458557128906\n",
      "Gradient Norm_Of_Each_Mini_Batch: 266.3639221191406\n",
      "Gradient Norm_Batch: 1021.044189453125\n",
      "8e-07\n",
      "Epoch [23/200], Loss: 4.8189, Gap to Optimality: 4.8189, NMSE: 6.72956366543076e-06, Correlation: 0.9999966533109887, R2: 0.999993270436236\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 420.0016784667969\n",
      "Gradient Norm_Of_Each_Mini_Batch: 392.29315185546875\n",
      "Gradient Norm_Of_Each_Mini_Batch: 224.65924072265625\n",
      "Gradient Norm_Batch: 841.2592163085938\n",
      "8e-07\n",
      "Epoch [24/200], Loss: 4.1013, Gap to Optimality: 4.1013, NMSE: 4.647702098736772e-06, Correlation: 0.9999976867045578, R2: 0.9999953522979186\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 359.04852294921875\n",
      "Gradient Norm_Of_Each_Mini_Batch: 313.08203125\n",
      "Gradient Norm_Of_Each_Mini_Batch: 163.5294952392578\n",
      "Gradient Norm_Batch: 693.8641357421875\n",
      "8e-07\n",
      "Epoch [25/200], Loss: 3.6140, Gap to Optimality: 3.6140, NMSE: 3.2342290978704114e-06, Correlation: 0.9999983908859906, R2: 0.9999967657710633\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 283.2092590332031\n",
      "Gradient Norm_Of_Each_Mini_Batch: 257.2772216796875\n",
      "Gradient Norm_Of_Each_Mini_Batch: 145.29867553710938\n",
      "Gradient Norm_Batch: 573.0435180664062\n",
      "8e-07\n",
      "Epoch [26/200], Loss: 3.2828, Gap to Optimality: 3.2828, NMSE: 2.2732390334567754e-06, Correlation: 0.999998869247899, R2: 0.9999977267608723\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 204.24749755859375\n",
      "Gradient Norm_Of_Each_Mini_Batch: 234.13677978515625\n",
      "Gradient Norm_Of_Each_Mini_Batch: 131.26992797851562\n",
      "Gradient Norm_Batch: 473.7933654785156\n",
      "8e-07\n",
      "Epoch [27/200], Loss: 3.0570, Gap to Optimality: 3.0570, NMSE: 1.6180617876671022e-06, Correlation: 0.9999991949851634, R2: 0.9999983819382503\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 198.33460998535156\n",
      "Gradient Norm_Of_Each_Mini_Batch: 191.7860107421875\n",
      "Gradient Norm_Of_Each_Mini_Batch: 101.73418426513672\n",
      "Gradient Norm_Batch: 392.08099365234375\n",
      "8e-07\n",
      "Epoch [28/200], Loss: 2.9023, Gap to Optimality: 2.9023, NMSE: 1.1690492556226673e-06, Correlation: 0.9999994187944056, R2: 0.9999988309507748\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 160.46746826171875\n",
      "Gradient Norm_Of_Each_Mini_Batch: 150.25299072265625\n",
      "Gradient Norm_Of_Each_Mini_Batch: 89.50670623779297\n",
      "Gradient Norm_Batch: 324.6478271484375\n",
      "8e-07\n",
      "Epoch [29/200], Loss: 2.7964, Gap to Optimality: 2.7964, NMSE: 8.61793807871436e-07, Correlation: 0.9999995717432105, R2: 0.9999991382061282\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 122.51677703857422\n",
      "Gradient Norm_Of_Each_Mini_Batch: 127.00568389892578\n",
      "Gradient Norm_Of_Each_Mini_Batch: 77.66657257080078\n",
      "Gradient Norm_Batch: 269.069091796875\n",
      "8e-07\n",
      "Epoch [30/200], Loss: 2.7239, Gap to Optimality: 2.7239, NMSE: 6.511773449346947e-07, Correlation: 0.9999996764113184, R2: 0.9999993488226913\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 97.9857177734375\n",
      "Gradient Norm_Of_Each_Mini_Batch: 117.45996856689453\n",
      "Gradient Norm_Of_Each_Mini_Batch: 66.53180694580078\n",
      "Gradient Norm_Batch: 223.13433837890625\n",
      "8e-07\n",
      "Epoch [31/200], Loss: 2.6740, Gap to Optimality: 2.6740, NMSE: 5.062192940386012e-07, Correlation: 0.9999997483856078, R2: 0.9999994937807244\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 100.26551818847656\n",
      "Gradient Norm_Of_Each_Mini_Batch: 82.68488311767578\n",
      "Gradient Norm_Of_Each_Mini_Batch: 45.800167083740234\n",
      "Gradient Norm_Batch: 185.35816955566406\n",
      "8e-07\n",
      "Epoch [32/200], Loss: 2.6398, Gap to Optimality: 2.6398, NMSE: 4.0690537161935936e-07, Correlation: 0.9999997977128184, R2: 0.999999593094679\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 75.68887329101562\n",
      "Gradient Norm_Of_Each_Mini_Batch: 81.98212432861328\n",
      "Gradient Norm_Of_Each_Mini_Batch: 40.61384201049805\n",
      "Gradient Norm_Batch: 154.18775939941406\n",
      "8e-07\n",
      "Epoch [33/200], Loss: 2.6162, Gap to Optimality: 2.6162, NMSE: 3.3843116398202255e-07, Correlation: 0.9999998317627126, R2: 0.9999996615688173\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 76.62903594970703\n",
      "Gradient Norm_Of_Each_Mini_Batch: 59.83961868286133\n",
      "Gradient Norm_Of_Each_Mini_Batch: 31.119997024536133\n",
      "Gradient Norm_Batch: 128.6403045654297\n",
      "8e-07\n",
      "Epoch [34/200], Loss: 2.6000, Gap to Optimality: 2.6000, NMSE: 2.9136299417586997e-07, Correlation: 0.9999998551886088, R2: 0.9999997086370136\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 57.929683685302734\n",
      "Gradient Norm_Of_Each_Mini_Batch: 47.56193923950195\n",
      "Gradient Norm_Of_Each_Mini_Batch: 35.42543029785156\n",
      "Gradient Norm_Batch: 107.48588562011719\n",
      "8e-07\n",
      "Epoch [35/200], Loss: 2.5888, Gap to Optimality: 2.5888, NMSE: 2.58605695080405e-07, Correlation: 0.9999998715498581, R2: 0.999999741394307\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 44.50719451904297\n",
      "Gradient Norm_Of_Each_Mini_Batch: 42.69511032104492\n",
      "Gradient Norm_Of_Each_Mini_Batch: 27.413488388061523\n",
      "Gradient Norm_Batch: 89.44188690185547\n",
      "8e-07\n",
      "Epoch [36/200], Loss: 2.5809, Gap to Optimality: 2.5809, NMSE: 2.3564574291867757e-07, Correlation: 0.9999998827807397, R2: 0.9999997643542616\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 31.354873657226562\n",
      "Gradient Norm_Of_Each_Mini_Batch: 41.57673263549805\n",
      "Gradient Norm_Of_Each_Mini_Batch: 29.743511199951172\n",
      "Gradient Norm_Batch: 75.01573944091797\n",
      "8e-07\n",
      "Epoch [37/200], Loss: 2.5755, Gap to Optimality: 2.5755, NMSE: 2.1990616971834243e-07, Correlation: 0.9999998906313994, R2: 0.999999780093828\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 35.43052673339844\n",
      "Gradient Norm_Of_Each_Mini_Batch: 39.832000732421875\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.949467658996582\n",
      "Gradient Norm_Batch: 62.58693313598633\n",
      "8e-07\n",
      "Epoch [38/200], Loss: 2.5717, Gap to Optimality: 2.5717, NMSE: 2.0886565721411898e-07, Correlation: 0.9999998960115191, R2: 0.999999791134327\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 27.986873626708984\n",
      "Gradient Norm_Of_Each_Mini_Batch: 30.74480628967285\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.211992263793945\n",
      "Gradient Norm_Batch: 52.40623474121094\n",
      "8e-07\n",
      "Epoch [39/200], Loss: 2.5691, Gap to Optimality: 2.5691, NMSE: 2.0117931853746995e-07, Correlation: 0.9999998997707823, R2: 0.9999997988206829\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 31.147010803222656\n",
      "Gradient Norm_Of_Each_Mini_Batch: 33.718143463134766\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.078704833984375\n",
      "Gradient Norm_Batch: 44.553897857666016\n",
      "8e-07\n",
      "Epoch [40/200], Loss: 2.5673, Gap to Optimality: 2.5673, NMSE: 1.9596416223066626e-07, Correlation: 0.9999999024185229, R2: 0.9999998040358342\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 28.48382568359375\n",
      "Gradient Norm_Of_Each_Mini_Batch: 30.670495986938477\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.877119064331055\n",
      "Gradient Norm_Batch: 37.90434265136719\n",
      "8e-07\n",
      "Epoch [41/200], Loss: 2.5660, Gap to Optimality: 2.5660, NMSE: 1.9226122560667136e-07, Correlation: 0.9999999042388085, R2: 0.9999998077387752\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 27.804187774658203\n",
      "Gradient Norm_Of_Each_Mini_Batch: 30.66461944580078\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.357878684997559\n",
      "Gradient Norm_Batch: 32.90726089477539\n",
      "8e-07\n",
      "Epoch [42/200], Loss: 2.5652, Gap to Optimality: 2.5652, NMSE: 1.8973967996771535e-07, Correlation: 0.9999999055492407, R2: 0.999999810260325\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.666831970214844\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.45377540588379\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.968544006347656\n",
      "Gradient Norm_Batch: 28.163103103637695\n",
      "8e-07\n",
      "Epoch [43/200], Loss: 2.5645, Gap to Optimality: 2.5645, NMSE: 1.878692756918099e-07, Correlation: 0.9999999064409346, R2: 0.9999998121307396\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.29033851623535\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.416488647460938\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.45945167541504\n",
      "Gradient Norm_Batch: 24.539722442626953\n",
      "8e-07\n",
      "Epoch [44/200], Loss: 2.5641, Gap to Optimality: 2.5641, NMSE: 1.865796264155506e-07, Correlation: 0.9999999070748169, R2: 0.9999998134203539\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.372617721557617\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.37949562072754\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.754131317138672\n",
      "Gradient Norm_Batch: 20.68305015563965\n",
      "8e-07\n",
      "Epoch [45/200], Loss: 2.5638, Gap to Optimality: 2.5638, NMSE: 1.8556900727162429e-07, Correlation: 0.9999999074961147, R2: 0.9999998144309941\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 26.0361328125\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.029138565063477\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.076066970825195\n",
      "Gradient Norm_Batch: 19.42290496826172\n",
      "8e-07\n",
      "Epoch [46/200], Loss: 2.5636, Gap to Optimality: 2.5636, NMSE: 1.8505195953366638e-07, Correlation: 0.9999999078119804, R2: 0.9999998149480551\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.204486846923828\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.77802085876465\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.412893295288086\n",
      "Gradient Norm_Batch: 16.83510971069336\n",
      "8e-07\n",
      "Epoch [47/200], Loss: 2.5634, Gap to Optimality: 2.5634, NMSE: 1.8454342409768287e-07, Correlation: 0.999999908011031, R2: 0.9999998154565829\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.51539421081543\n",
      "Gradient Norm_Of_Each_Mini_Batch: 25.012557983398438\n",
      "Gradient Norm_Of_Each_Mini_Batch: 13.286513328552246\n",
      "Gradient Norm_Batch: 14.863669395446777\n",
      "8e-07\n",
      "Epoch [48/200], Loss: 2.5633, Gap to Optimality: 2.5633, NMSE: 1.841706449567937e-07, Correlation: 0.9999999081483064, R2: 0.9999998158293517\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.798513412475586\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.919065475463867\n",
      "Gradient Norm_Of_Each_Mini_Batch: 24.440082550048828\n",
      "Gradient Norm_Batch: 15.216540336608887\n",
      "8e-07\n",
      "Epoch [49/200], Loss: 2.5632, Gap to Optimality: 2.5632, NMSE: 1.840572707578758e-07, Correlation: 0.9999999082858977, R2: 0.9999998159427103\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.75423812866211\n",
      "Gradient Norm_Of_Each_Mini_Batch: 25.81180191040039\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.233043670654297\n",
      "Gradient Norm_Batch: 14.320690155029297\n",
      "8e-07\n",
      "Epoch [50/200], Loss: 2.5632, Gap to Optimality: 2.5632, NMSE: 1.838794645436792e-07, Correlation: 0.9999999083698625, R2: 0.9999998161205395\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.676912307739258\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.24028968811035\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.280929565429688\n",
      "Gradient Norm_Batch: 12.155405044555664\n",
      "8e-07\n",
      "Epoch [51/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.8366105791756127e-07, Correlation: 0.9999999084235922, R2: 0.9999998163389288\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.5021915435791\n",
      "Gradient Norm_Of_Each_Mini_Batch: 25.891544342041016\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.504785537719727\n",
      "Gradient Norm_Batch: 11.946922302246094\n",
      "8e-07\n",
      "Epoch [52/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.8358889519731747e-07, Correlation: 0.9999999084657312, R2: 0.9999998164111161\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.754892349243164\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.1449031829834\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.681608200073242\n",
      "Gradient Norm_Batch: 11.921785354614258\n",
      "8e-07\n",
      "Epoch [53/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.8352604058691213e-07, Correlation: 0.9999999084951245, R2: 0.9999998164739572\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.176544189453125\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.630430221557617\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.980131149291992\n",
      "Gradient Norm_Batch: 14.102975845336914\n",
      "8e-07\n",
      "Epoch [54/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.836345404626627e-07, Correlation: 0.9999999085124517, R2: 0.9999998163654784\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 8.573958396911621\n",
      "Gradient Norm_Of_Each_Mini_Batch: 12.964276313781738\n",
      "Gradient Norm_Of_Each_Mini_Batch: 10.896200180053711\n",
      "Gradient Norm_Batch: 11.833169937133789\n",
      "8e-07\n",
      "Epoch [55/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.8349297192798986e-07, Correlation: 0.9999999085325405, R2: 0.9999998165070373\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 11.120526313781738\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.141181945800781\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.902851104736328\n",
      "Gradient Norm_Batch: 11.38206958770752\n",
      "8e-07\n",
      "Epoch [56/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.8345126306940074e-07, Correlation: 0.9999999085445883, R2: 0.99999981654873\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.37503433227539\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.450746536254883\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.820587158203125\n",
      "Gradient Norm_Batch: 11.642533302307129\n",
      "8e-07\n",
      "Epoch [57/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.8345062358093855e-07, Correlation: 0.9999999085504504, R2: 0.9999998165493933\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.769224166870117\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.136749267578125\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.801435470581055\n",
      "Gradient Norm_Batch: 12.432252883911133\n",
      "8e-07\n",
      "Epoch [58/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.8349200558986922e-07, Correlation: 0.9999999085626601, R2: 0.9999998165079916\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.8017635345459\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.439844131469727\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.108354568481445\n",
      "Gradient Norm_Batch: 12.837053298950195\n",
      "8e-07\n",
      "Epoch [59/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.835009584283398e-07, Correlation: 0.9999999085620125, R2: 0.999999816499037\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.243952751159668\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.475960731506348\n",
      "Gradient Norm_Of_Each_Mini_Batch: 13.695902824401855\n",
      "Gradient Norm_Batch: 11.50827693939209\n",
      "8e-07\n",
      "Epoch [60/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.834351479601537e-07, Correlation: 0.9999999085588417, R2: 0.9999998165648477\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 24.214075088500977\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.645000457763672\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.471385955810547\n",
      "Gradient Norm_Batch: 13.211865425109863\n",
      "8e-07\n",
      "Epoch [61/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.835403224959009e-07, Correlation: 0.999999908560763, R2: 0.9999998164596765\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.453645706176758\n",
      "Gradient Norm_Of_Each_Mini_Batch: 27.20093536376953\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.063867568969727\n",
      "Gradient Norm_Batch: 12.113691329956055\n",
      "8e-07\n",
      "Epoch [62/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.8346803187796468e-07, Correlation: 0.9999999085645606, R2: 0.9999998165319711\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.362438201904297\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.411535263061523\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.3648624420166\n",
      "Gradient Norm_Batch: 12.10277271270752\n",
      "8e-07\n",
      "Epoch [63/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.8345774321915087e-07, Correlation: 0.9999999085653414, R2: 0.9999998165422536\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 24.02243995666504\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.12258529663086\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.33167839050293\n",
      "Gradient Norm_Batch: 12.711427688598633\n",
      "8e-07\n",
      "Epoch [64/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.8350360164731683e-07, Correlation: 0.9999999085582469, R2: 0.9999998164963874\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.102083206176758\n",
      "Gradient Norm_Of_Each_Mini_Batch: 24.632558822631836\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.071897506713867\n",
      "Gradient Norm_Batch: 10.496353149414062\n",
      "8e-07\n",
      "Epoch [65/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833600720146933e-07, Correlation: 0.9999999085402139, R2: 0.9999998166399103\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.319629669189453\n",
      "Gradient Norm_Of_Each_Mini_Batch: 27.295686721801758\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.82259178161621\n",
      "Gradient Norm_Batch: 10.085771560668945\n",
      "8e-07\n",
      "Epoch [66/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8333248874569108e-07, Correlation: 0.9999999085648964, R2: 0.9999998166675242\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.99258804321289\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.81999969482422\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.62944221496582\n",
      "Gradient Norm_Batch: 9.46081829071045\n",
      "8e-07\n",
      "Epoch [67/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.832925988765055e-07, Correlation: 0.9999999085613255, R2: 0.9999998167074088\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.004791259765625\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.505260467529297\n",
      "Gradient Norm_Of_Each_Mini_Batch: 10.742130279541016\n",
      "Gradient Norm_Batch: 10.921372413635254\n",
      "8e-07\n",
      "Epoch [68/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8336207574520813e-07, Correlation: 0.9999999085640211, R2: 0.9999998166379273\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.645689010620117\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.605432510375977\n",
      "Gradient Norm_Of_Each_Mini_Batch: 12.87703800201416\n",
      "Gradient Norm_Batch: 10.999164581298828\n",
      "8e-07\n",
      "Epoch [69/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8337735241402697e-07, Correlation: 0.9999999085697004, R2: 0.9999998166226465\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.731952667236328\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.627254486083984\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.808879852294922\n",
      "Gradient Norm_Batch: 11.753788948059082\n",
      "8e-07\n",
      "Epoch [70/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.834281277979244e-07, Correlation: 0.9999999085712609, R2: 0.9999998165718588\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.519628524780273\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.24315643310547\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.941174507141113\n",
      "Gradient Norm_Batch: 10.578067779541016\n",
      "8e-07\n",
      "Epoch [71/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8335454399220907e-07, Correlation: 0.9999999085712057, R2: 0.999999816645443\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 27.074718475341797\n",
      "Gradient Norm_Of_Each_Mini_Batch: 24.075761795043945\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.796123504638672\n",
      "Gradient Norm_Batch: 10.942584037780762\n",
      "8e-07\n",
      "Epoch [72/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8338143092933024e-07, Correlation: 0.9999999085563479, R2: 0.999999816618578\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.033132553100586\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.592388153076172\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.378459930419922\n",
      "Gradient Norm_Batch: 11.32715892791748\n",
      "8e-07\n",
      "Epoch [73/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8340801943850238e-07, Correlation: 0.9999999085526327, R2: 0.9999998165919785\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.68267250061035\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.377002716064453\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.645918846130371\n",
      "Gradient Norm_Batch: 10.723456382751465\n",
      "8e-07\n",
      "Epoch [74/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8338295149078476e-07, Correlation: 0.9999999085643111, R2: 0.9999998166170398\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.609580993652344\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.50050163269043\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.169445037841797\n",
      "Gradient Norm_Batch: 11.035107612609863\n",
      "8e-07\n",
      "Epoch [75/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8339032692438195e-07, Correlation: 0.9999999085674088, R2: 0.9999998166096581\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 24.122180938720703\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.335304260253906\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.654024124145508\n",
      "Gradient Norm_Batch: 10.130634307861328\n",
      "8e-07\n",
      "Epoch [76/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8334635853989312e-07, Correlation: 0.9999999085569334, R2: 0.9999998166536438\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.434581756591797\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.87090301513672\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.773544311523438\n",
      "Gradient Norm_Batch: 11.130059242248535\n",
      "8e-07\n",
      "Epoch [77/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8337573237658944e-07, Correlation: 0.9999999085546143, R2: 0.9999998166242683\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.076414108276367\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.22746467590332\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.369281768798828\n",
      "Gradient Norm_Batch: 11.40035343170166\n",
      "8e-07\n",
      "Epoch [78/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8340834628816083e-07, Correlation: 0.9999999085687635, R2: 0.9999998165916482\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.14685821533203\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.217208862304688\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.079504013061523\n",
      "Gradient Norm_Batch: 12.848238945007324\n",
      "8e-07\n",
      "Epoch [79/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.8350044683757005e-07, Correlation: 0.999999908572843, R2: 0.9999998164995384\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.944665908813477\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.141164779663086\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.495277404785156\n",
      "Gradient Norm_Batch: 12.832637786865234\n",
      "8e-07\n",
      "Epoch [80/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.835165619468171e-07, Correlation: 0.9999999085669905, R2: 0.9999998164834387\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.29235076904297\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.784650802612305\n",
      "Gradient Norm_Of_Each_Mini_Batch: 13.777420043945312\n",
      "Gradient Norm_Batch: 10.927824974060059\n",
      "8e-07\n",
      "Epoch [81/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.83404196718584e-07, Correlation: 0.9999999085584019, R2: 0.9999998165958056\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.279390335083008\n",
      "Gradient Norm_Of_Each_Mini_Batch: 24.53679084777832\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.155494689941406\n",
      "Gradient Norm_Batch: 10.988458633422852\n",
      "8e-07\n",
      "Epoch [82/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8337843243898533e-07, Correlation: 0.9999999085678966, R2: 0.999999816621568\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.720401763916016\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.947111129760742\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.025272369384766\n",
      "Gradient Norm_Batch: 11.887003898620605\n",
      "8e-07\n",
      "Epoch [83/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.83444115009479e-07, Correlation: 0.9999999085713251, R2: 0.9999998165558704\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.008155822753906\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.1453914642334\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.430692672729492\n",
      "Gradient Norm_Batch: 11.818620681762695\n",
      "8e-07\n",
      "Epoch [84/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.834503819964084e-07, Correlation: 0.9999999085614675, R2: 0.9999998165496352\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.407670021057129\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.935192108154297\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.47076988220215\n",
      "Gradient Norm_Batch: 12.576035499572754\n",
      "8e-07\n",
      "Epoch [85/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.8350036157244176e-07, Correlation: 0.9999999085627762, R2: 0.9999998164996348\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.880796432495117\n",
      "Gradient Norm_Of_Each_Mini_Batch: 29.952190399169922\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.1434326171875\n",
      "Gradient Norm_Batch: 12.829293251037598\n",
      "8e-07\n",
      "Epoch [86/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.8350701225244848e-07, Correlation: 0.9999999085542883, R2: 0.9999998164929842\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.315593719482422\n",
      "Gradient Norm_Of_Each_Mini_Batch: 32.02505874633789\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.60503387451172\n",
      "Gradient Norm_Batch: 12.67214298248291\n",
      "8e-07\n",
      "Epoch [87/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.8350047525927948e-07, Correlation: 0.9999999085694423, R2: 0.99999981649953\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.531991958618164\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.035076141357422\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.183448791503906\n",
      "Gradient Norm_Batch: 11.852617263793945\n",
      "8e-07\n",
      "Epoch [88/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8344569241435238e-07, Correlation: 0.9999999085683391, R2: 0.9999998165543135\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.149749755859375\n",
      "Gradient Norm_Of_Each_Mini_Batch: 26.572586059570312\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.734630584716797\n",
      "Gradient Norm_Batch: 11.637679100036621\n",
      "8e-07\n",
      "Epoch [89/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8343845908930234e-07, Correlation: 0.9999999085551525, R2: 0.9999998165615516\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.285755157470703\n",
      "Gradient Norm_Of_Each_Mini_Batch: 26.679166793823242\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.31129264831543\n",
      "Gradient Norm_Batch: 11.559123039245605\n",
      "8e-07\n",
      "Epoch [90/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.834251577292889e-07, Correlation: 0.9999999085395066, R2: 0.9999998165748548\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.21494483947754\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.256202697753906\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.158771514892578\n",
      "Gradient Norm_Batch: 9.812369346618652\n",
      "8e-07\n",
      "Epoch [91/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833218448155094e-07, Correlation: 0.9999999085595316, R2: 0.9999998166781523\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.895723342895508\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.410804748535156\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.27228546142578\n",
      "Gradient Norm_Batch: 11.489786148071289\n",
      "8e-07\n",
      "Epoch [92/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8340629992508184e-07, Correlation: 0.9999999085695025, R2: 0.9999998165937101\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.79782485961914\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.968862533569336\n",
      "Gradient Norm_Of_Each_Mini_Batch: 32.50642013549805\n",
      "Gradient Norm_Batch: 12.333291053771973\n",
      "8e-07\n",
      "Epoch [93/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.834543041923098e-07, Correlation: 0.9999999085648429, R2: 0.9999998165456911\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 12.474505424499512\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.695396423339844\n",
      "Gradient Norm_Of_Each_Mini_Batch: 13.146130561828613\n",
      "Gradient Norm_Batch: 10.499241828918457\n",
      "8e-07\n",
      "Epoch [94/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8334881701775885e-07, Correlation: 0.9999999085654426, R2: 0.9999998166511874\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.142154693603516\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.15459632873535\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.318031311035156\n",
      "Gradient Norm_Batch: 10.44870376586914\n",
      "8e-07\n",
      "Epoch [95/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8334955598220404e-07, Correlation: 0.999999908563775, R2: 0.9999998166504511\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.337984085083008\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.7390193939209\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.75710105895996\n",
      "Gradient Norm_Batch: 10.901810646057129\n",
      "8e-07\n",
      "Epoch [96/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8337264862111624e-07, Correlation: 0.9999999085691544, R2: 0.9999998166273624\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.081876754760742\n",
      "Gradient Norm_Of_Each_Mini_Batch: 28.427780151367188\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.795957565307617\n",
      "Gradient Norm_Batch: 9.737014770507812\n",
      "8e-07\n",
      "Epoch [97/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8331739681798354e-07, Correlation: 0.9999999085654889, R2: 0.9999998166826121\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.804563522338867\n",
      "Gradient Norm_Of_Each_Mini_Batch: 24.081655502319336\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.58901309967041\n",
      "Gradient Norm_Batch: 10.374191284179688\n",
      "8e-07\n",
      "Epoch [98/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833464438050214e-07, Correlation: 0.9999999085662461, R2: 0.9999998166535522\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 25.474660873413086\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.07611083984375\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.706954956054688\n",
      "Gradient Norm_Batch: 8.409232139587402\n",
      "8e-07\n",
      "Epoch [99/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8324718098483572e-07, Correlation: 0.9999999085389482, R2: 0.9999998167528152\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.717555046081543\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.692787170410156\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.99958038330078\n",
      "Gradient Norm_Batch: 9.67951488494873\n",
      "8e-07\n",
      "Epoch [100/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8332313800328848e-07, Correlation: 0.9999999085551351, R2: 0.9999998166768708\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.674325942993164\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.68355369567871\n",
      "Gradient Norm_Of_Each_Mini_Batch: 13.9265775680542\n",
      "Gradient Norm_Batch: 11.120071411132812\n",
      "8e-07\n",
      "Epoch [101/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8337370022436517e-07, Correlation: 0.9999999085625307, R2: 0.9999998166263111\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 7.104742527008057\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.874797821044922\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.31476402282715\n",
      "Gradient Norm_Batch: 11.55538558959961\n",
      "8e-07\n",
      "Epoch [102/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8341154373047175e-07, Correlation: 0.9999999085693488, R2: 0.9999998165884525\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.314872741699219\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.913278579711914\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.314037322998047\n",
      "Gradient Norm_Batch: 10.271842002868652\n",
      "8e-07\n",
      "Epoch [103/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8335104812194913e-07, Correlation: 0.9999999085618437, R2: 0.9999998166489605\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.59207534790039\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.222063064575195\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.808076858520508\n",
      "Gradient Norm_Batch: 10.333181381225586\n",
      "8e-07\n",
      "Epoch [104/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8336471896418516e-07, Correlation: 0.9999999085599259, R2: 0.9999998166352706\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.510704040527344\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.940080642700195\n",
      "Gradient Norm_Of_Each_Mini_Batch: 12.09005355834961\n",
      "Gradient Norm_Batch: 10.3099946975708\n",
      "8e-07\n",
      "Epoch [105/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833312097687667e-07, Correlation: 0.9999999085602869, R2: 0.9999998166687769\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.497251510620117\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.083595275878906\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.572005271911621\n",
      "Gradient Norm_Batch: 10.236333847045898\n",
      "8e-07\n",
      "Epoch [106/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8332247009311686e-07, Correlation: 0.9999999085548132, R2: 0.9999998166775323\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.584627151489258\n",
      "Gradient Norm_Of_Each_Mini_Batch: 27.982481002807617\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.313920974731445\n",
      "Gradient Norm_Batch: 10.216312408447266\n",
      "8e-07\n",
      "Epoch [107/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8335629192733904e-07, Correlation: 0.9999999085592328, R2: 0.9999998166436922\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.118310928344727\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.532102584838867\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.639925003051758\n",
      "Gradient Norm_Batch: 8.423044204711914\n",
      "8e-07\n",
      "Epoch [108/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8324021766602527e-07, Correlation: 0.9999999085652596, R2: 0.9999998167597846\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 13.19386100769043\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.601722717285156\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.884710311889648\n",
      "Gradient Norm_Batch: 9.382827758789062\n",
      "8e-07\n",
      "Epoch [109/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8329153306240187e-07, Correlation: 0.9999999085677763, R2: 0.9999998167084423\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.465391159057617\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.99250602722168\n",
      "Gradient Norm_Of_Each_Mini_Batch: 11.187427520751953\n",
      "Gradient Norm_Batch: 9.492731094360352\n",
      "8e-07\n",
      "Epoch [110/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8330237594454957e-07, Correlation: 0.9999999085579503, R2: 0.999999816697605\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.401317596435547\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.150373458862305\n",
      "Gradient Norm_Of_Each_Mini_Batch: 13.989688873291016\n",
      "Gradient Norm_Batch: 10.14539623260498\n",
      "8e-07\n",
      "Epoch [111/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.83337832027064e-07, Correlation: 0.9999999085683676, R2: 0.9999998166621807\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.730091094970703\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.49894905090332\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.329092025756836\n",
      "Gradient Norm_Batch: 11.310136795043945\n",
      "8e-07\n",
      "Epoch [112/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8338614893309568e-07, Correlation: 0.9999999085658378, R2: 0.9999998166138562\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.68025779724121\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.91514778137207\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.610464096069336\n",
      "Gradient Norm_Batch: 10.227423667907715\n",
      "8e-07\n",
      "Epoch [113/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8335067863972654e-07, Correlation: 0.9999999085571152, R2: 0.9999998166493232\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.24793815612793\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.584375381469727\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.408967971801758\n",
      "Gradient Norm_Batch: 10.021961212158203\n",
      "8e-07\n",
      "Epoch [114/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8332757178995962e-07, Correlation: 0.9999999085664887, R2: 0.9999998166724299\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.182559967041016\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.31615447998047\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.928821563720703\n",
      "Gradient Norm_Batch: 9.426674842834473\n",
      "8e-07\n",
      "Epoch [115/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8330476336814172e-07, Correlation: 0.999999908562191, R2: 0.9999998166952291\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.917797088623047\n",
      "Gradient Norm_Of_Each_Mini_Batch: 27.00457191467285\n",
      "Gradient Norm_Of_Each_Mini_Batch: 11.98909854888916\n",
      "Gradient Norm_Batch: 9.930635452270508\n",
      "8e-07\n",
      "Epoch [116/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8331903106627578e-07, Correlation: 0.9999999085611517, R2: 0.9999998166809639\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.04363250732422\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.765609741210938\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.81134796142578\n",
      "Gradient Norm_Batch: 8.694087982177734\n",
      "8e-07\n",
      "Epoch [117/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.832485594377431e-07, Correlation: 0.9999999085642669, R2: 0.9999998167514454\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.763850212097168\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.842763900756836\n",
      "Gradient Norm_Of_Each_Mini_Batch: 24.662593841552734\n",
      "Gradient Norm_Batch: 11.033615112304688\n",
      "8e-07\n",
      "Epoch [118/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833734444289803e-07, Correlation: 0.999999908571009, R2: 0.999999816626556\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.929733276367188\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.858280181884766\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.963117599487305\n",
      "Gradient Norm_Batch: 10.54417610168457\n",
      "8e-07\n",
      "Epoch [119/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8334050366775045e-07, Correlation: 0.9999999085686365, R2: 0.9999998166594993\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.923699378967285\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.672504425048828\n",
      "Gradient Norm_Of_Each_Mini_Batch: 12.43089485168457\n",
      "Gradient Norm_Batch: 10.485806465148926\n",
      "8e-07\n",
      "Epoch [120/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.83340176818092e-07, Correlation: 0.9999999085665741, R2: 0.9999998166598343\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.26479148864746\n",
      "Gradient Norm_Of_Each_Mini_Batch: 24.261781692504883\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.976306915283203\n",
      "Gradient Norm_Batch: 8.352860450744629\n",
      "8e-07\n",
      "Epoch [121/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8324786310586205e-07, Correlation: 0.9999999085526601, R2: 0.9999998167521369\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.96816635131836\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.79969024658203\n",
      "Gradient Norm_Of_Each_Mini_Batch: 13.143217086791992\n",
      "Gradient Norm_Batch: 10.354376792907715\n",
      "8e-07\n",
      "Epoch [122/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8335717300033139e-07, Correlation: 0.9999999085617324, R2: 0.9999998166428131\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.873868942260742\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.1882266998291\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.86490821838379\n",
      "Gradient Norm_Batch: 10.797822952270508\n",
      "8e-07\n",
      "Epoch [123/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8337506446641783e-07, Correlation: 0.9999999085607234, R2: 0.9999998166249339\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.974010467529297\n",
      "Gradient Norm_Of_Each_Mini_Batch: 25.724597930908203\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.580541610717773\n",
      "Gradient Norm_Batch: 10.474191665649414\n",
      "8e-07\n",
      "Epoch [124/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8334561957544793e-07, Correlation: 0.9999999085560277, R2: 0.9999998166543805\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.18596839904785\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.08179473876953\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.984413146972656\n",
      "Gradient Norm_Batch: 9.974363327026367\n",
      "8e-07\n",
      "Epoch [125/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8332563911371835e-07, Correlation: 0.9999999085449891, R2: 0.9999998166743583\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 26.041549682617188\n",
      "Gradient Norm_Of_Each_Mini_Batch: 27.009687423706055\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.34579086303711\n",
      "Gradient Norm_Batch: 9.721898078918457\n",
      "8e-07\n",
      "Epoch [126/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8329892270685377e-07, Correlation: 0.9999999085277701, R2: 0.9999998167010793\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.333847045898438\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.15492057800293\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.880443572998047\n",
      "Gradient Norm_Batch: 9.600590705871582\n",
      "8e-07\n",
      "Epoch [127/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8331201090404647e-07, Correlation: 0.9999999085327782, R2: 0.9999998166879869\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.584224700927734\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.94429588317871\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.607736587524414\n",
      "Gradient Norm_Batch: 10.814190864562988\n",
      "8e-07\n",
      "Epoch [128/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8338744212087477e-07, Correlation: 0.9999999085641775, R2: 0.9999998166125613\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 31.701723098754883\n",
      "Gradient Norm_Of_Each_Mini_Batch: 35.386966705322266\n",
      "Gradient Norm_Of_Each_Mini_Batch: 11.511991500854492\n",
      "Gradient Norm_Batch: 10.579896926879883\n",
      "8e-07\n",
      "Epoch [129/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833569029940918e-07, Correlation: 0.9999999085528436, R2: 0.9999998166431211\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.261104583740234\n",
      "Gradient Norm_Of_Each_Mini_Batch: 26.105581283569336\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.72673225402832\n",
      "Gradient Norm_Batch: 9.589505195617676\n",
      "8e-07\n",
      "Epoch [130/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8332022477807186e-07, Correlation: 0.9999999085541826, R2: 0.9999998166797983\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.35797691345215\n",
      "Gradient Norm_Of_Each_Mini_Batch: 13.259130477905273\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.931556701660156\n",
      "Gradient Norm_Batch: 9.936507225036621\n",
      "8e-07\n",
      "Epoch [131/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833415694818541e-07, Correlation: 0.9999999085503934, R2: 0.9999998166584391\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.24763298034668\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.591691970825195\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.755386352539062\n",
      "Gradient Norm_Batch: 10.761256217956543\n",
      "8e-07\n",
      "Epoch [132/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833939222706249e-07, Correlation: 0.9999999085571014, R2: 0.9999998166060673\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.585792541503906\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.39481544494629\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.055992126464844\n",
      "Gradient Norm_Batch: 10.070599555969238\n",
      "8e-07\n",
      "Epoch [133/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8335771301281056e-07, Correlation: 0.999999908553311, R2: 0.999999816642297\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.206195831298828\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.07314109802246\n",
      "Gradient Norm_Of_Each_Mini_Batch: 13.55942153930664\n",
      "Gradient Norm_Batch: 9.60645866394043\n",
      "8e-07\n",
      "Epoch [134/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8332525542064104e-07, Correlation: 0.9999999085565389, R2: 0.9999998166747375\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.274925231933594\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.832475662231445\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.626312255859375\n",
      "Gradient Norm_Batch: 11.515565872192383\n",
      "8e-07\n",
      "Epoch [135/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8341629015594663e-07, Correlation: 0.9999999085636136, R2: 0.9999998165836973\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.098983764648438\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.463272094726562\n",
      "Gradient Norm_Of_Each_Mini_Batch: 25.41277313232422\n",
      "Gradient Norm_Batch: 12.457347869873047\n",
      "8e-07\n",
      "Epoch [136/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.8348232799780817e-07, Correlation: 0.9999999085703033, R2: 0.9999998165176627\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.00290298461914\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.355077743530273\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.297523498535156\n",
      "Gradient Norm_Batch: 11.272141456604004\n",
      "8e-07\n",
      "Epoch [137/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8340116980652965e-07, Correlation: 0.9999999085675517, R2: 0.9999998165988243\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.292980194091797\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.18706703186035\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.776727676391602\n",
      "Gradient Norm_Batch: 11.966923713684082\n",
      "8e-07\n",
      "Epoch [138/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8345063779179327e-07, Correlation: 0.9999999085713799, R2: 0.9999998165493444\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.102581024169922\n",
      "Gradient Norm_Of_Each_Mini_Batch: 26.507156372070312\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.88429069519043\n",
      "Gradient Norm_Batch: 12.750125885009766\n",
      "8e-07\n",
      "Epoch [139/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.8349533092987258e-07, Correlation: 0.9999999085644585, R2: 0.9999998165046803\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 12.474475860595703\n",
      "Gradient Norm_Of_Each_Mini_Batch: 24.79349136352539\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.630334854125977\n",
      "Gradient Norm_Batch: 11.885917663574219\n",
      "8e-07\n",
      "Epoch [140/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8345305363709485e-07, Correlation: 0.9999999085635203, R2: 0.9999998165469492\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 25.090490341186523\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.014965057373047\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.03348159790039\n",
      "Gradient Norm_Batch: 11.303877830505371\n",
      "8e-07\n",
      "Epoch [141/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8339343910156458e-07, Correlation: 0.9999999085453117, R2: 0.999999816606562\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.76474952697754\n",
      "Gradient Norm_Of_Each_Mini_Batch: 29.394479751586914\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.157527923583984\n",
      "Gradient Norm_Batch: 9.467533111572266\n",
      "8e-07\n",
      "Epoch [142/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8331789419789857e-07, Correlation: 0.9999999085545629, R2: 0.9999998166820969\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.506929397583008\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.565702438354492\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.817642211914062\n",
      "Gradient Norm_Batch: 10.135714530944824\n",
      "8e-07\n",
      "Epoch [143/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8333346929466643e-07, Correlation: 0.9999999085598547, R2: 0.9999998166665328\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.937925338745117\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.335433959960938\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.459905624389648\n",
      "Gradient Norm_Batch: 10.819368362426758\n",
      "8e-07\n",
      "Epoch [144/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8336562845888693e-07, Correlation: 0.9999999085635592, R2: 0.9999998166343954\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.410805702209473\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.129310607910156\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.12177085876465\n",
      "Gradient Norm_Batch: 9.735469818115234\n",
      "8e-07\n",
      "Epoch [145/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8330575812797179e-07, Correlation: 0.9999999085634717, R2: 0.9999998166942594\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.621789932250977\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.554485321044922\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.729209899902344\n",
      "Gradient Norm_Batch: 9.460945129394531\n",
      "8e-07\n",
      "Epoch [146/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8328077544538246e-07, Correlation: 0.9999999085593622, R2: 0.9999998167192249\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.0725154876709\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.52882957458496\n",
      "Gradient Norm_Of_Each_Mini_Batch: 11.656721115112305\n",
      "Gradient Norm_Batch: 9.492431640625\n",
      "8e-07\n",
      "Epoch [147/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8328208284401626e-07, Correlation: 0.9999999085529663, R2: 0.9999998167179391\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.34841537475586\n",
      "Gradient Norm_Of_Each_Mini_Batch: 27.049800872802734\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.057458877563477\n",
      "Gradient Norm_Batch: 10.057765007019043\n",
      "8e-07\n",
      "Epoch [148/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8329953377360653e-07, Correlation: 0.9999999085519853, R2: 0.9999998167004914\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.364154815673828\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.17334747314453\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.217689514160156\n",
      "Gradient Norm_Batch: 8.97883415222168\n",
      "8e-07\n",
      "Epoch [149/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8327646955640375e-07, Correlation: 0.9999999085440073, R2: 0.9999998167235341\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.82889747619629\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.92150115966797\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.125991821289062\n",
      "Gradient Norm_Batch: 9.935943603515625\n",
      "8e-07\n",
      "Epoch [150/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8331553519601584e-07, Correlation: 0.9999999085503244, R2: 0.9999998166844635\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.10303497314453\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.851762771606445\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.58739185333252\n",
      "Gradient Norm_Batch: 9.219757080078125\n",
      "8e-07\n",
      "Epoch [151/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8328815087897965e-07, Correlation: 0.9999999085599774, R2: 0.9999998167118566\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 24.280641555786133\n",
      "Gradient Norm_Of_Each_Mini_Batch: 24.962932586669922\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.969219207763672\n",
      "Gradient Norm_Batch: 8.864748001098633\n",
      "8e-07\n",
      "Epoch [152/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8324938366731658e-07, Correlation: 0.9999999085628047, R2: 0.9999998167506124\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 12.800601959228516\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.105772018432617\n",
      "Gradient Norm_Of_Each_Mini_Batch: 10.545963287353516\n",
      "Gradient Norm_Batch: 9.688722610473633\n",
      "8e-07\n",
      "Epoch [153/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8331594731080258e-07, Correlation: 0.999999908564261, R2: 0.9999998166840683\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.418764114379883\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.12973976135254\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.726137161254883\n",
      "Gradient Norm_Batch: 8.434349060058594\n",
      "8e-07\n",
      "Epoch [154/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8324946893244487e-07, Correlation: 0.9999999085557569, R2: 0.9999998167505373\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 13.829595565795898\n",
      "Gradient Norm_Of_Each_Mini_Batch: 25.537113189697266\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.034509658813477\n",
      "Gradient Norm_Batch: 9.849672317504883\n",
      "8e-07\n",
      "Epoch [155/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8332873708004627e-07, Correlation: 0.9999999085635533, R2: 0.9999998166712492\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.50244903564453\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.5660343170166\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.198518753051758\n",
      "Gradient Norm_Batch: 10.134282112121582\n",
      "8e-07\n",
      "Epoch [156/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833483906921174e-07, Correlation: 0.9999999085553296, R2: 0.9999998166516281\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.90443992614746\n",
      "Gradient Norm_Of_Each_Mini_Batch: 28.1785831451416\n",
      "Gradient Norm_Of_Each_Mini_Batch: 11.15358829498291\n",
      "Gradient Norm_Batch: 10.110345840454102\n",
      "8e-07\n",
      "Epoch [157/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8333405193970975e-07, Correlation: 0.9999999085497013, R2: 0.9999998166659478\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.409164428710938\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.556859970092773\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.8441219329834\n",
      "Gradient Norm_Batch: 10.270909309387207\n",
      "8e-07\n",
      "Epoch [158/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8332990237013291e-07, Correlation: 0.9999999085550567, R2: 0.9999998166700987\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.987913131713867\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.336347579956055\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.786527633666992\n",
      "Gradient Norm_Batch: 12.109204292297363\n",
      "8e-07\n",
      "Epoch [159/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.834400080724663e-07, Correlation: 0.9999999085780249, R2: 0.9999998165599935\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.542682647705078\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.235153198242188\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.285931587219238\n",
      "Gradient Norm_Batch: 12.532125473022461\n",
      "8e-07\n",
      "Epoch [160/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.8348042374327633e-07, Correlation: 0.9999999085733605, R2: 0.9999998165195757\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.289990425109863\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.338379859924316\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.3659029006958\n",
      "Gradient Norm_Batch: 11.347006797790527\n",
      "8e-07\n",
      "Epoch [161/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8341745544603327e-07, Correlation: 0.9999999085699081, R2: 0.9999998165825371\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.071640014648438\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.897258758544922\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.478515625\n",
      "Gradient Norm_Batch: 10.134095191955566\n",
      "8e-07\n",
      "Epoch [162/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8334117157792207e-07, Correlation: 0.9999999085662039, R2: 0.9999998166588363\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.013202667236328\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.122411727905273\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.864803314208984\n",
      "Gradient Norm_Batch: 9.408432006835938\n",
      "8e-07\n",
      "Epoch [163/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8330796081045264e-07, Correlation: 0.9999999085541597, R2: 0.9999998166920369\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.8095645904541\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.54681968688965\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.5993595123291\n",
      "Gradient Norm_Batch: 10.697005271911621\n",
      "8e-07\n",
      "Epoch [164/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833783045412929e-07, Correlation: 0.9999999085594515, R2: 0.9999998166217057\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.387346267700195\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.542051315307617\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.606955528259277\n",
      "Gradient Norm_Batch: 10.935133934020996\n",
      "8e-07\n",
      "Epoch [165/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8339115115395543e-07, Correlation: 0.9999999085676384, R2: 0.9999998166088525\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.890892028808594\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.90301513671875\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.72356414794922\n",
      "Gradient Norm_Batch: 12.767960548400879\n",
      "8e-07\n",
      "Epoch [166/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.8348295327541564e-07, Correlation: 0.9999999085791208, R2: 0.9999998165170491\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.405803680419922\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.90950584411621\n",
      "Gradient Norm_Of_Each_Mini_Batch: 12.107205390930176\n",
      "Gradient Norm_Batch: 10.585816383361816\n",
      "8e-07\n",
      "Epoch [167/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8336743323743576e-07, Correlation: 0.9999999085684109, R2: 0.9999998166325692\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.03558349609375\n",
      "Gradient Norm_Of_Each_Mini_Batch: 24.21156120300293\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.44950294494629\n",
      "Gradient Norm_Batch: 10.956010818481445\n",
      "8e-07\n",
      "Epoch [168/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.83382041996083e-07, Correlation: 0.9999999085677354, R2: 0.9999998166179535\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.483352661132812\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.755762100219727\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.21590232849121\n",
      "Gradient Norm_Batch: 10.533085823059082\n",
      "8e-07\n",
      "Epoch [169/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8335724405460496e-07, Correlation: 0.9999999085651884, R2: 0.9999998166427544\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.190521240234375\n",
      "Gradient Norm_Of_Each_Mini_Batch: 24.39702606201172\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.17371940612793\n",
      "Gradient Norm_Batch: 11.54927921295166\n",
      "8e-07\n",
      "Epoch [170/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8342890939493373e-07, Correlation: 0.9999999085629484, R2: 0.9999998165711\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.060991287231445\n",
      "Gradient Norm_Of_Each_Mini_Batch: 25.54059600830078\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.027206420898438\n",
      "Gradient Norm_Batch: 10.680363655090332\n",
      "8e-07\n",
      "Epoch [171/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833685985275224e-07, Correlation: 0.9999999085651021, R2: 0.9999998166313796\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.4669189453125\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.809541702270508\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.031721115112305\n",
      "Gradient Norm_Batch: 11.309916496276855\n",
      "8e-07\n",
      "Epoch [172/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.834001892575543e-07, Correlation: 0.9999999085705389, R2: 0.9999998165998215\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.121421813964844\n",
      "Gradient Norm_Of_Each_Mini_Batch: 24.912734985351562\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.195147514343262\n",
      "Gradient Norm_Batch: 10.114446640014648\n",
      "8e-07\n",
      "Epoch [173/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833267191386767e-07, Correlation: 0.9999999085669267, R2: 0.9999998166732782\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.282442092895508\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.52313995361328\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.661375999450684\n",
      "Gradient Norm_Batch: 10.00583553314209\n",
      "8e-07\n",
      "Epoch [174/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8331145668071258e-07, Correlation: 0.9999999085665996, R2: 0.9999998166885625\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.158628463745117\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.767005920410156\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.941604614257812\n",
      "Gradient Norm_Batch: 10.867192268371582\n",
      "8e-07\n",
      "Epoch [175/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8335686036152765e-07, Correlation: 0.9999999085696404, R2: 0.9999998166431605\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 24.284103393554688\n",
      "Gradient Norm_Of_Each_Mini_Batch: 31.925918579101562\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.032469749450684\n",
      "Gradient Norm_Batch: 11.048453330993652\n",
      "8e-07\n",
      "Epoch [176/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8340450935738772e-07, Correlation: 0.9999999085632876, R2: 0.9999998165955108\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 12.164651870727539\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.115385055541992\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.74970531463623\n",
      "Gradient Norm_Batch: 10.422114372253418\n",
      "8e-07\n",
      "Epoch [177/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8336454843392858e-07, Correlation: 0.9999999085643909, R2: 0.9999998166354601\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.801942825317383\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.904138565063477\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.143440246582031\n",
      "Gradient Norm_Batch: 10.277338981628418\n",
      "8e-07\n",
      "Epoch [178/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8336430684939842e-07, Correlation: 0.9999999085557395, R2: 0.9999998166356993\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 25.160404205322266\n",
      "Gradient Norm_Of_Each_Mini_Batch: 28.776165008544922\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.816932678222656\n",
      "Gradient Norm_Batch: 12.531697273254395\n",
      "8e-07\n",
      "Epoch [179/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.8349088293234672e-07, Correlation: 0.9999999085665557, R2: 0.9999998165091288\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.120147705078125\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.68651008605957\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.725977897644043\n",
      "Gradient Norm_Batch: 10.928359031677246\n",
      "8e-07\n",
      "Epoch [180/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8339405016831734e-07, Correlation: 0.999999908563902, R2: 0.9999998166059705\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.061141967773438\n",
      "Gradient Norm_Of_Each_Mini_Batch: 30.43594741821289\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.181278228759766\n",
      "Gradient Norm_Batch: 11.067883491516113\n",
      "8e-07\n",
      "Epoch [181/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8339258645028167e-07, Correlation: 0.9999999085573091, R2: 0.9999998166074338\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.50238609313965\n",
      "Gradient Norm_Of_Each_Mini_Batch: 28.516653060913086\n",
      "Gradient Norm_Of_Each_Mini_Batch: 12.357597351074219\n",
      "Gradient Norm_Batch: 10.890240669250488\n",
      "8e-07\n",
      "Epoch [182/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8339650864618307e-07, Correlation: 0.9999999085499657, R2: 0.9999998166034912\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.297008514404297\n",
      "Gradient Norm_Of_Each_Mini_Batch: 24.364070892333984\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.40406036376953\n",
      "Gradient Norm_Batch: 11.07097339630127\n",
      "8e-07\n",
      "Epoch [183/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8340301721764263e-07, Correlation: 0.9999999085641459, R2: 0.9999998165969897\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.98410987854004\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.578764915466309\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.76828956604004\n",
      "Gradient Norm_Batch: 10.3937406539917\n",
      "8e-07\n",
      "Epoch [184/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8334299056732561e-07, Correlation: 0.9999999085642357, R2: 0.999999816657024\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.00949478149414\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.255084991455078\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.865568161010742\n",
      "Gradient Norm_Batch: 9.16442584991455\n",
      "8e-07\n",
      "Epoch [185/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.832958531622353e-07, Correlation: 0.9999999085529107, R2: 0.9999998167041356\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.048412322998047\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.85618782043457\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.71186065673828\n",
      "Gradient Norm_Batch: 9.531977653503418\n",
      "8e-07\n",
      "Epoch [186/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833036122889098e-07, Correlation: 0.9999999085616066, R2: 0.999999816696398\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.887022972106934\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.063358306884766\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.643104553222656\n",
      "Gradient Norm_Batch: 10.0631742477417\n",
      "8e-07\n",
      "Epoch [187/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833322329503062e-07, Correlation: 0.9999999085671274, R2: 0.9999998166677883\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.139759063720703\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.57412338256836\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.522367477416992\n",
      "Gradient Norm_Batch: 10.595629692077637\n",
      "8e-07\n",
      "Epoch [188/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8338047880206432e-07, Correlation: 0.9999999085508192, R2: 0.9999998166195156\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.277074813842773\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.714173316955566\n",
      "Gradient Norm_Of_Each_Mini_Batch: 12.81973648071289\n",
      "Gradient Norm_Batch: 10.700939178466797\n",
      "8e-07\n",
      "Epoch [189/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8337536289436684e-07, Correlation: 0.9999999085640316, R2: 0.9999998166246316\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.559022903442383\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.38369369506836\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.076301574707031\n",
      "Gradient Norm_Batch: 10.77334213256836\n",
      "8e-07\n",
      "Epoch [190/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8338936058626132e-07, Correlation: 0.9999999085567177, R2: 0.9999998166106422\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.98226261138916\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.274654388427734\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.589406967163086\n",
      "Gradient Norm_Batch: 11.940205574035645\n",
      "8e-07\n",
      "Epoch [191/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8344336183417909e-07, Correlation: 0.9999999085719415, R2: 0.9999998165566408\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.839336395263672\n",
      "Gradient Norm_Of_Each_Mini_Batch: 32.383880615234375\n",
      "Gradient Norm_Of_Each_Mini_Batch: 30.142858505249023\n",
      "Gradient Norm_Batch: 10.250448226928711\n",
      "8e-07\n",
      "Epoch [192/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8334013418552786e-07, Correlation: 0.999999908549902, R2: 0.9999998166598725\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.682734489440918\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.6036319732666\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.860150337219238\n",
      "Gradient Norm_Batch: 10.093207359313965\n",
      "8e-07\n",
      "Epoch [193/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8334955598220404e-07, Correlation: 0.999999908556516, R2: 0.9999998166504445\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.625314712524414\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.683135986328125\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.208776473999023\n",
      "Gradient Norm_Batch: 9.6085786819458\n",
      "8e-07\n",
      "Epoch [194/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833237206483318e-07, Correlation: 0.9999999085552214, R2: 0.999999816676285\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.77897071838379\n",
      "Gradient Norm_Of_Each_Mini_Batch: 24.58360481262207\n",
      "Gradient Norm_Of_Each_Mini_Batch: 13.944029808044434\n",
      "Gradient Norm_Batch: 8.826784133911133\n",
      "8e-07\n",
      "Epoch [195/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8327254736050236e-07, Correlation: 0.9999999085578465, R2: 0.9999998167274499\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 13.363822937011719\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.261398315429688\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.72127628326416\n",
      "Gradient Norm_Batch: 10.451648712158203\n",
      "8e-07\n",
      "Epoch [196/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8335403240143933e-07, Correlation: 0.9999999085651395, R2: 0.9999998166459741\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.02829933166504\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.858383178710938\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.93317985534668\n",
      "Gradient Norm_Batch: 11.352997779846191\n",
      "8e-07\n",
      "Epoch [197/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8340895735491358e-07, Correlation: 0.9999999085657747, R2: 0.9999998165910261\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.43028450012207\n",
      "Gradient Norm_Of_Each_Mini_Batch: 24.258525848388672\n",
      "Gradient Norm_Of_Each_Mini_Batch: 25.874267578125\n",
      "Gradient Norm_Batch: 11.88025188446045\n",
      "8e-07\n",
      "Epoch [198/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.8346597130403097e-07, Correlation: 0.999999908550751, R2: 0.9999998165340237\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.276187896728516\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.59315299987793\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.321508407592773\n",
      "Gradient Norm_Batch: 12.874868392944336\n",
      "8e-07\n",
      "Epoch [199/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.8352766062434966e-07, Correlation: 0.9999999085492901, R2: 0.99999981647233\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.436141967773438\n",
      "Gradient Norm_Of_Each_Mini_Batch: 24.378429412841797\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.087934494018555\n",
      "Gradient Norm_Batch: 10.037759780883789\n",
      "8e-07\n",
      "Epoch [200/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833438290077538e-07, Correlation: 0.9999999085531188, R2: 0.9999998166561761\n",
      "Final gradient of the subproblem Core : 10.037759780883789\n",
      "(32, 32)\n",
      "---------------------------Testing with Best Lambda------------------------------\n",
      "Y Test Predicted:  [ 67.54944414  64.18962083 100.90644649 106.79448137  66.52883352\n",
      "  96.79614618 112.76403657  87.08125939  58.88512934  60.74056954\n",
      "  65.7534598   64.15707642  82.80160365  68.56402267 142.45887657\n",
      "  98.01972155  98.35293075  55.41632452  96.74733331  87.26442093\n",
      "  47.97817377  91.13721394  73.56515048  77.46466997  46.9643284\n",
      "  79.3802057   84.7106252   45.49444899  87.56685912  73.47963323\n",
      "  86.90812052  88.26721147 124.65329181  81.42689627  87.547559\n",
      "  82.79810649  98.73487064  69.03788473  80.63827699  93.08357382\n",
      "  68.40985355  87.27987502  62.04199761  57.34039866  56.02115032\n",
      "  89.3094113  110.5434086  134.61091222  43.26785902  74.48155214\n",
      "  96.23301792  87.89113435  79.23377426  90.72704251 125.88514951\n",
      " 160.37020302  60.51188345  86.88842712  89.57419739  85.49699443\n",
      "  79.97221691  95.54230965  57.09911608  50.36936041  70.86139382\n",
      " 131.23410185  94.51164535  83.16014869  62.92668176  84.10780904\n",
      "  76.25931076  96.48134431  77.08422055  65.79245106  71.77931363\n",
      "  65.14983518  61.02636719  68.75450296  66.41027724 104.9570191\n",
      "  79.74032942  91.80702115 110.44699432  62.24801554  57.61581702\n",
      "  98.12936234  67.83593479  72.12860867 127.85064951  63.30882931\n",
      " 103.69746325 150.07981671  67.41975998  92.38850804 123.67897925\n",
      "  46.91119569  64.66061138 108.22062049 101.13989356 113.37852827]\n",
      "Y Test Actual:  [ 67.54932471  64.19367989 100.89907103 106.80033802  66.5296898\n",
      "  96.80085769 112.75716496  87.08379176  58.86645104  60.75871896\n",
      "  65.76651181  64.14603363  82.80477245  68.55912818 142.45923938\n",
      "  98.00053671  98.36489423  55.40988723  96.73410604  87.25275894\n",
      "  47.97392502  91.14302569  73.57729565  77.46144622  46.97758628\n",
      "  79.38435102  84.7040345   45.4879342   87.55672081  73.49886432\n",
      "  86.90598491  88.25175908 124.63253198  81.43265772  87.55287141\n",
      "  82.8080788   98.72768177  69.0468243   80.62856336  93.08545506\n",
      "  68.40807412  87.26847367  62.04717769  57.33990441  56.01334774\n",
      "  89.31934273 110.55219975 134.61307846  43.26935217  74.47617341\n",
      "  96.21843559  87.89937403  79.21862917  90.71786386 125.8824493\n",
      " 160.38439066  60.50387741  86.87508783  89.5739495   85.50880057\n",
      "  79.97185552  95.54965657  57.09355445  50.35928763  70.86185745\n",
      " 131.24078613  94.50837386  83.15863626  62.93422433  84.09831139\n",
      "  76.26182003  96.48126457  77.07300387  65.77838739  71.76861932\n",
      "  65.14937109  61.0440168   68.74400314  66.41967439 104.95340089\n",
      "  79.75508697  91.80406117 110.44770514  62.24491669  57.60974417\n",
      "  98.13365195  67.82489044  72.14320497 127.85744229  63.30804812\n",
      " 103.70015697 150.07489827  67.42743244  92.3906977  123.66379363\n",
      "  46.92967844  64.66629945 108.22935093 101.14612067 113.38031778]\n",
      "Error Report on Testing _ With best Lambda\n",
      "SGD Learning Rate: 8e-07\n",
      "SGD_Alpha chosen for model:  2.5\n",
      "SGD_Test Normalized Estimation Error:  8.42588937518464e-09\n",
      "SGD_Test NMSE Loss:  1.1248859231505224e-08\n",
      "SGD_Test R2 Loss:  0.9999998440414636\n",
      "SGD_Test Correlation:  0.9999999222115704\n",
      "Objective Function Values 1110.0198425673607\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final gradient: 10.037759780883789\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------------------Learning Rate 7e-07 -----------------------------------------\n",
      "Objective Function Value: 3408906.126530059\n",
      "--------------------------------------------------------------BCD iteration 0 --------------------------------------------------------------\n",
      "---------------------------------------------Sep 0 Factor 0 -------------------------------------------------\n",
      "Iteration: 0, Separation Rank: 0, Factor Matrix: 0, Objective Function Value: 4178689.296386482\n",
      "---------------------------------------------Sep 0 Factor 1 -------------------------------------------------\n",
      "Iteration: 0, Separation Rank: 0, Factor Matrix: 1, Objective Function Value: 3488977.6201287964\n",
      "---------------------------------------------Sep 1 Factor 0 -------------------------------------------------\n",
      "Iteration: 0, Separation Rank: 1, Factor Matrix: 0, Objective Function Value: 3092821.460810512\n",
      "---------------------------------------------Sep 1 Factor 1 -------------------------------------------------\n",
      "Iteration: 0, Separation Rank: 1, Factor Matrix: 1, Objective Function Value: 4026588.625912\n",
      "---------------------------------------------Core-------------------------------------------------\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 347346.0\n",
      "Gradient Norm_Of_Each_Mini_Batch: 275818.125\n",
      "Gradient Norm_Of_Each_Mini_Batch: 106834.671875\n",
      "Gradient Norm_Batch: 486598.3125\n",
      "7e-07\n",
      "Epoch [1/200], Loss: 183950.1094, Gap to Optimality: 183950.1094, NMSE: 0.5332693457603455, Correlation: 0.7268739569406096, R2: 0.46673068856601774\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 186933.78125\n",
      "Gradient Norm_Of_Each_Mini_Batch: 162852.5625\n",
      "Gradient Norm_Of_Each_Mini_Batch: 73509.5\n",
      "Gradient Norm_Batch: 289539.15625\n",
      "7e-07\n",
      "Epoch [2/200], Loss: 71175.9453, Gap to Optimality: 71175.9453, NMSE: 0.2063359171152115, Correlation: 0.9287670265445006, R2: 0.7936640627120349\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 119250.9453125\n",
      "Gradient Norm_Of_Each_Mini_Batch: 92293.1328125\n",
      "Gradient Norm_Of_Each_Mini_Batch: 44040.39453125\n",
      "Gradient Norm_Batch: 179824.5625\n",
      "7e-07\n",
      "Epoch [3/200], Loss: 30116.3809, Gap to Optimality: 30116.3809, NMSE: 0.08730309456586838, Correlation: 0.9717225516010622, R2: 0.9126969071924457\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 65661.71875\n",
      "Gradient Norm_Of_Each_Mini_Batch: 66893.625\n",
      "Gradient Norm_Of_Each_Mini_Batch: 28468.72265625\n",
      "Gradient Norm_Batch: 116246.1015625\n",
      "7e-07\n",
      "Epoch [4/200], Loss: 13834.6738, Gap to Optimality: 13834.6738, NMSE: 0.04010153189301491, Correlation: 0.9864314773218811, R2: 0.959898472005588\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 37783.01171875\n",
      "Gradient Norm_Of_Each_Mini_Batch: 44349.98046875\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22357.9765625\n",
      "Gradient Norm_Batch: 78010.3046875\n",
      "7e-07\n",
      "Epoch [5/200], Loss: 6855.6694, Gap to Optimality: 6855.6694, NMSE: 0.019868768751621246, Correlation: 0.9927804567539047, R2: 0.9801312325209487\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 29981.66796875\n",
      "Gradient Norm_Of_Each_Mini_Batch: 26654.587890625\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14932.5146484375\n",
      "Gradient Norm_Batch: 54297.5\n",
      "7e-07\n",
      "Epoch [6/200], Loss: 3636.2434, Gap to Optimality: 3636.2434, NMSE: 0.010535230860114098, Correlation: 0.9959178417127822, R2: 0.9894647696472099\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23234.357421875\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18265.8203125\n",
      "Gradient Norm_Of_Each_Mini_Batch: 10243.8642578125\n",
      "Gradient Norm_Batch: 38989.203125\n",
      "7e-07\n",
      "Epoch [7/200], Loss: 2039.5673, Gap to Optimality: 2039.5673, NMSE: 0.005906177219003439, Correlation: 0.9975823965626358, R2: 0.9940938229057238\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15731.568359375\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14239.7451171875\n",
      "Gradient Norm_Of_Each_Mini_Batch: 7273.91015625\n",
      "Gradient Norm_Batch: 28732.56640625\n",
      "7e-07\n",
      "Epoch [8/200], Loss: 1198.7461, Gap to Optimality: 1198.7461, NMSE: 0.003468438284471631, Correlation: 0.9985170415437491, R2: 0.9965315617801352\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 10652.6630859375\n",
      "Gradient Norm_Of_Each_Mini_Batch: 11764.6748046875\n",
      "Gradient Norm_Of_Each_Mini_Batch: 5130.86376953125\n",
      "Gradient Norm_Batch: 21680.43359375\n",
      "7e-07\n",
      "Epoch [9/200], Loss: 733.4381, Gap to Optimality: 733.4381, NMSE: 0.0021193698048591614, Correlation: 0.9990600383339744, R2: 0.9978806302464918\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 9216.67578125\n",
      "Gradient Norm_Of_Each_Mini_Batch: 7304.541015625\n",
      "Gradient Norm_Of_Each_Mini_Batch: 3951.237060546875\n",
      "Gradient Norm_Batch: 16734.3515625\n",
      "7e-07\n",
      "Epoch [10/200], Loss: 464.8606, Gap to Optimality: 464.8606, NMSE: 0.0013406669022515416, Correlation: 0.9993904881932503, R2: 0.9986593330385398\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 7032.6669921875\n",
      "Gradient Norm_Of_Each_Mini_Batch: 5849.97216796875\n",
      "Gradient Norm_Of_Each_Mini_Batch: 3254.423828125\n",
      "Gradient Norm_Batch: 13144.82421875\n",
      "7e-07\n",
      "Epoch [11/200], Loss: 302.6887, Gap to Optimality: 302.6887, NMSE: 0.0008704602951183915, Correlation: 0.9995962464912131, R2: 0.9991295396407782\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 4531.00537109375\n",
      "Gradient Norm_Of_Each_Mini_Batch: 5178.154296875\n",
      "Gradient Norm_Of_Each_Mini_Batch: 3118.422607421875\n",
      "Gradient Norm_Batch: 10473.984375\n",
      "7e-07\n",
      "Epoch [12/200], Loss: 201.5009, Gap to Optimality: 201.5009, NMSE: 0.0005770671414211392, Correlation: 0.999728836125492, R2: 0.9994229328189776\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 4297.158203125\n",
      "Gradient Norm_Of_Each_Mini_Batch: 4042.664306640625\n",
      "Gradient Norm_Of_Each_Mini_Batch: 2069.26025390625\n",
      "Gradient Norm_Batch: 8444.236328125\n",
      "7e-07\n",
      "Epoch [13/200], Loss: 136.5930, Gap to Optimality: 136.5930, NMSE: 0.00038886177935637534, Correlation: 0.999815305159414, R2: 0.9996111382138185\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 4079.4228515625\n",
      "Gradient Norm_Of_Each_Mini_Batch: 3101.142578125\n",
      "Gradient Norm_Of_Each_Mini_Batch: 1150.3917236328125\n",
      "Gradient Norm_Batch: 6869.875\n",
      "7e-07\n",
      "Epoch [14/200], Loss: 94.0110, Gap to Optimality: 94.0110, NMSE: 0.00026538901147432625, Correlation: 0.9998729724922892, R2: 0.999734611014889\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 2879.209716796875\n",
      "Gradient Norm_Of_Each_Mini_Batch: 2576.15576171875\n",
      "Gradient Norm_Of_Each_Mini_Batch: 1346.7392578125\n",
      "Gradient Norm_Batch: 5636.7783203125\n",
      "7e-07\n",
      "Epoch [15/200], Loss: 65.7308, Gap to Optimality: 65.7308, NMSE: 0.00018338298832532018, Correlation: 0.9999115674555955, R2: 0.9998166170084168\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 2083.4541015625\n",
      "Gradient Norm_Of_Each_Mini_Batch: 2456.001708984375\n",
      "Gradient Norm_Of_Each_Mini_Batch: 957.3800659179688\n",
      "Gradient Norm_Batch: 4652.13427734375\n",
      "7e-07\n",
      "Epoch [16/200], Loss: 46.5762, Gap to Optimality: 46.5762, NMSE: 0.00012783741112798452, Correlation: 0.9999380103396485, R2: 0.9998721625871226\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 1866.9110107421875\n",
      "Gradient Norm_Of_Each_Mini_Batch: 1856.0419921875\n",
      "Gradient Norm_Of_Each_Mini_Batch: 905.0181884765625\n",
      "Gradient Norm_Batch: 3861.865478515625\n",
      "7e-07\n",
      "Epoch [17/200], Loss: 33.4793, Gap to Optimality: 33.4793, NMSE: 8.985710155684501e-05, Correlation: 0.9999562621389109, R2: 0.999910142895235\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 1522.4188232421875\n",
      "Gradient Norm_Of_Each_Mini_Batch: 1473.6802978515625\n",
      "Gradient Norm_Of_Each_Mini_Batch: 894.1454467773438\n",
      "Gradient Norm_Batch: 3220.773193359375\n",
      "7e-07\n",
      "Epoch [18/200], Loss: 24.4255, Gap to Optimality: 24.4255, NMSE: 6.360022962326184e-05, Correlation: 0.9999689038728188, R2: 0.9999363997721413\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 1348.0811767578125\n",
      "Gradient Norm_Of_Each_Mini_Batch: 1209.9483642578125\n",
      "Gradient Norm_Of_Each_Mini_Batch: 664.8267211914062\n",
      "Gradient Norm_Batch: 2696.330078125\n",
      "7e-07\n",
      "Epoch [19/200], Loss: 18.1084, Gap to Optimality: 18.1084, NMSE: 4.527924465946853e-05, Correlation: 0.9999777986726845, R2: 0.9999547207570183\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 1063.6812744140625\n",
      "Gradient Norm_Of_Each_Mini_Batch: 1089.96875\n",
      "Gradient Norm_Of_Each_Mini_Batch: 525.4638061523438\n",
      "Gradient Norm_Batch: 2265.299560546875\n",
      "7e-07\n",
      "Epoch [20/200], Loss: 13.6728, Gap to Optimality: 13.6728, NMSE: 3.2414711313322186e-05, Correlation: 0.9999840710852432, R2: 0.9999675852923563\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 1020.3919067382812\n",
      "Gradient Norm_Of_Each_Mini_Batch: 805.6836547851562\n",
      "Gradient Norm_Of_Each_Mini_Batch: 453.8270568847656\n",
      "Gradient Norm_Batch: 1907.95263671875\n",
      "7e-07\n",
      "Epoch [21/200], Loss: 10.5326, Gap to Optimality: 10.5326, NMSE: 2.330646566406358e-05, Correlation: 0.9999885222750325, R2: 0.9999766935329434\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 729.8439331054688\n",
      "Gradient Norm_Of_Each_Mini_Batch: 757.3565063476562\n",
      "Gradient Norm_Of_Each_Mini_Batch: 435.69744873046875\n",
      "Gradient Norm_Batch: 1611.169189453125\n",
      "7e-07\n",
      "Epoch [22/200], Loss: 8.3026, Gap to Optimality: 8.3026, NMSE: 1.6837822840898298e-05, Correlation: 0.9999916931330881, R2: 0.9999831621764848\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 632.650390625\n",
      "Gradient Norm_Of_Each_Mini_Batch: 662.3251953125\n",
      "Gradient Norm_Of_Each_Mini_Batch: 317.42144775390625\n",
      "Gradient Norm_Batch: 1363.4945068359375\n",
      "7e-07\n",
      "Epoch [23/200], Loss: 6.7092, Gap to Optimality: 6.7092, NMSE: 1.2215749848110136e-05, Correlation: 0.9999939683515416, R2: 0.9999877842499264\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 555.8452758789062\n",
      "Gradient Norm_Of_Each_Mini_Batch: 516.2803955078125\n",
      "Gradient Norm_Of_Each_Mini_Batch: 294.0997009277344\n",
      "Gradient Norm_Batch: 1156.1617431640625\n",
      "7e-07\n",
      "Epoch [24/200], Loss: 5.5677, Gap to Optimality: 5.5677, NMSE: 8.904071364668198e-06, Correlation: 0.9999955987222585, R2: 0.9999910959288439\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 444.8697204589844\n",
      "Gradient Norm_Of_Each_Mini_Batch: 482.1234130859375\n",
      "Gradient Norm_Of_Each_Mini_Batch: 253.8335418701172\n",
      "Gradient Norm_Batch: 981.8116455078125\n",
      "7e-07\n",
      "Epoch [25/200], Loss: 4.7451, Gap to Optimality: 4.7451, NMSE: 6.517368547065416e-06, Correlation: 0.9999967739997476, R2: 0.9999934826311495\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 440.67413330078125\n",
      "Gradient Norm_Of_Each_Mini_Batch: 363.2467956542969\n",
      "Gradient Norm_Of_Each_Mini_Batch: 177.95111083984375\n",
      "Gradient Norm_Batch: 835.0886840820312\n",
      "7e-07\n",
      "Epoch [26/200], Loss: 4.1511, Gap to Optimality: 4.1511, NMSE: 4.7939834075805265e-06, Correlation: 0.9999976259191674, R2: 0.999995206016577\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 304.871337890625\n",
      "Gradient Norm_Of_Each_Mini_Batch: 332.0831604003906\n",
      "Gradient Norm_Of_Each_Mini_Batch: 215.6891326904297\n",
      "Gradient Norm_Batch: 711.2464599609375\n",
      "7e-07\n",
      "Epoch [27/200], Loss: 3.7214, Gap to Optimality: 3.7214, NMSE: 3.5471659884933615e-06, Correlation: 0.9999982419250434, R2: 0.9999964528341252\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 306.4970703125\n",
      "Gradient Norm_Of_Each_Mini_Batch: 264.3542785644531\n",
      "Gradient Norm_Of_Each_Mini_Batch: 150.87252807617188\n",
      "Gradient Norm_Batch: 606.4171752929688\n",
      "7e-07\n",
      "Epoch [28/200], Loss: 3.4093, Gap to Optimality: 3.4093, NMSE: 2.6411232738610124e-06, Correlation: 0.9999986902814014, R2: 0.9999973588770186\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 239.06536865234375\n",
      "Gradient Norm_Of_Each_Mini_Batch: 242.92578125\n",
      "Gradient Norm_Of_Each_Mini_Batch: 125.87523651123047\n",
      "Gradient Norm_Batch: 517.873779296875\n",
      "7e-07\n",
      "Epoch [29/200], Loss: 3.1827, Gap to Optimality: 3.1827, NMSE: 1.9836165847664233e-06, Correlation: 0.9999990160022655, R2: 0.9999980163833643\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 250.06056213378906\n",
      "Gradient Norm_Of_Each_Mini_Batch: 181.23036193847656\n",
      "Gradient Norm_Of_Each_Mini_Batch: 97.89714813232422\n",
      "Gradient Norm_Batch: 442.39337158203125\n",
      "7e-07\n",
      "Epoch [30/200], Loss: 3.0172, Gap to Optimality: 3.0172, NMSE: 1.502912596151873e-06, Correlation: 0.9999992534797604, R2: 0.9999984970873874\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 185.99533081054688\n",
      "Gradient Norm_Of_Each_Mini_Batch: 163.36988830566406\n",
      "Gradient Norm_Of_Each_Mini_Batch: 114.15088653564453\n",
      "Gradient Norm_Batch: 378.42779541015625\n",
      "7e-07\n",
      "Epoch [31/200], Loss: 2.8963, Gap to Optimality: 2.8963, NMSE: 1.1521406122483313e-06, Correlation: 0.9999994281865094, R2: 0.9999988478593905\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 156.2998046875\n",
      "Gradient Norm_Of_Each_Mini_Batch: 136.42991638183594\n",
      "Gradient Norm_Of_Each_Mini_Batch: 88.94496154785156\n",
      "Gradient Norm_Batch: 323.9671325683594\n",
      "7e-07\n",
      "Epoch [32/200], Loss: 2.8082, Gap to Optimality: 2.8082, NMSE: 8.960358286458359e-07, Correlation: 0.9999995548268724, R2: 0.9999991039641803\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 123.06416320800781\n",
      "Gradient Norm_Of_Each_Mini_Batch: 136.7190704345703\n",
      "Gradient Norm_Of_Each_Mini_Batch: 69.81666564941406\n",
      "Gradient Norm_Batch: 277.4319152832031\n",
      "7e-07\n",
      "Epoch [33/200], Loss: 2.7433, Gap to Optimality: 2.7433, NMSE: 7.07673507349682e-07, Correlation: 0.9999996482331104, R2: 0.9999992923265499\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 101.22797393798828\n",
      "Gradient Norm_Of_Each_Mini_Batch: 119.78610229492188\n",
      "Gradient Norm_Of_Each_Mini_Batch: 62.0078125\n",
      "Gradient Norm_Batch: 237.82568359375\n",
      "7e-07\n",
      "Epoch [34/200], Loss: 2.6958, Gap to Optimality: 2.6958, NMSE: 5.696454650205851e-07, Correlation: 0.9999997168272152, R2: 0.9999994303545451\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 110.53535461425781\n",
      "Gradient Norm_Of_Each_Mini_Batch: 81.76651763916016\n",
      "Gradient Norm_Of_Each_Mini_Batch: 50.5990104675293\n",
      "Gradient Norm_Batch: 204.08682250976562\n",
      "7e-07\n",
      "Epoch [35/200], Loss: 2.6609, Gap to Optimality: 2.6609, NMSE: 4.684030159296526e-07, Correlation: 0.9999997671458691, R2: 0.999999531596996\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 86.59644317626953\n",
      "Gradient Norm_Of_Each_Mini_Batch: 83.55228424072266\n",
      "Gradient Norm_Of_Each_Mini_Batch: 42.12533950805664\n",
      "Gradient Norm_Batch: 175.20831298828125\n",
      "7e-07\n",
      "Epoch [36/200], Loss: 2.6353, Gap to Optimality: 2.6353, NMSE: 3.9386310390909784e-07, Correlation: 0.9999998041599534, R2: 0.9999996061369106\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 83.97288513183594\n",
      "Gradient Norm_Of_Each_Mini_Batch: 63.78560256958008\n",
      "Gradient Norm_Of_Each_Mini_Batch: 42.773948669433594\n",
      "Gradient Norm_Batch: 150.43614196777344\n",
      "7e-07\n",
      "Epoch [37/200], Loss: 2.6164, Gap to Optimality: 2.6164, NMSE: 3.388182960861741e-07, Correlation: 0.9999998314152683, R2: 0.9999996611817004\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 56.91852569580078\n",
      "Gradient Norm_Of_Each_Mini_Batch: 65.00524139404297\n",
      "Gradient Norm_Of_Each_Mini_Batch: 36.98017501831055\n",
      "Gradient Norm_Batch: 129.35743713378906\n",
      "7e-07\n",
      "Epoch [38/200], Loss: 2.6025, Gap to Optimality: 2.6025, NMSE: 2.983396143463324e-07, Correlation: 0.9999998515972357, R2: 0.9999997016603823\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 59.46677017211914\n",
      "Gradient Norm_Of_Each_Mini_Batch: 52.67923355102539\n",
      "Gradient Norm_Of_Each_Mini_Batch: 36.745361328125\n",
      "Gradient Norm_Batch: 111.2417984008789\n",
      "7e-07\n",
      "Epoch [39/200], Loss: 2.5922, Gap to Optimality: 2.5922, NMSE: 2.6837957989300776e-07, Correlation: 0.999999866475304, R2: 0.9999997316204128\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 44.057857513427734\n",
      "Gradient Norm_Of_Each_Mini_Batch: 49.318878173828125\n",
      "Gradient Norm_Of_Each_Mini_Batch: 28.106781005859375\n",
      "Gradient Norm_Batch: 95.7606201171875\n",
      "7e-07\n",
      "Epoch [40/200], Loss: 2.5846, Gap to Optimality: 2.5846, NMSE: 2.4628636197121523e-07, Correlation: 0.999999877441594, R2: 0.999999753713634\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 45.21076965332031\n",
      "Gradient Norm_Of_Each_Mini_Batch: 39.56766891479492\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.925626754760742\n",
      "Gradient Norm_Batch: 82.30215454101562\n",
      "7e-07\n",
      "Epoch [41/200], Loss: 2.5789, Gap to Optimality: 2.5789, NMSE: 2.2989098624748294e-07, Correlation: 0.9999998855040468, R2: 0.9999997701090095\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 40.699195861816406\n",
      "Gradient Norm_Of_Each_Mini_Batch: 35.692466735839844\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.940473556518555\n",
      "Gradient Norm_Batch: 71.20718383789062\n",
      "7e-07\n",
      "Epoch [42/200], Loss: 2.5749, Gap to Optimality: 2.5749, NMSE: 2.1799695559820975e-07, Correlation: 0.9999998914602984, R2: 0.9999997820030562\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 36.56084060668945\n",
      "Gradient Norm_Of_Each_Mini_Batch: 35.41990661621094\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.035165786743164\n",
      "Gradient Norm_Batch: 61.36037063598633\n",
      "7e-07\n",
      "Epoch [43/200], Loss: 2.5718, Gap to Optimality: 2.5718, NMSE: 2.0905694952944032e-07, Correlation: 0.9999998958275943, R2: 0.9999997909430326\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 29.88686752319336\n",
      "Gradient Norm_Of_Each_Mini_Batch: 34.041534423828125\n",
      "Gradient Norm_Of_Each_Mini_Batch: 24.94213104248047\n",
      "Gradient Norm_Batch: 52.956295013427734\n",
      "7e-07\n",
      "Epoch [44/200], Loss: 2.5695, Gap to Optimality: 2.5695, NMSE: 2.023916039206597e-07, Correlation: 0.999999899126849, R2: 0.9999997976084112\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 31.587364196777344\n",
      "Gradient Norm_Of_Each_Mini_Batch: 39.45407485961914\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.772254943847656\n",
      "Gradient Norm_Batch: 45.92449188232422\n",
      "7e-07\n",
      "Epoch [45/200], Loss: 2.5678, Gap to Optimality: 2.5678, NMSE: 1.9750692104025802e-07, Correlation: 0.9999999015477984, R2: 0.9999998024930817\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.530397415161133\n",
      "Gradient Norm_Of_Each_Mini_Batch: 24.71729850769043\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.873706817626953\n",
      "Gradient Norm_Batch: 39.89257049560547\n",
      "7e-07\n",
      "Epoch [46/200], Loss: 2.5666, Gap to Optimality: 2.5666, NMSE: 1.9387572081086546e-07, Correlation: 0.9999999033496589, R2: 0.9999998061242892\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.446739196777344\n",
      "Gradient Norm_Of_Each_Mini_Batch: 24.01709747314453\n",
      "Gradient Norm_Of_Each_Mini_Batch: 13.945711135864258\n",
      "Gradient Norm_Batch: 34.98733139038086\n",
      "7e-07\n",
      "Epoch [47/200], Loss: 2.5657, Gap to Optimality: 2.5657, NMSE: 1.912504643541979e-07, Correlation: 0.9999999046927934, R2: 0.9999998087495462\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 25.70502471923828\n",
      "Gradient Norm_Of_Each_Mini_Batch: 33.35343551635742\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.861162185668945\n",
      "Gradient Norm_Batch: 30.378402709960938\n",
      "7e-07\n",
      "Epoch [48/200], Loss: 2.5650, Gap to Optimality: 2.5650, NMSE: 1.891917520424613e-07, Correlation: 0.9999999056836054, R2: 0.9999998108082498\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.697553634643555\n",
      "Gradient Norm_Of_Each_Mini_Batch: 26.513904571533203\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.253217697143555\n",
      "Gradient Norm_Batch: 26.93025779724121\n",
      "7e-07\n",
      "Epoch [49/200], Loss: 2.5645, Gap to Optimality: 2.5645, NMSE: 1.8773333465560427e-07, Correlation: 0.9999999064305385, R2: 0.9999998122666683\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.654083251953125\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.038326263427734\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.345056533813477\n",
      "Gradient Norm_Batch: 23.76181411743164\n",
      "7e-07\n",
      "Epoch [50/200], Loss: 2.5641, Gap to Optimality: 2.5641, NMSE: 1.8660554701455112e-07, Correlation: 0.9999999069781332, R2: 0.999999813394448\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.594493865966797\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.180408477783203\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.94587516784668\n",
      "Gradient Norm_Batch: 21.9052791595459\n",
      "7e-07\n",
      "Epoch [51/200], Loss: 2.5639, Gap to Optimality: 2.5639, NMSE: 1.8586793260055856e-07, Correlation: 0.9999999073964538, R2: 0.9999998141320612\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.17048454284668\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.46531105041504\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.274255752563477\n",
      "Gradient Norm_Batch: 19.18260955810547\n",
      "7e-07\n",
      "Epoch [52/200], Loss: 2.5636, Gap to Optimality: 2.5636, NMSE: 1.8520175615321932e-07, Correlation: 0.9999999076814433, R2: 0.9999998147982341\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.06183433532715\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.565555572509766\n",
      "Gradient Norm_Of_Each_Mini_Batch: 13.197718620300293\n",
      "Gradient Norm_Batch: 17.62771224975586\n",
      "7e-07\n",
      "Epoch [53/200], Loss: 2.5635, Gap to Optimality: 2.5635, NMSE: 1.8476207230833097e-07, Correlation: 0.9999999079081533, R2: 0.9999998152379209\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.890438079833984\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.02813148498535\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.219389915466309\n",
      "Gradient Norm_Batch: 16.249120712280273\n",
      "7e-07\n",
      "Epoch [54/200], Loss: 2.5634, Gap to Optimality: 2.5634, NMSE: 1.8442413818320347e-07, Correlation: 0.9999999080736808, R2: 0.9999998155758688\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.77113151550293\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.121065139770508\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.853841781616211\n",
      "Gradient Norm_Batch: 14.651577949523926\n",
      "7e-07\n",
      "Epoch [55/200], Loss: 2.5633, Gap to Optimality: 2.5633, NMSE: 1.8412777080811793e-07, Correlation: 0.9999999081954732, R2: 0.9999998158722371\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.225605010986328\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.69330596923828\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.548755645751953\n",
      "Gradient Norm_Batch: 13.323749542236328\n",
      "7e-07\n",
      "Epoch [56/200], Loss: 2.5632, Gap to Optimality: 2.5632, NMSE: 1.839045182805421e-07, Correlation: 0.99999990828808, R2: 0.9999998160954734\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.37114906311035\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.18145751953125\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.2731990814209\n",
      "Gradient Norm_Batch: 12.23819637298584\n",
      "7e-07\n",
      "Epoch [57/200], Loss: 2.5632, Gap to Optimality: 2.5632, NMSE: 1.8374210242200206e-07, Correlation: 0.9999999083419299, R2: 0.9999998162578979\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.799787521362305\n",
      "Gradient Norm_Of_Each_Mini_Batch: 28.570415496826172\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.876962661743164\n",
      "Gradient Norm_Batch: 11.485468864440918\n",
      "7e-07\n",
      "Epoch [58/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.8362652554060332e-07, Correlation: 0.999999908402768, R2: 0.9999998163734736\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.198030471801758\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.891881942749023\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.577775955200195\n",
      "Gradient Norm_Batch: 12.33625316619873\n",
      "7e-07\n",
      "Epoch [59/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.8363189724368567e-07, Correlation: 0.9999999084529787, R2: 0.9999998163681025\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.740617752075195\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.863054275512695\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.700716972351074\n",
      "Gradient Norm_Batch: 12.186980247497559\n",
      "7e-07\n",
      "Epoch [60/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.8359067155415687e-07, Correlation: 0.9999999084778846, R2: 0.9999998164093303\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.7283878326416\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.96432876586914\n",
      "Gradient Norm_Of_Each_Mini_Batch: 24.710031509399414\n",
      "Gradient Norm_Batch: 13.077861785888672\n",
      "7e-07\n",
      "Epoch [61/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.836072556216095e-07, Correlation: 0.9999999085046684, R2: 0.9999998163927473\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 30.605438232421875\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.796459197998047\n",
      "Gradient Norm_Of_Each_Mini_Batch: 24.803020477294922\n",
      "Gradient Norm_Batch: 14.934115409851074\n",
      "7e-07\n",
      "Epoch [62/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.836888543493842e-07, Correlation: 0.9999999085232311, R2: 0.999999816311142\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.20775604248047\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.547836303710938\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.556355476379395\n",
      "Gradient Norm_Batch: 11.679758071899414\n",
      "7e-07\n",
      "Epoch [63/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.8349886943269667e-07, Correlation: 0.9999999085270329, R2: 0.9999998165011386\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.825210571289062\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.616186141967773\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.74178409576416\n",
      "Gradient Norm_Batch: 11.265830993652344\n",
      "7e-07\n",
      "Epoch [64/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.8345180308187992e-07, Correlation: 0.9999999085376678, R2: 0.9999998165481844\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.964170455932617\n",
      "Gradient Norm_Of_Each_Mini_Batch: 25.208717346191406\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.4749813079834\n",
      "Gradient Norm_Batch: 11.275476455688477\n",
      "7e-07\n",
      "Epoch [65/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8343882857152494e-07, Correlation: 0.9999999085480578, R2: 0.9999998165611804\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.921158790588379\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.417144775390625\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.536361694335938\n",
      "Gradient Norm_Batch: 10.589088439941406\n",
      "7e-07\n",
      "Epoch [66/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833753202618027e-07, Correlation: 0.9999999085559953, R2: 0.9999998166246847\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.639127731323242\n",
      "Gradient Norm_Of_Each_Mini_Batch: 13.771143913269043\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.844697952270508\n",
      "Gradient Norm_Batch: 10.578235626220703\n",
      "7e-07\n",
      "Epoch [67/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.83380095108987e-07, Correlation: 0.9999999085513399, R2: 0.9999998166199126\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.943758010864258\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.83253288269043\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.76263427734375\n",
      "Gradient Norm_Batch: 10.478684425354004\n",
      "7e-07\n",
      "Epoch [68/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8337387075462175e-07, Correlation: 0.9999999085563654, R2: 0.9999998166261138\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.09520721435547\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.205930709838867\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.988353729248047\n",
      "Gradient Norm_Batch: 10.895089149475098\n",
      "7e-07\n",
      "Epoch [69/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8339703444780753e-07, Correlation: 0.9999999085579989, R2: 0.999999816602989\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.797937393188477\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.322242736816406\n",
      "Gradient Norm_Of_Each_Mini_Batch: 24.286991119384766\n",
      "Gradient Norm_Batch: 12.40966510772705\n",
      "7e-07\n",
      "Epoch [70/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.8348724495353963e-07, Correlation: 0.9999999085613905, R2: 0.999999816512752\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.83032989501953\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.147371292114258\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.554373741149902\n",
      "Gradient Norm_Batch: 11.30553913116455\n",
      "7e-07\n",
      "Epoch [71/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8340985263876064e-07, Correlation: 0.9999999085667935, R2: 0.9999998165901307\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.291831970214844\n",
      "Gradient Norm_Of_Each_Mini_Batch: 25.13199234008789\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.121352195739746\n",
      "Gradient Norm_Batch: 10.93283748626709\n",
      "7e-07\n",
      "Epoch [72/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833898153336122e-07, Correlation: 0.9999999085674168, R2: 0.9999998166101695\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 11.650425910949707\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.257957458496094\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.90835189819336\n",
      "Gradient Norm_Batch: 10.652857780456543\n",
      "7e-07\n",
      "Epoch [73/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8336471896418516e-07, Correlation: 0.9999999085685626, R2: 0.9999998166352781\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.677600860595703\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.635587692260742\n",
      "Gradient Norm_Of_Each_Mini_Batch: 11.840944290161133\n",
      "Gradient Norm_Batch: 11.277897834777832\n",
      "7e-07\n",
      "Epoch [74/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8341548013722786e-07, Correlation: 0.9999999085682448, R2: 0.9999998165845265\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.163579940795898\n",
      "Gradient Norm_Of_Each_Mini_Batch: 13.905350685119629\n",
      "Gradient Norm_Of_Each_Mini_Batch: 12.118674278259277\n",
      "Gradient Norm_Batch: 11.147452354431152\n",
      "7e-07\n",
      "Epoch [75/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8340571728003852e-07, Correlation: 0.9999999085677983, R2: 0.9999998165942863\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.692964553833008\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.78584098815918\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.249534606933594\n",
      "Gradient Norm_Batch: 10.738568305969238\n",
      "7e-07\n",
      "Epoch [76/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833772813597534e-07, Correlation: 0.9999999085605131, R2: 0.999999816622715\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.061119079589844\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.94097137451172\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.172334671020508\n",
      "Gradient Norm_Batch: 10.889955520629883\n",
      "7e-07\n",
      "Epoch [77/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833804645912096e-07, Correlation: 0.9999999085652449, R2: 0.9999998166195354\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.947214126586914\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.68271255493164\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.981502532958984\n",
      "Gradient Norm_Batch: 11.28859806060791\n",
      "7e-07\n",
      "Epoch [78/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8340020346840902e-07, Correlation: 0.9999999085727982, R2: 0.9999998165997857\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.256486892700195\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.359647750854492\n",
      "Gradient Norm_Of_Each_Mini_Batch: 11.686071395874023\n",
      "Gradient Norm_Batch: 10.061455726623535\n",
      "7e-07\n",
      "Epoch [79/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833329719147514e-07, Correlation: 0.9999999085700146, R2: 0.999999816667035\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.576451301574707\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.976993560791016\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.94190216064453\n",
      "Gradient Norm_Batch: 9.028956413269043\n",
      "7e-07\n",
      "Epoch [80/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8327992279409955e-07, Correlation: 0.999999908561444, R2: 0.9999998167200775\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.157991409301758\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.44978904724121\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.45107078552246\n",
      "Gradient Norm_Batch: 10.595147132873535\n",
      "7e-07\n",
      "Epoch [81/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8336095308768563e-07, Correlation: 0.999999908570493, R2: 0.9999998166390357\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 24.262609481811523\n",
      "Gradient Norm_Of_Each_Mini_Batch: 25.710819244384766\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.587064743041992\n",
      "Gradient Norm_Batch: 11.43364143371582\n",
      "7e-07\n",
      "Epoch [82/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8341098950713786e-07, Correlation: 0.9999999085604213, R2: 0.9999998165890006\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.577686309814453\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.91704750061035\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.366668701171875\n",
      "Gradient Norm_Batch: 10.827091217041016\n",
      "7e-07\n",
      "Epoch [83/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8337564711146115e-07, Correlation: 0.9999999085620852, R2: 0.9999998166243643\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 13.522110939025879\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.772850036621094\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.3190975189209\n",
      "Gradient Norm_Batch: 10.30875301361084\n",
      "7e-07\n",
      "Epoch [84/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8334544904519134e-07, Correlation: 0.9999999085675291, R2: 0.9999998166545496\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.876849174499512\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.57303810119629\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.515201568603516\n",
      "Gradient Norm_Batch: 10.940683364868164\n",
      "7e-07\n",
      "Epoch [85/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8338258200856217e-07, Correlation: 0.9999999085706356, R2: 0.9999998166174199\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.103609085083008\n",
      "Gradient Norm_Of_Each_Mini_Batch: 24.37307357788086\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.991806030273438\n",
      "Gradient Norm_Batch: 11.226241111755371\n",
      "7e-07\n",
      "Epoch [86/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.834106626574794e-07, Correlation: 0.9999999085675148, R2: 0.9999998165893469\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.43876075744629\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.25710105895996\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.103231430053711\n",
      "Gradient Norm_Batch: 11.376173973083496\n",
      "7e-07\n",
      "Epoch [87/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8341849283842748e-07, Correlation: 0.999999908565744, R2: 0.9999998165815074\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.81566047668457\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.47003173828125\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.48119068145752\n",
      "Gradient Norm_Batch: 10.73322868347168\n",
      "7e-07\n",
      "Epoch [88/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833647615967493e-07, Correlation: 0.9999999085713203, R2: 0.9999998166352482\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.694887161254883\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.048267364501953\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.651968955993652\n",
      "Gradient Norm_Batch: 9.123970985412598\n",
      "7e-07\n",
      "Epoch [89/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8328077544538246e-07, Correlation: 0.9999999085677499, R2: 0.9999998167192001\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.99065589904785\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.177600860595703\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.840351104736328\n",
      "Gradient Norm_Batch: 10.42244815826416\n",
      "7e-07\n",
      "Epoch [90/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8334490903271217e-07, Correlation: 0.9999999085716352, R2: 0.9999998166550903\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.687088012695312\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.45513343811035\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.7066707611084\n",
      "Gradient Norm_Batch: 10.5216646194458\n",
      "7e-07\n",
      "Epoch [91/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833670921769226e-07, Correlation: 0.9999999085657519, R2: 0.9999998166328902\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.373992919921875\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.521947860717773\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.2169246673584\n",
      "Gradient Norm_Batch: 9.24596881866455\n",
      "7e-07\n",
      "Epoch [92/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.832995053518971e-07, Correlation: 0.9999999085516156, R2: 0.9999998167004936\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.508098602294922\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.77096176147461\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.882387161254883\n",
      "Gradient Norm_Batch: 8.544208526611328\n",
      "7e-07\n",
      "Epoch [93/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8326414874536567e-07, Correlation: 0.9999999085484186, R2: 0.9999998167358283\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.830461502075195\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.37030601501465\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.730194091796875\n",
      "Gradient Norm_Batch: 9.862710952758789\n",
      "7e-07\n",
      "Epoch [94/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8334225160288042e-07, Correlation: 0.9999999085510028, R2: 0.9999998166577455\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.612053871154785\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.930025100708008\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.841306686401367\n",
      "Gradient Norm_Batch: 10.538579940795898\n",
      "7e-07\n",
      "Epoch [95/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8337611606966675e-07, Correlation: 0.9999999085607066, R2: 0.9999998166238765\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.101444244384766\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.46775245666504\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.350142478942871\n",
      "Gradient Norm_Batch: 11.600872039794922\n",
      "7e-07\n",
      "Epoch [96/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8343759222716471e-07, Correlation: 0.999999908566489, R2: 0.9999998165624043\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.331754684448242\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.88092803955078\n",
      "Gradient Norm_Of_Each_Mini_Batch: 11.216215133666992\n",
      "Gradient Norm_Batch: 11.577654838562012\n",
      "7e-07\n",
      "Epoch [97/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8343568797263288e-07, Correlation: 0.9999999085681963, R2: 0.9999998165643065\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.540388107299805\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.107519149780273\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.381840705871582\n",
      "Gradient Norm_Batch: 11.65493392944336\n",
      "7e-07\n",
      "Epoch [98/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8344040597639832e-07, Correlation: 0.9999999085684802, R2: 0.9999998165595823\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.6362247467041\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.011425018310547\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.290367126464844\n",
      "Gradient Norm_Batch: 12.067651748657227\n",
      "7e-07\n",
      "Epoch [99/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8345426155974565e-07, Correlation: 0.999999908574312, R2: 0.9999998165457257\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.708459854125977\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.621925354003906\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.260656356811523\n",
      "Gradient Norm_Batch: 9.438600540161133\n",
      "7e-07\n",
      "Epoch [100/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8330246120967786e-07, Correlation: 0.999999908564145, R2: 0.9999998166975375\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 27.234071731567383\n",
      "Gradient Norm_Of_Each_Mini_Batch: 25.0382137298584\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.899124145507812\n",
      "Gradient Norm_Batch: 10.64856243133545\n",
      "7e-07\n",
      "Epoch [101/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833419531749314e-07, Correlation: 0.9999999085674794, R2: 0.9999998166580361\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.59768295288086\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.140220642089844\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.090852737426758\n",
      "Gradient Norm_Batch: 9.570067405700684\n",
      "7e-07\n",
      "Epoch [102/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.832991358696745e-07, Correlation: 0.9999999085648718, R2: 0.9999998167008541\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.52522850036621\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.685667037963867\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.133485794067383\n",
      "Gradient Norm_Batch: 9.751110076904297\n",
      "7e-07\n",
      "Epoch [103/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8331384410430474e-07, Correlation: 0.9999999085658076, R2: 0.9999998166861491\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.750844955444336\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.254718780517578\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.454736709594727\n",
      "Gradient Norm_Batch: 10.26652717590332\n",
      "7e-07\n",
      "Epoch [104/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8334178264467482e-07, Correlation: 0.9999999085685792, R2: 0.9999998166582144\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.23916244506836\n",
      "Gradient Norm_Of_Each_Mini_Batch: 27.909040451049805\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.15496826171875\n",
      "Gradient Norm_Batch: 11.374518394470215\n",
      "7e-07\n",
      "Epoch [105/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8340369933866896e-07, Correlation: 0.99999990857066, R2: 0.9999998165962982\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.987302780151367\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.781282424926758\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.829683303833008\n",
      "Gradient Norm_Batch: 9.893882751464844\n",
      "7e-07\n",
      "Epoch [106/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8332097795337177e-07, Correlation: 0.9999999085668037, R2: 0.9999998166790208\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 25.642414093017578\n",
      "Gradient Norm_Of_Each_Mini_Batch: 28.486604690551758\n",
      "Gradient Norm_Of_Each_Mini_Batch: 10.718180656433105\n",
      "Gradient Norm_Batch: 9.711718559265137\n",
      "7e-07\n",
      "Epoch [107/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8332713125346345e-07, Correlation: 0.9999999085587002, R2: 0.9999998166728529\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.128599166870117\n",
      "Gradient Norm_Of_Each_Mini_Batch: 24.746294021606445\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.352721214294434\n",
      "Gradient Norm_Batch: 10.212807655334473\n",
      "7e-07\n",
      "Epoch [108/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8334685591980815e-07, Correlation: 0.9999999085642975, R2: 0.9999998166531492\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 24.820131301879883\n",
      "Gradient Norm_Of_Each_Mini_Batch: 30.145830154418945\n",
      "Gradient Norm_Of_Each_Mini_Batch: 13.279239654541016\n",
      "Gradient Norm_Batch: 10.02071475982666\n",
      "7e-07\n",
      "Epoch [109/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8333369666834187e-07, Correlation: 0.9999999085611256, R2: 0.9999998166662989\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.805590629577637\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.011645317077637\n",
      "Gradient Norm_Of_Each_Mini_Batch: 13.668763160705566\n",
      "Gradient Norm_Batch: 8.871392250061035\n",
      "7e-07\n",
      "Epoch [110/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.832732436923834e-07, Correlation: 0.9999999085615026, R2: 0.9999998167267451\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.951129913330078\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.005029678344727\n",
      "Gradient Norm_Of_Each_Mini_Batch: 12.597737312316895\n",
      "Gradient Norm_Batch: 9.235576629638672\n",
      "7e-07\n",
      "Epoch [111/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.832947305047128e-07, Correlation: 0.9999999085609659, R2: 0.9999998167052602\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 25.761884689331055\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.22777557373047\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.278841018676758\n",
      "Gradient Norm_Batch: 8.63767147064209\n",
      "7e-07\n",
      "Epoch [112/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.832567875226232e-07, Correlation: 0.9999999085563818, R2: 0.9999998167432103\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.25531578063965\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.06525993347168\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.91039276123047\n",
      "Gradient Norm_Batch: 8.8912992477417\n",
      "7e-07\n",
      "Epoch [113/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.832656693068202e-07, Correlation: 0.9999999085554145, R2: 0.9999998167343216\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.208091735839844\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.164220809936523\n",
      "Gradient Norm_Of_Each_Mini_Batch: 9.628022193908691\n",
      "Gradient Norm_Batch: 10.45966625213623\n",
      "7e-07\n",
      "Epoch [114/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833492291325456e-07, Correlation: 0.9999999085730845, R2: 0.9999998166507742\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.380556106567383\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.168811798095703\n",
      "Gradient Norm_Of_Each_Mini_Batch: 13.164491653442383\n",
      "Gradient Norm_Batch: 10.181645393371582\n",
      "7e-07\n",
      "Epoch [115/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8333243190227222e-07, Correlation: 0.9999999085698184, R2: 0.9999998166675766\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.063156127929688\n",
      "Gradient Norm_Of_Each_Mini_Batch: 24.390840530395508\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.222071647644043\n",
      "Gradient Norm_Batch: 9.155240058898926\n",
      "7e-07\n",
      "Epoch [116/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8327826012409787e-07, Correlation: 0.9999999085657836, R2: 0.9999998167217423\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.4871187210083\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.36161231994629\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.229690551757812\n",
      "Gradient Norm_Batch: 9.745129585266113\n",
      "7e-07\n",
      "Epoch [117/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8331780893277028e-07, Correlation: 0.9999999085647518, R2: 0.9999998166821877\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.65438175201416\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.2408504486084\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.757930755615234\n",
      "Gradient Norm_Batch: 10.441505432128906\n",
      "7e-07\n",
      "Epoch [118/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8334880280690413e-07, Correlation: 0.9999999085718692, R2: 0.9999998166512091\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.624338150024414\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.482698440551758\n",
      "Gradient Norm_Of_Each_Mini_Batch: 11.905426979064941\n",
      "Gradient Norm_Batch: 9.921789169311523\n",
      "7e-07\n",
      "Epoch [119/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8331853368636075e-07, Correlation: 0.9999999085717933, R2: 0.9999998166814454\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.286894798278809\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.000268936157227\n",
      "Gradient Norm_Of_Each_Mini_Batch: 8.479691505432129\n",
      "Gradient Norm_Batch: 10.311853408813477\n",
      "7e-07\n",
      "Epoch [120/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8333430773509463e-07, Correlation: 0.9999999085758701, R2: 0.9999998166656806\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.673233032226562\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.671406745910645\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.522357940673828\n",
      "Gradient Norm_Batch: 9.99109172821045\n",
      "7e-07\n",
      "Epoch [121/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833293339359443e-07, Correlation: 0.9999999085706133, R2: 0.9999998166706716\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.818218231201172\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.444391250610352\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.678241729736328\n",
      "Gradient Norm_Batch: 10.991586685180664\n",
      "7e-07\n",
      "Epoch [122/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8338012353069644e-07, Correlation: 0.9999999085716232, R2: 0.9999998166198806\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.330812454223633\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.285676956176758\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.030393600463867\n",
      "Gradient Norm_Batch: 11.134655952453613\n",
      "7e-07\n",
      "Epoch [123/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833950449281474e-07, Correlation: 0.999999908573923, R2: 0.9999998166049715\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.7801456451416\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.211450576782227\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.344982147216797\n",
      "Gradient Norm_Batch: 11.894556045532227\n",
      "7e-07\n",
      "Epoch [124/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8343875751725136e-07, Correlation: 0.9999999085739402, R2: 0.9999998165612385\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.299102783203125\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.10879898071289\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.792373657226562\n",
      "Gradient Norm_Batch: 11.983210563659668\n",
      "7e-07\n",
      "Epoch [125/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.83448506163586e-07, Correlation: 0.9999999085742279, R2: 0.9999998165514967\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.001033782958984\n",
      "Gradient Norm_Of_Each_Mini_Batch: 25.992053985595703\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.157928466796875\n",
      "Gradient Norm_Batch: 11.505500793457031\n",
      "7e-07\n",
      "Epoch [126/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8342612406740955e-07, Correlation: 0.999999908568368, R2: 0.9999998165738652\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.142597198486328\n",
      "Gradient Norm_Of_Each_Mini_Batch: 12.08201789855957\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.72649574279785\n",
      "Gradient Norm_Batch: 11.799056053161621\n",
      "7e-07\n",
      "Epoch [127/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8345116359341773e-07, Correlation: 0.9999999085641128, R2: 0.9999998165488617\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.877805709838867\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.278833389282227\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.562588691711426\n",
      "Gradient Norm_Batch: 11.8779296875\n",
      "7e-07\n",
      "Epoch [128/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8344320551477722e-07, Correlation: 0.9999999085679483, R2: 0.9999998165567924\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.292265892028809\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.8660945892334\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.09422492980957\n",
      "Gradient Norm_Batch: 10.99742317199707\n",
      "7e-07\n",
      "Epoch [129/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833852394383939e-07, Correlation: 0.9999999085708716, R2: 0.9999998166147581\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.32966423034668\n",
      "Gradient Norm_Of_Each_Mini_Batch: 25.720829010009766\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.109519958496094\n",
      "Gradient Norm_Batch: 10.529279708862305\n",
      "7e-07\n",
      "Epoch [130/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8337249230171437e-07, Correlation: 0.9999999085587603, R2: 0.9999998166275098\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.375272750854492\n",
      "Gradient Norm_Of_Each_Mini_Batch: 24.22817039489746\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.310033798217773\n",
      "Gradient Norm_Batch: 10.601319313049316\n",
      "7e-07\n",
      "Epoch [131/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8338629104164283e-07, Correlation: 0.9999999085598144, R2: 0.9999998166137041\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 24.448884963989258\n",
      "Gradient Norm_Of_Each_Mini_Batch: 24.33513832092285\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.700075149536133\n",
      "Gradient Norm_Batch: 10.748970031738281\n",
      "7e-07\n",
      "Epoch [132/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8341067686833412e-07, Correlation: 0.9999999085418891, R2: 0.9999998165893189\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.193204879760742\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.21314239501953\n",
      "Gradient Norm_Of_Each_Mini_Batch: 11.517927169799805\n",
      "Gradient Norm_Batch: 11.133013725280762\n",
      "7e-07\n",
      "Epoch [133/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8341923180287267e-07, Correlation: 0.9999999085619162, R2: 0.9999998165807709\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.631925582885742\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.902507781982422\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.677635192871094\n",
      "Gradient Norm_Batch: 11.035409927368164\n",
      "7e-07\n",
      "Epoch [134/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8340864471610985e-07, Correlation: 0.9999999085613318, R2: 0.9999998165913566\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.9345645904541\n",
      "Gradient Norm_Of_Each_Mini_Batch: 31.654388427734375\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.93305015563965\n",
      "Gradient Norm_Batch: 10.091086387634277\n",
      "7e-07\n",
      "Epoch [135/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8335320817186584e-07, Correlation: 0.9999999085498307, R2: 0.9999998166467957\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.996429443359375\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.901029586791992\n",
      "Gradient Norm_Of_Each_Mini_Batch: 13.709023475646973\n",
      "Gradient Norm_Batch: 9.719353675842285\n",
      "7e-07\n",
      "Epoch [136/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8333093976252712e-07, Correlation: 0.9999999085473406, R2: 0.9999998166690548\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.8411922454834\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.38034439086914\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.555671691894531\n",
      "Gradient Norm_Batch: 10.972118377685547\n",
      "7e-07\n",
      "Epoch [137/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8340041663122975e-07, Correlation: 0.9999999085642447, R2: 0.9999998165995764\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.100933074951172\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.348302841186523\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.598499298095703\n",
      "Gradient Norm_Batch: 12.156289100646973\n",
      "7e-07\n",
      "Epoch [138/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.8346733554608363e-07, Correlation: 0.9999999085682028, R2: 0.9999998165326655\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.150978088378906\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.655046463012695\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.400276184082031\n",
      "Gradient Norm_Batch: 11.14363956451416\n",
      "7e-07\n",
      "Epoch [139/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8340459462251602e-07, Correlation: 0.9999999085697878, R2: 0.9999998165954103\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.16927146911621\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.886539459228516\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.006813049316406\n",
      "Gradient Norm_Batch: 11.020240783691406\n",
      "7e-07\n",
      "Epoch [140/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8340148244533339e-07, Correlation: 0.9999999085654199, R2: 0.9999998165985144\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.566044807434082\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.33360481262207\n",
      "Gradient Norm_Of_Each_Mini_Batch: 12.883137702941895\n",
      "Gradient Norm_Batch: 10.582466125488281\n",
      "7e-07\n",
      "Epoch [141/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8337463814077637e-07, Correlation: 0.9999999085600529, R2: 0.9999998166253699\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.41307258605957\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.665250778198242\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.251873970031738\n",
      "Gradient Norm_Batch: 10.09421157836914\n",
      "7e-07\n",
      "Epoch [142/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833309113408177e-07, Correlation: 0.999999908569576, R2: 0.9999998166690833\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.187681198120117\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.582128524780273\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.731220245361328\n",
      "Gradient Norm_Batch: 10.22566032409668\n",
      "7e-07\n",
      "Epoch [143/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833312097687667e-07, Correlation: 0.9999999085729323, R2: 0.9999998166687883\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.674692153930664\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.47471046447754\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.32781410217285\n",
      "Gradient Norm_Batch: 10.466049194335938\n",
      "7e-07\n",
      "Epoch [144/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833509344351114e-07, Correlation: 0.9999999085710869, R2: 0.999999816649061\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.90615463256836\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.390586853027344\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.319286346435547\n",
      "Gradient Norm_Batch: 10.007464408874512\n",
      "7e-07\n",
      "Epoch [145/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8333827256356017e-07, Correlation: 0.9999999085602927, R2: 0.9999998166617217\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.362907409667969\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.831405639648438\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.464712142944336\n",
      "Gradient Norm_Batch: 10.232909202575684\n",
      "7e-07\n",
      "Epoch [146/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8334367268835194e-07, Correlation: 0.9999999085621837, R2: 0.9999998166563426\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.00281524658203\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.008590698242188\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.275423049926758\n",
      "Gradient Norm_Batch: 10.165329933166504\n",
      "7e-07\n",
      "Epoch [147/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833345635304795e-07, Correlation: 0.9999999085645356, R2: 0.9999998166654219\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.995018005371094\n",
      "Gradient Norm_Of_Each_Mini_Batch: 27.207836151123047\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.377357482910156\n",
      "Gradient Norm_Batch: 10.265426635742188\n",
      "7e-07\n",
      "Epoch [148/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8334320373014634e-07, Correlation: 0.9999999085693545, R2: 0.9999998166567822\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.437828063964844\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.652721405029297\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.139406204223633\n",
      "Gradient Norm_Batch: 10.36013412475586\n",
      "7e-07\n",
      "Epoch [149/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8334968387989647e-07, Correlation: 0.9999999085701552, R2: 0.9999998166503091\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 24.64908218383789\n",
      "Gradient Norm_Of_Each_Mini_Batch: 26.820425033569336\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.872974395751953\n",
      "Gradient Norm_Batch: 9.265277862548828\n",
      "7e-07\n",
      "Epoch [150/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8329588158394472e-07, Correlation: 0.999999908556539, R2: 0.9999998167041171\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 24.75421905517578\n",
      "Gradient Norm_Of_Each_Mini_Batch: 25.157814025878906\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.208023071289062\n",
      "Gradient Norm_Batch: 10.814486503601074\n",
      "7e-07\n",
      "Epoch [151/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8337814822189102e-07, Correlation: 0.99999990856477, R2: 0.9999998166218321\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.103717803955078\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.32639503479004\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.335479736328125\n",
      "Gradient Norm_Batch: 10.508273124694824\n",
      "7e-07\n",
      "Epoch [152/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8336294260734576e-07, Correlation: 0.9999999085670153, R2: 0.9999998166370645\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.932011604309082\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.77116870880127\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.88396644592285\n",
      "Gradient Norm_Batch: 9.9389009475708\n",
      "7e-07\n",
      "Epoch [153/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833353167057794e-07, Correlation: 0.9999999085607655, R2: 0.9999998166646923\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 25.956899642944336\n",
      "Gradient Norm_Of_Each_Mini_Batch: 27.504817962646484\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.41441535949707\n",
      "Gradient Norm_Batch: 12.097955703735352\n",
      "7e-07\n",
      "Epoch [154/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.8346614183428755e-07, Correlation: 0.9999999085635165, R2: 0.9999998165338493\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.193361282348633\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.415054321289062\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.62679386138916\n",
      "Gradient Norm_Batch: 10.556303977966309\n",
      "7e-07\n",
      "Epoch [155/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8337910034915694e-07, Correlation: 0.9999999085545506, R2: 0.9999998166208801\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.160919189453125\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.175277709960938\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.6802921295166\n",
      "Gradient Norm_Batch: 10.560145378112793\n",
      "7e-07\n",
      "Epoch [156/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8337449603222922e-07, Correlation: 0.9999999085653465, R2: 0.9999998166254975\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.54606819152832\n",
      "Gradient Norm_Of_Each_Mini_Batch: 27.06443214416504\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.952518463134766\n",
      "Gradient Norm_Batch: 11.088638305664062\n",
      "7e-07\n",
      "Epoch [157/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8339694918267924e-07, Correlation: 0.9999999085625731, R2: 0.9999998166030614\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.196229934692383\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.138328552246094\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.768308639526367\n",
      "Gradient Norm_Batch: 10.895045280456543\n",
      "7e-07\n",
      "Epoch [158/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8339059693062154e-07, Correlation: 0.9999999085667862, R2: 0.9999998166093969\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.398452758789062\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.594221115112305\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.106159210205078\n",
      "Gradient Norm_Batch: 11.70712947845459\n",
      "7e-07\n",
      "Epoch [159/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8343713747981383e-07, Correlation: 0.9999999085724046, R2: 0.9999998165628745\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.209802627563477\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.18325424194336\n",
      "Gradient Norm_Of_Each_Mini_Batch: 13.313933372497559\n",
      "Gradient Norm_Batch: 10.239996910095215\n",
      "7e-07\n",
      "Epoch [160/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8334863227664755e-07, Correlation: 0.9999999085682827, R2: 0.9999998166513752\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.149423599243164\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.44046401977539\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.187820434570312\n",
      "Gradient Norm_Batch: 8.990192413330078\n",
      "7e-07\n",
      "Epoch [161/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8328313444726518e-07, Correlation: 0.9999999085581843, R2: 0.9999998167168717\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.448163986206055\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.177108764648438\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.69786262512207\n",
      "Gradient Norm_Batch: 10.192422866821289\n",
      "7e-07\n",
      "Epoch [162/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833614788893101e-07, Correlation: 0.9999999085560881, R2: 0.9999998166385079\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.503990173339844\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.938169479370117\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.82621192932129\n",
      "Gradient Norm_Batch: 9.860812187194824\n",
      "7e-07\n",
      "Epoch [163/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8334142737330694e-07, Correlation: 0.9999999085539123, R2: 0.9999998166585564\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.88937759399414\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.412229537963867\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.735344886779785\n",
      "Gradient Norm_Batch: 9.316455841064453\n",
      "7e-07\n",
      "Epoch [164/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8330173645608738e-07, Correlation: 0.999999908559875, R2: 0.9999998166982559\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.80452537536621\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.783977508544922\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.653905868530273\n",
      "Gradient Norm_Batch: 9.866219520568848\n",
      "7e-07\n",
      "Epoch [165/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833301581655178e-07, Correlation: 0.9999999085581781, R2: 0.9999998166698406\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.238605499267578\n",
      "Gradient Norm_Of_Each_Mini_Batch: 25.677860260009766\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.73711395263672\n",
      "Gradient Norm_Batch: 10.259997367858887\n",
      "7e-07\n",
      "Epoch [166/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8334809226416837e-07, Correlation: 0.9999999085666412, R2: 0.9999998166519186\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.557727813720703\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.060091018676758\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.82764720916748\n",
      "Gradient Norm_Batch: 11.386284828186035\n",
      "7e-07\n",
      "Epoch [167/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8340817575790425e-07, Correlation: 0.9999999085751051, R2: 0.9999998165918161\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.361055374145508\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.707571029663086\n",
      "Gradient Norm_Of_Each_Mini_Batch: 24.884790420532227\n",
      "Gradient Norm_Batch: 12.95506763458252\n",
      "7e-07\n",
      "Epoch [168/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.8348876551499416e-07, Correlation: 0.9999999085813793, R2: 0.9999998165112302\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 29.1043758392334\n",
      "Gradient Norm_Of_Each_Mini_Batch: 27.557119369506836\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.092336654663086\n",
      "Gradient Norm_Batch: 12.661920547485352\n",
      "7e-07\n",
      "Epoch [169/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.834668381661686e-07, Correlation: 0.9999999085754202, R2: 0.9999998165331759\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.92024040222168\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.120094299316406\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.275585174560547\n",
      "Gradient Norm_Batch: 10.642273902893066\n",
      "7e-07\n",
      "Epoch [170/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8335545348691085e-07, Correlation: 0.9999999085681344, R2: 0.9999998166445564\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.73464584350586\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.77094078063965\n",
      "Gradient Norm_Of_Each_Mini_Batch: 12.981281280517578\n",
      "Gradient Norm_Batch: 11.040921211242676\n",
      "7e-07\n",
      "Epoch [171/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.83388777941218e-07, Correlation: 0.9999999085670166, R2: 0.999999816611209\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 26.058334350585938\n",
      "Gradient Norm_Of_Each_Mini_Batch: 25.371644973754883\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.729665756225586\n",
      "Gradient Norm_Batch: 11.276139259338379\n",
      "7e-07\n",
      "Epoch [172/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8339521545840398e-07, Correlation: 0.9999999085605139, R2: 0.9999998166048008\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.172773361206055\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.10199737548828\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.94448471069336\n",
      "Gradient Norm_Batch: 10.346324920654297\n",
      "7e-07\n",
      "Epoch [173/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8334731066715904e-07, Correlation: 0.9999999085700113, R2: 0.9999998166526788\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.864051818847656\n",
      "Gradient Norm_Of_Each_Mini_Batch: 24.295392990112305\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.751874923706055\n",
      "Gradient Norm_Batch: 10.231236457824707\n",
      "7e-07\n",
      "Epoch [174/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.83337931503047e-07, Correlation: 0.9999999085692644, R2: 0.9999998166620698\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.70867919921875\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.420652389526367\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.487377166748047\n",
      "Gradient Norm_Batch: 10.019560813903809\n",
      "7e-07\n",
      "Epoch [175/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8332609386106924e-07, Correlation: 0.9999999085686615, R2: 0.9999998166738958\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 13.365923881530762\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.499608993530273\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.79961585998535\n",
      "Gradient Norm_Batch: 10.58205795288086\n",
      "7e-07\n",
      "Epoch [176/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8335620666221075e-07, Correlation: 0.9999999085721604, R2: 0.9999998166437863\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 27.41318130493164\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.386634826660156\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.655353546142578\n",
      "Gradient Norm_Batch: 11.841708183288574\n",
      "7e-07\n",
      "Epoch [177/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.834293641422846e-07, Correlation: 0.9999999085729303, R2: 0.9999998165706339\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.535629272460938\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.382079124450684\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.996319770812988\n",
      "Gradient Norm_Batch: 10.677404403686523\n",
      "7e-07\n",
      "Epoch [178/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8335991569529142e-07, Correlation: 0.9999999085740149, R2: 0.9999998166400809\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.332630157470703\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.765897750854492\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.809141159057617\n",
      "Gradient Norm_Batch: 11.657073020935059\n",
      "7e-07\n",
      "Epoch [179/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8341957286338584e-07, Correlation: 0.9999999085768164, R2: 0.9999998165804117\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.885866165161133\n",
      "Gradient Norm_Of_Each_Mini_Batch: 24.74367904663086\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.627700805664062\n",
      "Gradient Norm_Batch: 11.114070892333984\n",
      "7e-07\n",
      "Epoch [180/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8338246832172445e-07, Correlation: 0.9999999085692481, R2: 0.9999998166175359\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 27.758440017700195\n",
      "Gradient Norm_Of_Each_Mini_Batch: 26.323244094848633\n",
      "Gradient Norm_Of_Each_Mini_Batch: 13.840208053588867\n",
      "Gradient Norm_Batch: 12.192620277404785\n",
      "7e-07\n",
      "Epoch [181/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.834469287587126e-07, Correlation: 0.9999999085719392, R2: 0.9999998165530651\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.667266845703125\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.500368118286133\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.926769256591797\n",
      "Gradient Norm_Batch: 11.082765579223633\n",
      "7e-07\n",
      "Epoch [182/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8338907636916701e-07, Correlation: 0.9999999085682632, R2: 0.9999998166109101\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.9105281829834\n",
      "Gradient Norm_Of_Each_Mini_Batch: 29.294734954833984\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.265420913696289\n",
      "Gradient Norm_Batch: 9.45433235168457\n",
      "7e-07\n",
      "Epoch [183/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8330531759147561e-07, Correlation: 0.9999999085636458, R2: 0.9999998166946816\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.048870086669922\n",
      "Gradient Norm_Of_Each_Mini_Batch: 27.112138748168945\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.969407081604004\n",
      "Gradient Norm_Batch: 8.892596244812012\n",
      "7e-07\n",
      "Epoch [184/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.832670619705823e-07, Correlation: 0.9999999085630429, R2: 0.9999998167329484\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.119184494018555\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.500099182128906\n",
      "Gradient Norm_Of_Each_Mini_Batch: 24.26206398010254\n",
      "Gradient Norm_Batch: 9.916786193847656\n",
      "7e-07\n",
      "Epoch [185/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8332309537072433e-07, Correlation: 0.999999908567279, R2: 0.9999998166769123\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.918855667114258\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.58245849609375\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.985689163208008\n",
      "Gradient Norm_Batch: 10.628142356872559\n",
      "7e-07\n",
      "Epoch [186/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833608251899932e-07, Correlation: 0.9999999085695536, R2: 0.9999998166391667\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.213163375854492\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.777098655700684\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.541450500488281\n",
      "Gradient Norm_Batch: 10.623984336853027\n",
      "7e-07\n",
      "Epoch [187/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8336733376145276e-07, Correlation: 0.999999908567764, R2: 0.999999816632659\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.98474884033203\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.041519165039062\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.234527587890625\n",
      "Gradient Norm_Batch: 10.812577247619629\n",
      "7e-07\n",
      "Epoch [188/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833784040172759e-07, Correlation: 0.9999999085668781, R2: 0.9999998166215804\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.203989028930664\n",
      "Gradient Norm_Of_Each_Mini_Batch: 26.63581085205078\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.785499572753906\n",
      "Gradient Norm_Batch: 10.379868507385254\n",
      "7e-07\n",
      "Epoch [189/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833462448530554e-07, Correlation: 0.9999999085686458, R2: 0.9999998166537482\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.147552490234375\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.663763046264648\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.927667617797852\n",
      "Gradient Norm_Batch: 9.98840045928955\n",
      "7e-07\n",
      "Epoch [190/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8333207663090434e-07, Correlation: 0.9999999085643757, R2: 0.999999816667923\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.426101684570312\n",
      "Gradient Norm_Of_Each_Mini_Batch: 26.59117317199707\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.639612197875977\n",
      "Gradient Norm_Batch: 10.51545238494873\n",
      "7e-07\n",
      "Epoch [191/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8336285734221747e-07, Correlation: 0.9999999085599507, R2: 0.9999998166371333\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.618389129638672\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.317357063293457\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.567607879638672\n",
      "Gradient Norm_Batch: 11.060503959655762\n",
      "7e-07\n",
      "Epoch [192/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8339862606353563e-07, Correlation: 0.9999999085644132, R2: 0.9999998166013637\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.280939102172852\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.153755187988281\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.929283142089844\n",
      "Gradient Norm_Batch: 11.907746315002441\n",
      "7e-07\n",
      "Epoch [193/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8343219210237294e-07, Correlation: 0.9999999085675912, R2: 0.9999998165677942\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.722637176513672\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.23324966430664\n",
      "Gradient Norm_Of_Each_Mini_Batch: 13.16958999633789\n",
      "Gradient Norm_Batch: 10.579343795776367\n",
      "7e-07\n",
      "Epoch [194/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8336086782255734e-07, Correlation: 0.999999908571111, R2: 0.9999998166391204\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.88658618927002\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.045675277709961\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.034652709960938\n",
      "Gradient Norm_Batch: 11.027945518493652\n",
      "7e-07\n",
      "Epoch [195/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8339837026815076e-07, Correlation: 0.9999999085652218, R2: 0.9999998166016344\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.365459442138672\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.97473907470703\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.79259490966797\n",
      "Gradient Norm_Batch: 10.316481590270996\n",
      "7e-07\n",
      "Epoch [196/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8335524032409012e-07, Correlation: 0.9999999085628812, R2: 0.9999998166447648\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 12.356216430664062\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.717896461486816\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.094945907592773\n",
      "Gradient Norm_Batch: 10.856550216674805\n",
      "7e-07\n",
      "Epoch [197/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833909379911347e-07, Correlation: 0.9999999085624836, R2: 0.9999998166090603\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.144515991210938\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.17488670349121\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.052106857299805\n",
      "Gradient Norm_Batch: 10.724279403686523\n",
      "7e-07\n",
      "Epoch [198/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833826246411263e-07, Correlation: 0.9999999085670404, R2: 0.9999998166173741\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.772275924682617\n",
      "Gradient Norm_Of_Each_Mini_Batch: 28.086381912231445\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.589427947998047\n",
      "Gradient Norm_Batch: 9.901886940002441\n",
      "7e-07\n",
      "Epoch [199/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8332865181491798e-07, Correlation: 0.9999999085651023, R2: 0.9999998166713473\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.317986488342285\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.271774291992188\n",
      "Gradient Norm_Of_Each_Mini_Batch: 11.058287620544434\n",
      "Gradient Norm_Batch: 9.739408493041992\n",
      "7e-07\n",
      "Epoch [200/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8332579543312022e-07, Correlation: 0.9999999085619385, R2: 0.9999998166742031\n",
      "Final gradient of the subproblem Core : 9.739408493041992\n",
      "(32, 32)\n",
      "---------------------------Testing with Best Lambda------------------------------\n",
      "Y Test Predicted:  [ 67.54933717  64.18959258 100.90631507 106.7944236   66.5288956\n",
      "  96.79612548 112.76400822  87.0813133   58.8850715   60.74048532\n",
      "  65.75347618  64.15719629  82.80162451  68.56404511 142.45883124\n",
      "  98.01996603  98.35296587  55.4162726   96.74724342  87.26453132\n",
      "  47.97817974  91.13721234  73.56508886  77.46465038  46.96426171\n",
      "  79.38008468  84.7105329   45.49462235  87.56687099  73.47975574\n",
      "  86.90817516  88.2672015  124.65321669  81.42694488  87.5474471\n",
      "  82.79813663  98.73500028  69.03771436  80.63835     93.08345306\n",
      "  68.40990435  87.28010705  62.04195575  57.34038097  56.0211007\n",
      "  89.30942331 110.54333293 134.611001    43.26800791  74.48156251\n",
      "  96.23311621  87.8912414   79.23392007  90.72726892 125.88522158\n",
      " 160.37025052  60.51193818  86.88830811  89.57418897  85.49696059\n",
      "  79.97211214  95.54239744  57.09921199  50.36939453  70.86125772\n",
      " 131.23408494  94.51161409  83.16030467  62.92653911  84.10776778\n",
      "  76.25923587  96.48133289  77.08424955  65.79257741  71.77929852\n",
      "  65.14980002  61.02637587  68.75450558  66.41029515 104.95707199\n",
      "  79.74046736  91.80694777 110.44688769  62.24793335  57.61570499\n",
      "  98.12933194  67.83591191  72.12854404 127.8506857   63.30875622\n",
      " 103.6974318  150.07988922  67.41979619  92.38859375 123.67899249\n",
      "  46.91115151  64.66066007 108.22044518 101.13983839 113.37860949]\n",
      "Y Test Actual:  [ 67.54932471  64.19367989 100.89907103 106.80033802  66.5296898\n",
      "  96.80085769 112.75716496  87.08379176  58.86645104  60.75871896\n",
      "  65.76651181  64.14603363  82.80477245  68.55912818 142.45923938\n",
      "  98.00053671  98.36489423  55.40988723  96.73410604  87.25275894\n",
      "  47.97392502  91.14302569  73.57729565  77.46144622  46.97758628\n",
      "  79.38435102  84.7040345   45.4879342   87.55672081  73.49886432\n",
      "  86.90598491  88.25175908 124.63253198  81.43265772  87.55287141\n",
      "  82.8080788   98.72768177  69.0468243   80.62856336  93.08545506\n",
      "  68.40807412  87.26847367  62.04717769  57.33990441  56.01334774\n",
      "  89.31934273 110.55219975 134.61307846  43.26935217  74.47617341\n",
      "  96.21843559  87.89937403  79.21862917  90.71786386 125.8824493\n",
      " 160.38439066  60.50387741  86.87508783  89.5739495   85.50880057\n",
      "  79.97185552  95.54965657  57.09355445  50.35928763  70.86185745\n",
      " 131.24078613  94.50837386  83.15863626  62.93422433  84.09831139\n",
      "  76.26182003  96.48126457  77.07300387  65.77838739  71.76861932\n",
      "  65.14937109  61.0440168   68.74400314  66.41967439 104.95340089\n",
      "  79.75508697  91.80406117 110.44770514  62.24491669  57.60974417\n",
      "  98.13365195  67.82489044  72.14320497 127.85744229  63.30804812\n",
      " 103.70015697 150.07489827  67.42743244  92.3906977  123.66379363\n",
      "  46.92967844  64.66629945 108.22935093 101.14612067 113.38031778]\n",
      "Error Report on Testing _ With best Lambda\n",
      "SGD Learning Rate: 7e-07\n",
      "SGD_Alpha chosen for model:  2.5\n",
      "SGD_Test Normalized Estimation Error:  8.559647048137412e-09\n",
      "SGD_Test NMSE Loss:  1.129317056425374e-08\n",
      "SGD_Test R2 Loss:  0.9999998434271141\n",
      "SGD_Test Correlation:  0.9999999219085625\n",
      "Objective Function Values 1110.019836413514\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final gradient: 9.739408493041992\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------------------Learning Rate 6e-07 -----------------------------------------\n",
      "Objective Function Value: 3408906.126530059\n",
      "--------------------------------------------------------------BCD iteration 0 --------------------------------------------------------------\n",
      "---------------------------------------------Sep 0 Factor 0 -------------------------------------------------\n",
      "Iteration: 0, Separation Rank: 0, Factor Matrix: 0, Objective Function Value: 4178689.296386482\n",
      "---------------------------------------------Sep 0 Factor 1 -------------------------------------------------\n",
      "Iteration: 0, Separation Rank: 0, Factor Matrix: 1, Objective Function Value: 3488977.6201287964\n",
      "---------------------------------------------Sep 1 Factor 0 -------------------------------------------------\n",
      "Iteration: 0, Separation Rank: 1, Factor Matrix: 0, Objective Function Value: 3092821.460810512\n",
      "---------------------------------------------Sep 1 Factor 1 -------------------------------------------------\n",
      "Iteration: 0, Separation Rank: 1, Factor Matrix: 1, Objective Function Value: 4026588.625912\n",
      "---------------------------------------------Core-------------------------------------------------\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 302472.21875\n",
      "Gradient Norm_Of_Each_Mini_Batch: 251469.625\n",
      "Gradient Norm_Of_Each_Mini_Batch: 116342.6953125\n",
      "Gradient Norm_Batch: 475634.15625\n",
      "6e-07\n",
      "Epoch [1/200], Loss: 184341.7656, Gap to Optimality: 184341.7656, NMSE: 0.5344038009643555, Correlation: 0.7090582851661488, R2: 0.46559617846971235\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 204599.859375\n",
      "Gradient Norm_Of_Each_Mini_Batch: 151676.359375\n",
      "Gradient Norm_Of_Each_Mini_Batch: 67143.0078125\n",
      "Gradient Norm_Batch: 308913.84375\n",
      "6e-07\n",
      "Epoch [2/200], Loss: 86451.8203, Gap to Optimality: 86451.8203, NMSE: 0.25062012672424316, Correlation: 0.8941849459834618, R2: 0.749379882646421\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 124200.1640625\n",
      "Gradient Norm_Of_Each_Mini_Batch: 101916.9296875\n",
      "Gradient Norm_Of_Each_Mini_Batch: 52016.703125\n",
      "Gradient Norm_Batch: 208604.78125\n",
      "6e-07\n",
      "Epoch [3/200], Loss: 44149.2930, Gap to Optimality: 44149.2930, NMSE: 0.12798424065113068, Correlation: 0.9483559680470568, R2: 0.8720157661303846\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 91529.7890625\n",
      "Gradient Norm_Of_Each_Mini_Batch: 66608.53125\n",
      "Gradient Norm_Of_Each_Mini_Batch: 31795.57421875\n",
      "Gradient Norm_Batch: 146579.71875\n",
      "6e-07\n",
      "Epoch [4/200], Loss: 24298.7188, Gap to Optimality: 24298.7188, NMSE: 0.07043672353029251, Correlation: 0.9709266058387741, R2: 0.9295632818954144\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 54776.375\n",
      "Gradient Norm_Of_Each_Mini_Batch: 58023.296875\n",
      "Gradient Norm_Of_Each_Mini_Batch: 26466.419921875\n",
      "Gradient Norm_Batch: 106707.25\n",
      "6e-07\n",
      "Epoch [5/200], Loss: 14208.7354, Gap to Optimality: 14208.7354, NMSE: 0.04118531942367554, Correlation: 0.982300326702706, R2: 0.9588146828143068\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 38719.44140625\n",
      "Gradient Norm_Of_Each_Mini_Batch: 43096.375\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18277.09765625\n",
      "Gradient Norm_Batch: 80326.9453125\n",
      "6e-07\n",
      "Epoch [6/200], Loss: 8733.1543, Gap to Optimality: 8733.1543, NMSE: 0.025311226025223732, Correlation: 0.9887169699680307, R2: 0.9746887742618181\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 32270.97265625\n",
      "Gradient Norm_Of_Each_Mini_Batch: 29263.79296875\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14359.8046875\n",
      "Gradient Norm_Batch: 62180.2109375\n",
      "6e-07\n",
      "Epoch [7/200], Loss: 5570.1821, Gap to Optimality: 5570.1821, NMSE: 0.01614150032401085, Correlation: 0.9926083731594618, R2: 0.9838584993485028\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 25043.62109375\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18203.38671875\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16348.216796875\n",
      "Gradient Norm_Batch: 49132.5625\n",
      "6e-07\n",
      "Epoch [8/200], Loss: 3647.8237, Gap to Optimality: 3647.8237, NMSE: 0.010568390600383282, Correlation: 0.9950651968821109, R2: 0.9894316093747989\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19332.859375\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17084.240234375\n",
      "Gradient Norm_Of_Each_Mini_Batch: 11174.404296875\n",
      "Gradient Norm_Batch: 39373.62109375\n",
      "6e-07\n",
      "Epoch [9/200], Loss: 2432.0071, Gap to Optimality: 2432.0071, NMSE: 0.007043597288429737, Correlation: 0.9966636453563742, R2: 0.992956402621494\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 13794.8046875\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14641.7412109375\n",
      "Gradient Norm_Of_Each_Mini_Batch: 9914.802734375\n",
      "Gradient Norm_Batch: 31889.171875\n",
      "6e-07\n",
      "Epoch [10/200], Loss: 1643.9525, Gap to Optimality: 1643.9525, NMSE: 0.004758920986205339, Correlation: 0.9977199338665088, R2: 0.995241078760259\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 11819.3603515625\n",
      "Gradient Norm_Of_Each_Mini_Batch: 11604.4775390625\n",
      "Gradient Norm_Of_Each_Mini_Batch: 7299.5654296875\n",
      "Gradient Norm_Batch: 26059.36328125\n",
      "6e-07\n",
      "Epoch [11/200], Loss: 1123.8807, Gap to Optimality: 1123.8807, NMSE: 0.0032511556055396795, Correlation: 0.9984304412641739, R2: 0.9967488446605393\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 11187.248046875\n",
      "Gradient Norm_Of_Each_Mini_Batch: 10005.8857421875\n",
      "Gradient Norm_Of_Each_Mini_Batch: 3743.73828125\n",
      "Gradient Norm_Batch: 21423.869140625\n",
      "6e-07\n",
      "Epoch [12/200], Loss: 774.2806, Gap to Optimality: 774.2806, NMSE: 0.002237606793642044, Correlation: 0.9989140877845669, R2: 0.9977623930143282\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 8490.1962890625\n",
      "Gradient Norm_Of_Each_Mini_Batch: 8081.79345703125\n",
      "Gradient Norm_Of_Each_Mini_Batch: 4342.19091796875\n",
      "Gradient Norm_Batch: 17695.09765625\n",
      "6e-07\n",
      "Epoch [13/200], Loss: 537.2786, Gap to Optimality: 537.2786, NMSE: 0.0015504930634051561, Correlation: 0.9992443669518272, R2: 0.9984495070911137\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 7282.00048828125\n",
      "Gradient Norm_Of_Each_Mini_Batch: 6885.03515625\n",
      "Gradient Norm_Of_Each_Mini_Batch: 3173.47802734375\n",
      "Gradient Norm_Batch: 14670.7119140625\n",
      "6e-07\n",
      "Epoch [14/200], Loss: 375.0048, Gap to Optimality: 375.0048, NMSE: 0.0010800260351970792, Correlation: 0.9994716943553631, R2: 0.9989199739817289\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 5767.6533203125\n",
      "Gradient Norm_Of_Each_Mini_Batch: 5667.16015625\n",
      "Gradient Norm_Of_Each_Mini_Batch: 3035.186279296875\n",
      "Gradient Norm_Batch: 12200.119140625\n",
      "6e-07\n",
      "Epoch [15/200], Loss: 263.2188, Gap to Optimality: 263.2188, NMSE: 0.0007559311343356967, Correlation: 0.9996291881534539, R2: 0.999244068883929\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 5172.19384765625\n",
      "Gradient Norm_Of_Each_Mini_Batch: 4570.99267578125\n",
      "Gradient Norm_Of_Each_Mini_Batch: 2356.53515625\n",
      "Gradient Norm_Batch: 10174.2373046875\n",
      "6e-07\n",
      "Epoch [16/200], Loss: 185.7260, Gap to Optimality: 185.7260, NMSE: 0.000531258643604815, Correlation: 0.9997390236721296, R2: 0.9994687413663086\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 4164.86865234375\n",
      "Gradient Norm_Of_Each_Mini_Batch: 3918.20849609375\n",
      "Gradient Norm_Of_Each_Mini_Batch: 1873.1494140625\n",
      "Gradient Norm_Batch: 8503.4697265625\n",
      "6e-07\n",
      "Epoch [17/200], Loss: 131.7607, Gap to Optimality: 131.7607, NMSE: 0.00037479616003111005, Correlation: 0.9998154669613648, R2: 0.9996252038584116\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 3221.732666015625\n",
      "Gradient Norm_Of_Each_Mini_Batch: 3548.8193359375\n",
      "Gradient Norm_Of_Each_Mini_Batch: 1849.5318603515625\n",
      "Gradient Norm_Batch: 7120.3955078125\n",
      "6e-07\n",
      "Epoch [18/200], Loss: 93.9642, Gap to Optimality: 93.9642, NMSE: 0.00026521109975874424, Correlation: 0.9998693235937949, R2: 0.9997347889070584\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 2652.74755859375\n",
      "Gradient Norm_Of_Each_Mini_Batch: 2778.994140625\n",
      "Gradient Norm_Of_Each_Mini_Batch: 1581.32958984375\n",
      "Gradient Norm_Batch: 5974.71484375\n",
      "6e-07\n",
      "Epoch [19/200], Loss: 67.4647, Gap to Optimality: 67.4647, NMSE: 0.00018837802053894848, Correlation: 0.9999070331533386, R2: 0.9998116219821758\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 2391.96533203125\n",
      "Gradient Norm_Of_Each_Mini_Batch: 2219.587646484375\n",
      "Gradient Norm_Of_Each_Mini_Batch: 1318.1529541015625\n",
      "Gradient Norm_Batch: 5022.6787109375\n",
      "6e-07\n",
      "Epoch [20/200], Loss: 48.7835, Gap to Optimality: 48.7835, NMSE: 0.0001342127361567691, Correlation: 0.9999336963885196, R2: 0.9998657872629487\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 1916.00390625\n",
      "Gradient Norm_Of_Each_Mini_Batch: 2069.02490234375\n",
      "Gradient Norm_Of_Each_Mini_Batch: 943.5343017578125\n",
      "Gradient Norm_Batch: 4228.18212890625\n",
      "6e-07\n",
      "Epoch [21/200], Loss: 35.5611, Gap to Optimality: 35.5611, NMSE: 9.587378008291125e-05, Correlation: 0.9999525697926246, R2: 0.9999041262167774\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 1692.5433349609375\n",
      "Gradient Norm_Of_Each_Mini_Batch: 1617.18408203125\n",
      "Gradient Norm_Of_Each_Mini_Batch: 864.8670654296875\n",
      "Gradient Norm_Batch: 3564.727294921875\n",
      "6e-07\n",
      "Epoch [22/200], Loss: 26.1824, Gap to Optimality: 26.1824, NMSE: 6.867954652989283e-05, Correlation: 0.9999659914600644, R2: 0.9999313204539847\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 1498.3907470703125\n",
      "Gradient Norm_Of_Each_Mini_Batch: 1247.1759033203125\n",
      "Gradient Norm_Of_Each_Mini_Batch: 805.8412475585938\n",
      "Gradient Norm_Batch: 3009.64990234375\n",
      "6e-07\n",
      "Epoch [23/200], Loss: 19.5104, Gap to Optimality: 19.5104, NMSE: 4.9332546041114256e-05, Correlation: 0.9999755400928872, R2: 0.9999506674579098\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 1233.04931640625\n",
      "Gradient Norm_Of_Each_Mini_Batch: 1156.1944580078125\n",
      "Gradient Norm_Of_Each_Mini_Batch: 569.7271728515625\n",
      "Gradient Norm_Batch: 2544.198974609375\n",
      "6e-07\n",
      "Epoch [24/200], Loss: 14.7495, Gap to Optimality: 14.7495, NMSE: 3.5527031286619604e-05, Correlation: 0.9999823716320675, R2: 0.9999644729671242\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 959.0700073242188\n",
      "Gradient Norm_Of_Each_Mini_Batch: 966.3447265625\n",
      "Gradient Norm_Of_Each_Mini_Batch: 597.764404296875\n",
      "Gradient Norm_Batch: 2153.375732421875\n",
      "6e-07\n",
      "Epoch [25/200], Loss: 11.3450, Gap to Optimality: 11.3450, NMSE: 2.565470276749693e-05, Correlation: 0.9999872634273769, R2: 0.999974345299025\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 863.8905639648438\n",
      "Gradient Norm_Of_Each_Mini_Batch: 789.59521484375\n",
      "Gradient Norm_Of_Each_Mini_Batch: 464.659912109375\n",
      "Gradient Norm_Batch: 1824.9471435546875\n",
      "6e-07\n",
      "Epoch [26/200], Loss: 8.9051, Gap to Optimality: 8.9051, NMSE: 1.8578964954940602e-05, Correlation: 0.9999907717602595, R2: 0.9999814210362384\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 753.6968994140625\n",
      "Gradient Norm_Of_Each_Mini_Batch: 739.3116455078125\n",
      "Gradient Norm_Of_Each_Mini_Batch: 363.0079345703125\n",
      "Gradient Norm_Batch: 1547.52734375\n",
      "6e-07\n",
      "Epoch [27/200], Loss: 7.1472, Gap to Optimality: 7.1472, NMSE: 1.3481018868333194e-05, Correlation: 0.9999933010964543, R2: 0.9999865189804602\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 661.103271484375\n",
      "Gradient Norm_Of_Each_Mini_Batch: 586.9306640625\n",
      "Gradient Norm_Of_Each_Mini_Batch: 291.2257080078125\n",
      "Gradient Norm_Batch: 1313.990478515625\n",
      "6e-07\n",
      "Epoch [28/200], Loss: 5.8841, Gap to Optimality: 5.8841, NMSE: 9.817616046348121e-06, Correlation: 0.9999951178326794, R2: 0.9999901823844114\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 573.8552856445312\n",
      "Gradient Norm_Of_Each_Mini_Batch: 444.974365234375\n",
      "Gradient Norm_Of_Each_Mini_Batch: 278.3428955078125\n",
      "Gradient Norm_Batch: 1116.86962890625\n",
      "6e-07\n",
      "Epoch [29/200], Loss: 4.9734, Gap to Optimality: 4.9734, NMSE: 7.176427516242256e-06, Correlation: 0.9999964309765123, R2: 0.9999928235719725\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 467.9350280761719\n",
      "Gradient Norm_Of_Each_Mini_Batch: 455.0415954589844\n",
      "Gradient Norm_Of_Each_Mini_Batch: 189.40316772460938\n",
      "Gradient Norm_Batch: 949.9711303710938\n",
      "6e-07\n",
      "Epoch [30/200], Loss: 4.3142, Gap to Optimality: 4.3142, NMSE: 5.264270839688834e-06, Correlation: 0.9999973810266739, R2: 0.9999947357292814\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 333.5083923339844\n",
      "Gradient Norm_Of_Each_Mini_Batch: 403.3861389160156\n",
      "Gradient Norm_Of_Each_Mini_Batch: 209.9081573486328\n",
      "Gradient Norm_Batch: 808.7802734375\n",
      "6e-07\n",
      "Epoch [31/200], Loss: 3.8373, Gap to Optimality: 3.8373, NMSE: 3.880917120113736e-06, Correlation: 0.999998069216172, R2: 0.9999961190829769\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 317.56634521484375\n",
      "Gradient Norm_Of_Each_Mini_Batch: 333.98968505859375\n",
      "Gradient Norm_Of_Each_Mini_Batch: 157.32357788085938\n",
      "Gradient Norm_Batch: 689.1255493164062\n",
      "6e-07\n",
      "Epoch [32/200], Loss: 3.4915, Gap to Optimality: 3.4915, NMSE: 2.8779722924809903e-06, Correlation: 0.9999985678165996, R2: 0.9999971220279515\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 295.330810546875\n",
      "Gradient Norm_Of_Each_Mini_Batch: 264.283203125\n",
      "Gradient Norm_Of_Each_Mini_Batch: 142.88363647460938\n",
      "Gradient Norm_Batch: 587.4974975585938\n",
      "6e-07\n",
      "Epoch [33/200], Loss: 3.2401, Gap to Optimality: 3.2401, NMSE: 2.148566409232444e-06, Correlation: 0.9999989307136684, R2: 0.9999978514335937\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 241.28871154785156\n",
      "Gradient Norm_Of_Each_Mini_Batch: 216.9301300048828\n",
      "Gradient Norm_Of_Each_Mini_Batch: 132.43295288085938\n",
      "Gradient Norm_Batch: 501.3433837890625\n",
      "6e-07\n",
      "Epoch [34/200], Loss: 3.0576, Gap to Optimality: 3.0576, NMSE: 1.6189368352570455e-06, Correlation: 0.9999991944726062, R2: 0.9999983810631748\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 207.65609741210938\n",
      "Gradient Norm_Of_Each_Mini_Batch: 194.3612518310547\n",
      "Gradient Norm_Of_Each_Mini_Batch: 102.67752075195312\n",
      "Gradient Norm_Batch: 428.09881591796875\n",
      "6e-07\n",
      "Epoch [35/200], Loss: 2.9246, Gap to Optimality: 2.9246, NMSE: 1.233149760082597e-06, Correlation: 0.9999993863483616, R2: 0.9999987668502789\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 183.35682678222656\n",
      "Gradient Norm_Of_Each_Mini_Batch: 153.08929443359375\n",
      "Gradient Norm_Of_Each_Mini_Batch: 85.16050720214844\n",
      "Gradient Norm_Batch: 365.7826232910156\n",
      "6e-07\n",
      "Epoch [36/200], Loss: 2.8277, Gap to Optimality: 2.8277, NMSE: 9.518146839582187e-07, Correlation: 0.9999995262637181, R2: 0.9999990481852445\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 157.8396759033203\n",
      "Gradient Norm_Of_Each_Mini_Batch: 119.57733917236328\n",
      "Gradient Norm_Of_Each_Mini_Batch: 92.1469497680664\n",
      "Gradient Norm_Batch: 312.7878112792969\n",
      "6e-07\n",
      "Epoch [37/200], Loss: 2.7569, Gap to Optimality: 2.7569, NMSE: 7.465073963430768e-07, Correlation: 0.9999996285668017, R2: 0.9999992534925499\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 132.7454071044922\n",
      "Gradient Norm_Of_Each_Mini_Batch: 113.87783813476562\n",
      "Gradient Norm_Of_Each_Mini_Batch: 70.90410614013672\n",
      "Gradient Norm_Batch: 267.5411071777344\n",
      "6e-07\n",
      "Epoch [38/200], Loss: 2.7052, Gap to Optimality: 2.7052, NMSE: 5.96324639445811e-07, Correlation: 0.99999970322914, R2: 0.9999994036753609\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 111.06919860839844\n",
      "Gradient Norm_Of_Each_Mini_Batch: 99.49047088623047\n",
      "Gradient Norm_Of_Each_Mini_Batch: 62.871334075927734\n",
      "Gradient Norm_Batch: 229.01626586914062\n",
      "6e-07\n",
      "Epoch [39/200], Loss: 2.6674, Gap to Optimality: 2.6674, NMSE: 4.865669893661106e-07, Correlation: 0.9999997577920047, R2: 0.9999995134329981\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 93.07255554199219\n",
      "Gradient Norm_Of_Each_Mini_Batch: 108.58187103271484\n",
      "Gradient Norm_Of_Each_Mini_Batch: 51.89900588989258\n",
      "Gradient Norm_Batch: 195.92138671875\n",
      "6e-07\n",
      "Epoch [40/200], Loss: 2.6395, Gap to Optimality: 2.6395, NMSE: 4.057249327615864e-07, Correlation: 0.9999997979059777, R2: 0.9999995942750717\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 72.46803283691406\n",
      "Gradient Norm_Of_Each_Mini_Batch: 88.00690460205078\n",
      "Gradient Norm_Of_Each_Mini_Batch: 42.705204010009766\n",
      "Gradient Norm_Batch: 167.9378204345703\n",
      "6e-07\n",
      "Epoch [41/200], Loss: 2.6193, Gap to Optimality: 2.6193, NMSE: 3.4683495186982327e-07, Correlation: 0.9999998273083542, R2: 0.9999996531650502\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 77.72081756591797\n",
      "Gradient Norm_Of_Each_Mini_Batch: 62.25969314575195\n",
      "Gradient Norm_Of_Each_Mini_Batch: 37.565547943115234\n",
      "Gradient Norm_Batch: 143.96466064453125\n",
      "6e-07\n",
      "Epoch [42/200], Loss: 2.6044, Gap to Optimality: 2.6044, NMSE: 3.0360624236891454e-07, Correlation: 0.9999998487870906, R2: 0.9999996963937583\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 67.70314025878906\n",
      "Gradient Norm_Of_Each_Mini_Batch: 58.41628646850586\n",
      "Gradient Norm_Of_Each_Mini_Batch: 28.9470272064209\n",
      "Gradient Norm_Batch: 123.60077667236328\n",
      "6e-07\n",
      "Epoch [43/200], Loss: 2.5935, Gap to Optimality: 2.5935, NMSE: 2.7196583118893614e-07, Correlation: 0.9999998645556242, R2: 0.9999997280341651\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 52.36943054199219\n",
      "Gradient Norm_Of_Each_Mini_Batch: 51.86742401123047\n",
      "Gradient Norm_Of_Each_Mini_Batch: 36.04978942871094\n",
      "Gradient Norm_Batch: 106.09925079345703\n",
      "6e-07\n",
      "Epoch [44/200], Loss: 2.5854, Gap to Optimality: 2.5854, NMSE: 2.4860534608706075e-07, Correlation: 0.9999998761780713, R2: 0.9999997513946614\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 49.3977165222168\n",
      "Gradient Norm_Of_Each_Mini_Batch: 57.05007553100586\n",
      "Gradient Norm_Of_Each_Mini_Batch: 39.559452056884766\n",
      "Gradient Norm_Batch: 90.83116912841797\n",
      "6e-07\n",
      "Epoch [45/200], Loss: 2.5795, Gap to Optimality: 2.5795, NMSE: 2.3121570791317936e-07, Correlation: 0.9999998847488221, R2: 0.9999997687842852\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 39.23143005371094\n",
      "Gradient Norm_Of_Each_Mini_Batch: 42.40909957885742\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.20073699951172\n",
      "Gradient Norm_Batch: 78.17149353027344\n",
      "6e-07\n",
      "Epoch [46/200], Loss: 2.5752, Gap to Optimality: 2.5752, NMSE: 2.1868839894523262e-07, Correlation: 0.9999998910052466, R2: 0.9999997813115863\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 33.65414810180664\n",
      "Gradient Norm_Of_Each_Mini_Batch: 34.648799896240234\n",
      "Gradient Norm_Of_Each_Mini_Batch: 29.48369789123535\n",
      "Gradient Norm_Batch: 67.37255096435547\n",
      "6e-07\n",
      "Epoch [47/200], Loss: 2.5720, Gap to Optimality: 2.5720, NMSE: 2.0944743539530464e-07, Correlation: 0.999999895630518, R2: 0.9999997905525607\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 36.269615173339844\n",
      "Gradient Norm_Of_Each_Mini_Batch: 30.569293975830078\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.439922332763672\n",
      "Gradient Norm_Batch: 57.94357681274414\n",
      "6e-07\n",
      "Epoch [48/200], Loss: 2.5696, Gap to Optimality: 2.5696, NMSE: 2.0258806898709736e-07, Correlation: 0.9999998989967706, R2: 0.9999997974119513\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 31.625885009765625\n",
      "Gradient Norm_Of_Each_Mini_Batch: 33.54348373413086\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.846200942993164\n",
      "Gradient Norm_Batch: 50.014495849609375\n",
      "6e-07\n",
      "Epoch [49/200], Loss: 2.5679, Gap to Optimality: 2.5679, NMSE: 1.975664360998053e-07, Correlation: 0.9999999014824449, R2: 0.999999802433559\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.387210845947266\n",
      "Gradient Norm_Of_Each_Mini_Batch: 37.18392562866211\n",
      "Gradient Norm_Of_Each_Mini_Batch: 25.430465698242188\n",
      "Gradient Norm_Batch: 43.25422286987305\n",
      "6e-07\n",
      "Epoch [50/200], Loss: 2.5666, Gap to Optimality: 2.5666, NMSE: 1.9384231109143002e-07, Correlation: 0.9999999033517222, R2: 0.9999998061576919\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 30.941076278686523\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.295204162597656\n",
      "Gradient Norm_Of_Each_Mini_Batch: 12.653359413146973\n",
      "Gradient Norm_Batch: 37.72846984863281\n",
      "6e-07\n",
      "Epoch [51/200], Loss: 2.5657, Gap to Optimality: 2.5657, NMSE: 1.9116335181479371e-07, Correlation: 0.999999904701992, R2: 0.9999998088366432\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 27.293344497680664\n",
      "Gradient Norm_Of_Each_Mini_Batch: 25.547630310058594\n",
      "Gradient Norm_Of_Each_Mini_Batch: 26.98817253112793\n",
      "Gradient Norm_Batch: 33.11490249633789\n",
      "6e-07\n",
      "Epoch [52/200], Loss: 2.5650, Gap to Optimality: 2.5650, NMSE: 1.8917580746347085e-07, Correlation: 0.9999999057212775, R2: 0.9999998108241748\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.585424423217773\n",
      "Gradient Norm_Of_Each_Mini_Batch: 24.534059524536133\n",
      "Gradient Norm_Of_Each_Mini_Batch: 13.64051628112793\n",
      "Gradient Norm_Batch: 29.161489486694336\n",
      "6e-07\n",
      "Epoch [53/200], Loss: 2.5645, Gap to Optimality: 2.5645, NMSE: 1.8769216580949433e-07, Correlation: 0.9999999064652022, R2: 0.9999998123078202\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 25.358640670776367\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.085002899169922\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.7692928314209\n",
      "Gradient Norm_Batch: 25.689102172851562\n",
      "6e-07\n",
      "Epoch [54/200], Loss: 2.5641, Gap to Optimality: 2.5641, NMSE: 1.8656770350844454e-07, Correlation: 0.9999999070174033, R2: 0.9999998134322968\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.267536163330078\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.811660766601562\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.492633819580078\n",
      "Gradient Norm_Batch: 23.316335678100586\n",
      "6e-07\n",
      "Epoch [55/200], Loss: 2.5639, Gap to Optimality: 2.5639, NMSE: 1.8581835092845722e-07, Correlation: 0.9999999074135854, R2: 0.9999998141816633\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.438796997070312\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.206844329833984\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.083772659301758\n",
      "Gradient Norm_Batch: 20.697996139526367\n",
      "6e-07\n",
      "Epoch [56/200], Loss: 2.5636, Gap to Optimality: 2.5636, NMSE: 1.8517347655233607e-07, Correlation: 0.9999999077115247, R2: 0.9999998148265219\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.603805541992188\n",
      "Gradient Norm_Of_Each_Mini_Batch: 28.366670608520508\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.692035675048828\n",
      "Gradient Norm_Batch: 17.751419067382812\n",
      "6e-07\n",
      "Epoch [57/200], Loss: 2.5635, Gap to Optimality: 2.5635, NMSE: 1.8463573780991283e-07, Correlation: 0.9999999079212122, R2: 0.9999998153642703\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.929668426513672\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.46870231628418\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.42262840270996\n",
      "Gradient Norm_Batch: 15.463203430175781\n",
      "6e-07\n",
      "Epoch [58/200], Loss: 2.5633, Gap to Optimality: 2.5633, NMSE: 1.8423943970447e-07, Correlation: 0.9999999080869418, R2: 0.9999998157605516\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.867725372314453\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.772526741027832\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.685481071472168\n",
      "Gradient Norm_Batch: 14.19461441040039\n",
      "6e-07\n",
      "Epoch [59/200], Loss: 2.5633, Gap to Optimality: 2.5633, NMSE: 1.8400010048935656e-07, Correlation: 0.9999999082094462, R2: 0.9999998159999007\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.773052215576172\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.57071876525879\n",
      "Gradient Norm_Of_Each_Mini_Batch: 24.2927188873291\n",
      "Gradient Norm_Batch: 14.042426109313965\n",
      "6e-07\n",
      "Epoch [60/200], Loss: 2.5632, Gap to Optimality: 2.5632, NMSE: 1.8386728584118828e-07, Correlation: 0.999999908309081, R2: 0.9999998161327288\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.486249923706055\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.835334777832031\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.38568878173828\n",
      "Gradient Norm_Batch: 13.878547668457031\n",
      "6e-07\n",
      "Epoch [61/200], Loss: 2.5632, Gap to Optimality: 2.5632, NMSE: 1.8378395338913833e-07, Correlation: 0.9999999083706805, R2: 0.9999998162160302\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.2982177734375\n",
      "Gradient Norm_Of_Each_Mini_Batch: 25.1177978515625\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.98981475830078\n",
      "Gradient Norm_Batch: 11.604011535644531\n",
      "6e-07\n",
      "Epoch [62/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.8358927889039478e-07, Correlation: 0.9999999084154538, R2: 0.9999998164107129\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.12005615234375\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.983617782592773\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.763713836669922\n",
      "Gradient Norm_Batch: 12.199169158935547\n",
      "6e-07\n",
      "Epoch [63/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.8358240083671262e-07, Correlation: 0.9999999084622023, R2: 0.9999998164176147\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.55682373046875\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.632610321044922\n",
      "Gradient Norm_Of_Each_Mini_Batch: 13.26551628112793\n",
      "Gradient Norm_Batch: 11.692119598388672\n",
      "6e-07\n",
      "Epoch [64/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.8351859409904137e-07, Correlation: 0.9999999084883645, R2: 0.9999998164814052\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.178316116333008\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.455352783203125\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.635372161865234\n",
      "Gradient Norm_Batch: 12.046370506286621\n",
      "6e-07\n",
      "Epoch [65/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.8351785513459618e-07, Correlation: 0.999999908512214, R2: 0.999999816482131\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.648341178894043\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.093334197998047\n",
      "Gradient Norm_Of_Each_Mini_Batch: 13.252913475036621\n",
      "Gradient Norm_Batch: 11.775184631347656\n",
      "6e-07\n",
      "Epoch [66/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.8348303854054393e-07, Correlation: 0.9999999085288043, R2: 0.9999998165169653\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.30547523498535\n",
      "Gradient Norm_Of_Each_Mini_Batch: 25.475955963134766\n",
      "Gradient Norm_Of_Each_Mini_Batch: 11.557110786437988\n",
      "Gradient Norm_Batch: 12.298250198364258\n",
      "6e-07\n",
      "Epoch [67/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.8350024788560404e-07, Correlation: 0.9999999085392094, R2: 0.9999998164997496\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.563777923583984\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.68410873413086\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.31366539001465\n",
      "Gradient Norm_Batch: 10.77015209197998\n",
      "6e-07\n",
      "Epoch [68/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8339564178404544e-07, Correlation: 0.9999999085455152, R2: 0.9999998166043608\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.041690826416016\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.11440658569336\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.03768253326416\n",
      "Gradient Norm_Batch: 11.300946235656738\n",
      "6e-07\n",
      "Epoch [69/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8342014129757445e-07, Correlation: 0.9999999085556205, R2: 0.9999998165798697\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.31595230102539\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.271947860717773\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.396791458129883\n",
      "Gradient Norm_Batch: 12.067363739013672\n",
      "6e-07\n",
      "Epoch [70/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.8346592867146683e-07, Correlation: 0.9999999085573792, R2: 0.999999816534055\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.404260635375977\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.198867797851562\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.457473754882812\n",
      "Gradient Norm_Batch: 12.394089698791504\n",
      "6e-07\n",
      "Epoch [71/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.8347573416122032e-07, Correlation: 0.9999999085595775, R2: 0.9999998165242525\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 12.151108741760254\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.37729263305664\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.679601669311523\n",
      "Gradient Norm_Batch: 11.520085334777832\n",
      "6e-07\n",
      "Epoch [72/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8342583985031524e-07, Correlation: 0.9999999085630089, R2: 0.9999998165741539\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 28.089282989501953\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.81229019165039\n",
      "Gradient Norm_Of_Each_Mini_Batch: 24.862369537353516\n",
      "Gradient Norm_Batch: 11.961167335510254\n",
      "6e-07\n",
      "Epoch [73/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.834551568435927e-07, Correlation: 0.9999999085645165, R2: 0.9999998165448524\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 12.621962547302246\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.746936798095703\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.59286117553711\n",
      "Gradient Norm_Batch: 11.21759033203125\n",
      "6e-07\n",
      "Epoch [74/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8339991925131471e-07, Correlation: 0.9999999085689312, R2: 0.9999998166000842\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.853954315185547\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.96934700012207\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.270214080810547\n",
      "Gradient Norm_Batch: 11.885358810424805\n",
      "6e-07\n",
      "Epoch [75/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.834434613101621e-07, Correlation: 0.9999999085712354, R2: 0.9999998165565244\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.919108390808105\n",
      "Gradient Norm_Of_Each_Mini_Batch: 29.218809127807617\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.803810119628906\n",
      "Gradient Norm_Batch: 10.639065742492676\n",
      "6e-07\n",
      "Epoch [76/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8336626794734912e-07, Correlation: 0.9999999085699216, R2: 0.9999998166337248\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.35077667236328\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.41914176940918\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.523256301879883\n",
      "Gradient Norm_Batch: 11.489541053771973\n",
      "6e-07\n",
      "Epoch [77/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8341967233936884e-07, Correlation: 0.9999999085681311, R2: 0.9999998165803416\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.4645938873291\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.419225692749023\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.942752838134766\n",
      "Gradient Norm_Batch: 11.109161376953125\n",
      "6e-07\n",
      "Epoch [78/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.834030740610615e-07, Correlation: 0.9999999085664987, R2: 0.9999998165969256\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.417560577392578\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.582050323486328\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.11587905883789\n",
      "Gradient Norm_Batch: 10.581786155700684\n",
      "6e-07\n",
      "Epoch [79/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8336255891426845e-07, Correlation: 0.9999999085664357, R2: 0.9999998166374523\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.05404281616211\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.440763473510742\n",
      "Gradient Norm_Of_Each_Mini_Batch: 13.754220962524414\n",
      "Gradient Norm_Batch: 9.315587043762207\n",
      "6e-07\n",
      "Epoch [80/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8329667739180877e-07, Correlation: 0.9999999085618408, R2: 0.9999998167033196\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.716529846191406\n",
      "Gradient Norm_Of_Each_Mini_Batch: 26.650514602661133\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.47043228149414\n",
      "Gradient Norm_Batch: 10.164966583251953\n",
      "6e-07\n",
      "Epoch [81/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8334142737330694e-07, Correlation: 0.9999999085641619, R2: 0.9999998166585643\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.15315055847168\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.51168441772461\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.00332260131836\n",
      "Gradient Norm_Batch: 11.428262710571289\n",
      "6e-07\n",
      "Epoch [82/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8341228269491694e-07, Correlation: 0.9999999085705253, R2: 0.9999998165877327\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.528635025024414\n",
      "Gradient Norm_Of_Each_Mini_Batch: 26.0115966796875\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.827463150024414\n",
      "Gradient Norm_Batch: 10.780029296875\n",
      "6e-07\n",
      "Epoch [83/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8337179596983333e-07, Correlation: 0.9999999085730904, R2: 0.9999998166282016\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.05922508239746\n",
      "Gradient Norm_Of_Each_Mini_Batch: 13.092249870300293\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.901691436767578\n",
      "Gradient Norm_Batch: 10.883428573608398\n",
      "6e-07\n",
      "Epoch [84/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833847562693336e-07, Correlation: 0.9999999085679924, R2: 0.9999998166152269\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 12.791875839233398\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.256208419799805\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.283374786376953\n",
      "Gradient Norm_Batch: 10.46484661102295\n",
      "6e-07\n",
      "Epoch [85/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833628004987986e-07, Correlation: 0.9999999085674728, R2: 0.999999816637196\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 26.304410934448242\n",
      "Gradient Norm_Of_Each_Mini_Batch: 36.193023681640625\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.609041213989258\n",
      "Gradient Norm_Batch: 11.080052375793457\n",
      "6e-07\n",
      "Epoch [86/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8339437701797578e-07, Correlation: 0.9999999085676388, R2: 0.9999998166056303\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 25.869609832763672\n",
      "Gradient Norm_Of_Each_Mini_Batch: 24.50135040283203\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.487871170043945\n",
      "Gradient Norm_Batch: 12.379073143005371\n",
      "6e-07\n",
      "Epoch [87/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.8346128172197496e-07, Correlation: 0.9999999085724331, R2: 0.9999998165387064\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 13.599124908447266\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.705533981323242\n",
      "Gradient Norm_Of_Each_Mini_Batch: 11.480875968933105\n",
      "Gradient Norm_Batch: 11.219947814941406\n",
      "6e-07\n",
      "Epoch [88/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8339443386139465e-07, Correlation: 0.9999999085724808, R2: 0.9999998166055656\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.619372367858887\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.51009750366211\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.724416732788086\n",
      "Gradient Norm_Batch: 10.176946640014648\n",
      "6e-07\n",
      "Epoch [89/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8334182527723897e-07, Correlation: 0.9999999085677714, R2: 0.9999998166581846\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.30923080444336\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.68773651123047\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.066543579101562\n",
      "Gradient Norm_Batch: 10.050235748291016\n",
      "6e-07\n",
      "Epoch [90/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8333386719859845e-07, Correlation: 0.9999999085672163, R2: 0.9999998166661118\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.47920036315918\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.021514892578125\n",
      "Gradient Norm_Of_Each_Mini_Batch: 13.749306678771973\n",
      "Gradient Norm_Batch: 10.429808616638184\n",
      "6e-07\n",
      "Epoch [91/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8336135099161766e-07, Correlation: 0.9999999085664859, R2: 0.9999998166386552\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.6342716217041\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.849023818969727\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.112869262695312\n",
      "Gradient Norm_Batch: 9.542875289916992\n",
      "6e-07\n",
      "Epoch [92/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8331499518353667e-07, Correlation: 0.999999908557311, R2: 0.999999816685005\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.077123641967773\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.161462783813477\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.962121963500977\n",
      "Gradient Norm_Batch: 10.05521011352539\n",
      "6e-07\n",
      "Epoch [93/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8333450668706064e-07, Correlation: 0.9999999085680714, R2: 0.9999998166655055\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.263090133666992\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.46927261352539\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.9979829788208\n",
      "Gradient Norm_Batch: 10.340835571289062\n",
      "6e-07\n",
      "Epoch [94/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8334947071707575e-07, Correlation: 0.9999999085690201, R2: 0.9999998166505435\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.01177978515625\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.4493408203125\n",
      "Gradient Norm_Of_Each_Mini_Batch: 13.691893577575684\n",
      "Gradient Norm_Batch: 10.161208152770996\n",
      "6e-07\n",
      "Epoch [95/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833340093071456e-07, Correlation: 0.9999999085722702, R2: 0.9999998166660059\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.21352195739746\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.099010467529297\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.600189208984375\n",
      "Gradient Norm_Batch: 10.349700927734375\n",
      "6e-07\n",
      "Epoch [96/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8334426954424998e-07, Correlation: 0.9999999085713247, R2: 0.9999998166557361\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.284393310546875\n",
      "Gradient Norm_Of_Each_Mini_Batch: 24.418882369995117\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.131925582885742\n",
      "Gradient Norm_Batch: 10.200394630432129\n",
      "6e-07\n",
      "Epoch [97/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833412852647598e-07, Correlation: 0.9999999085689323, R2: 0.9999998166587266\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.224863052368164\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.97406005859375\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.90796661376953\n",
      "Gradient Norm_Batch: 9.999190330505371\n",
      "6e-07\n",
      "Epoch [98/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8333153661842516e-07, Correlation: 0.9999999085648473, R2: 0.9999998166684696\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.621219635009766\n",
      "Gradient Norm_Of_Each_Mini_Batch: 24.582550048828125\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.307825088500977\n",
      "Gradient Norm_Batch: 10.078499794006348\n",
      "6e-07\n",
      "Epoch [99/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8332056583858503e-07, Correlation: 0.9999999085733284, R2: 0.9999998166794465\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.551362037658691\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.816219329833984\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.441408157348633\n",
      "Gradient Norm_Batch: 10.50300407409668\n",
      "6e-07\n",
      "Epoch [100/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8335771301281056e-07, Correlation: 0.9999999085703568, R2: 0.9999998166422788\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.93128204345703\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.831052780151367\n",
      "Gradient Norm_Of_Each_Mini_Batch: 13.737772941589355\n",
      "Gradient Norm_Batch: 10.060328483581543\n",
      "6e-07\n",
      "Epoch [101/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833407452522806e-07, Correlation: 0.9999999085668461, R2: 0.9999998166592542\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.649402618408203\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.984302520751953\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.643516540527344\n",
      "Gradient Norm_Batch: 9.702858924865723\n",
      "6e-07\n",
      "Epoch [102/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8331728313114581e-07, Correlation: 0.9999999085647631, R2: 0.9999998166827043\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.178050994873047\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.882776260375977\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.60566520690918\n",
      "Gradient Norm_Batch: 10.368985176086426\n",
      "6e-07\n",
      "Epoch [103/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8336118046136107e-07, Correlation: 0.9999999085652902, R2: 0.9999998166388243\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 12.846207618713379\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.367745399475098\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.642732620239258\n",
      "Gradient Norm_Batch: 10.088061332702637\n",
      "6e-07\n",
      "Epoch [104/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8334020523980143e-07, Correlation: 0.999999908567276, R2: 0.9999998166597991\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.683879852294922\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.872291564941406\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.651066780090332\n",
      "Gradient Norm_Batch: 11.209403991699219\n",
      "6e-07\n",
      "Epoch [105/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833988818589205e-07, Correlation: 0.9999999085744887, R2: 0.9999998166011248\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.761104583740234\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.439128875732422\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.915813446044922\n",
      "Gradient Norm_Batch: 11.632932662963867\n",
      "6e-07\n",
      "Epoch [106/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8342119290082337e-07, Correlation: 0.9999999085757286, R2: 0.9999998165788044\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 24.300384521484375\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.534074783325195\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.402423858642578\n",
      "Gradient Norm_Batch: 9.482625961303711\n",
      "6e-07\n",
      "Epoch [107/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8330378281916637e-07, Correlation: 0.9999999085658444, R2: 0.9999998166962127\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.602943420410156\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.66021728515625\n",
      "Gradient Norm_Of_Each_Mini_Batch: 13.298569679260254\n",
      "Gradient Norm_Batch: 8.872635841369629\n",
      "6e-07\n",
      "Epoch [108/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8326980466554232e-07, Correlation: 0.9999999085629466, R2: 0.9999998167301801\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.043258666992188\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.49241065979004\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.797609329223633\n",
      "Gradient Norm_Batch: 9.71607780456543\n",
      "6e-07\n",
      "Epoch [109/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8330963769130904e-07, Correlation: 0.9999999085697707, R2: 0.9999998166903575\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.72418212890625\n",
      "Gradient Norm_Of_Each_Mini_Batch: 24.901697158813477\n",
      "Gradient Norm_Of_Each_Mini_Batch: 12.959528923034668\n",
      "Gradient Norm_Batch: 10.55256175994873\n",
      "6e-07\n",
      "Epoch [110/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833566329878522e-07, Correlation: 0.999999908573551, R2: 0.9999998166433777\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.509832382202148\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.099843978881836\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.556398391723633\n",
      "Gradient Norm_Batch: 10.404438018798828\n",
      "6e-07\n",
      "Epoch [111/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833561640296466e-07, Correlation: 0.9999999085697729, R2: 0.9999998166438321\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.779260635375977\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.230073928833008\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.272607803344727\n",
      "Gradient Norm_Batch: 10.149903297424316\n",
      "6e-07\n",
      "Epoch [112/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833394236427921e-07, Correlation: 0.9999999085697353, R2: 0.9999998166605856\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.8594913482666\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.920618057250977\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.967737197875977\n",
      "Gradient Norm_Batch: 9.437108993530273\n",
      "6e-07\n",
      "Epoch [113/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.832965068615522e-07, Correlation: 0.9999999085680109, R2: 0.9999998167035072\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.65235710144043\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.924943923950195\n",
      "Gradient Norm_Of_Each_Mini_Batch: 13.682876586914062\n",
      "Gradient Norm_Batch: 8.862578392028809\n",
      "6e-07\n",
      "Epoch [114/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.832704299431498e-07, Correlation: 0.9999999085648581, R2: 0.9999998167295624\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.000118255615234\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.119993209838867\n",
      "Gradient Norm_Of_Each_Mini_Batch: 13.836256980895996\n",
      "Gradient Norm_Batch: 9.733661651611328\n",
      "6e-07\n",
      "Epoch [115/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833038112408758e-07, Correlation: 0.9999999085712077, R2: 0.9999998166962025\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.099868774414062\n",
      "Gradient Norm_Of_Each_Mini_Batch: 25.799407958984375\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.600082397460938\n",
      "Gradient Norm_Batch: 10.315398216247559\n",
      "6e-07\n",
      "Epoch [116/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833419531749314e-07, Correlation: 0.9999999085728342, R2: 0.9999998166580589\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.799360275268555\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.319351196289062\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.73141098022461\n",
      "Gradient Norm_Batch: 10.95016860961914\n",
      "6e-07\n",
      "Epoch [117/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8338454310651287e-07, Correlation: 0.9999999085729451, R2: 0.9999998166154419\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.533548355102539\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.45528221130371\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.14580726623535\n",
      "Gradient Norm_Batch: 10.897908210754395\n",
      "6e-07\n",
      "Epoch [118/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8337638607590634e-07, Correlation: 0.9999999085754224, R2: 0.9999998166236267\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 12.94310188293457\n",
      "Gradient Norm_Of_Each_Mini_Batch: 13.448161125183105\n",
      "Gradient Norm_Of_Each_Mini_Batch: 12.934953689575195\n",
      "Gradient Norm_Batch: 10.156106948852539\n",
      "6e-07\n",
      "Epoch [119/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8333085449739883e-07, Correlation: 0.9999999085735984, R2: 0.999999816669147\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.499713897705078\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.855487823486328\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.678483963012695\n",
      "Gradient Norm_Batch: 9.942720413208008\n",
      "6e-07\n",
      "Epoch [120/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8332114848362835e-07, Correlation: 0.9999999085710466, R2: 0.9999998166788454\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.404808044433594\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.792104721069336\n",
      "Gradient Norm_Of_Each_Mini_Batch: 13.843393325805664\n",
      "Gradient Norm_Batch: 9.843530654907227\n",
      "6e-07\n",
      "Epoch [121/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8331417095396318e-07, Correlation: 0.9999999085707023, R2: 0.9999998166858232\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.204452514648438\n",
      "Gradient Norm_Of_Each_Mini_Batch: 26.53327751159668\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.839712142944336\n",
      "Gradient Norm_Batch: 9.721539497375488\n",
      "6e-07\n",
      "Epoch [122/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8331336093524442e-07, Correlation: 0.9999999085650372, R2: 0.9999998166866405\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.556656837463379\n",
      "Gradient Norm_Of_Each_Mini_Batch: 13.608470916748047\n",
      "Gradient Norm_Of_Each_Mini_Batch: 13.42522144317627\n",
      "Gradient Norm_Batch: 10.060938835144043\n",
      "6e-07\n",
      "Epoch [123/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8333373930090602e-07, Correlation: 0.9999999085650239, R2: 0.9999998166662584\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.595056533813477\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.553627014160156\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.511545181274414\n",
      "Gradient Norm_Batch: 10.699206352233887\n",
      "6e-07\n",
      "Epoch [124/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8337534868351213e-07, Correlation: 0.9999999085664915, R2: 0.999999816624645\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.170560836791992\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.60314178466797\n",
      "Gradient Norm_Of_Each_Mini_Batch: 12.509183883666992\n",
      "Gradient Norm_Batch: 10.872272491455078\n",
      "6e-07\n",
      "Epoch [125/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8338424467856385e-07, Correlation: 0.999999908570239, R2: 0.9999998166157655\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.483200073242188\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.239274024963379\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.723098754882812\n",
      "Gradient Norm_Batch: 10.043771743774414\n",
      "6e-07\n",
      "Epoch [126/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8334826279442495e-07, Correlation: 0.9999999085576899, R2: 0.999999816651734\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.6250057220459\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.763139724731445\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.089941024780273\n",
      "Gradient Norm_Batch: 10.143228530883789\n",
      "6e-07\n",
      "Epoch [127/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8333481932586437e-07, Correlation: 0.9999999085682071, R2: 0.9999998166651771\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.60025978088379\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.252095222473145\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.29722023010254\n",
      "Gradient Norm_Batch: 9.767682075500488\n",
      "6e-07\n",
      "Epoch [128/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833087566183167e-07, Correlation: 0.9999999085660728, R2: 0.9999998166912554\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 12.655755996704102\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.589415550231934\n",
      "Gradient Norm_Of_Each_Mini_Batch: 13.038424491882324\n",
      "Gradient Norm_Batch: 9.706356048583984\n",
      "6e-07\n",
      "Epoch [129/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8330730711113574e-07, Correlation: 0.9999999085666915, R2: 0.9999998166926806\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.614057540893555\n",
      "Gradient Norm_Of_Each_Mini_Batch: 24.01428985595703\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.91290855407715\n",
      "Gradient Norm_Batch: 10.355718612670898\n",
      "6e-07\n",
      "Epoch [130/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833545724139185e-07, Correlation: 0.9999999085618392, R2: 0.9999998166454351\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.99709129333496\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.872817993164062\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.58864688873291\n",
      "Gradient Norm_Batch: 10.357772827148438\n",
      "6e-07\n",
      "Epoch [131/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8335565243887686e-07, Correlation: 0.99999990856252, R2: 0.9999998166443603\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.641077041625977\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.25080108642578\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.437747955322266\n",
      "Gradient Norm_Batch: 10.069724082946777\n",
      "6e-07\n",
      "Epoch [132/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8333734885800368e-07, Correlation: 0.9999999085655547, R2: 0.9999998166626389\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.047101974487305\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.007793426513672\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.660895347595215\n",
      "Gradient Norm_Batch: 10.573739051818848\n",
      "6e-07\n",
      "Epoch [133/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8336612583880196e-07, Correlation: 0.9999999085627552, R2: 0.99999981663388\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.25613021850586\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.493823051452637\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.96832847595215\n",
      "Gradient Norm_Batch: 11.353941917419434\n",
      "6e-07\n",
      "Epoch [134/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8341651752962207e-07, Correlation: 0.9999999085682845, R2: 0.9999998165834992\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 24.701086044311523\n",
      "Gradient Norm_Of_Each_Mini_Batch: 24.40207862854004\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.303218841552734\n",
      "Gradient Norm_Batch: 12.25733757019043\n",
      "6e-07\n",
      "Epoch [135/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.834531815347873e-07, Correlation: 0.999999908578171, R2: 0.999999816546826\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.55731773376465\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.61366844177246\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.95335578918457\n",
      "Gradient Norm_Batch: 11.229869842529297\n",
      "6e-07\n",
      "Epoch [136/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8339987661875057e-07, Correlation: 0.9999999085720926, R2: 0.9999998166001179\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.079252243041992\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.369077682495117\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.826875686645508\n",
      "Gradient Norm_Batch: 11.136453628540039\n",
      "6e-07\n",
      "Epoch [137/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8339372331865889e-07, Correlation: 0.9999999085716608, R2: 0.9999998166062778\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.642459869384766\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.57801055908203\n",
      "Gradient Norm_Of_Each_Mini_Batch: 12.552824974060059\n",
      "Gradient Norm_Batch: 11.201986312866211\n",
      "6e-07\n",
      "Epoch [138/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833951301932757e-07, Correlation: 0.9999999085739174, R2: 0.9999998166048754\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.807870864868164\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.50775146484375\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.355079650878906\n",
      "Gradient Norm_Batch: 11.430459976196289\n",
      "6e-07\n",
      "Epoch [139/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8341661700560508e-07, Correlation: 0.9999999085720516, R2: 0.9999998165833774\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.653560638427734\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.620532989501953\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.13666820526123\n",
      "Gradient Norm_Batch: 12.262619972229004\n",
      "6e-07\n",
      "Epoch [140/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.8346658237078373e-07, Correlation: 0.9999999085744324, R2: 0.9999998165334215\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.320079803466797\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.44784927368164\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.07206153869629\n",
      "Gradient Norm_Batch: 11.069573402404785\n",
      "6e-07\n",
      "Epoch [141/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.834043104054217e-07, Correlation: 0.9999999085665976, R2: 0.9999998165956889\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.18368148803711\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.989255905151367\n",
      "Gradient Norm_Of_Each_Mini_Batch: 10.231681823730469\n",
      "Gradient Norm_Batch: 10.74705696105957\n",
      "6e-07\n",
      "Epoch [142/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8338002405471343e-07, Correlation: 0.9999999085680498, R2: 0.9999998166199597\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.111286163330078\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.279630661010742\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.722953796386719\n",
      "Gradient Norm_Batch: 10.388046264648438\n",
      "6e-07\n",
      "Epoch [143/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8335462925733736e-07, Correlation: 0.9999999085684615, R2: 0.9999998166453634\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.66117286682129\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.30215072631836\n",
      "Gradient Norm_Of_Each_Mini_Batch: 13.425509452819824\n",
      "Gradient Norm_Batch: 10.161219596862793\n",
      "6e-07\n",
      "Epoch [144/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8334635853989312e-07, Correlation: 0.9999999085644783, R2: 0.9999998166536402\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.796892166137695\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.245006561279297\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.108224868774414\n",
      "Gradient Norm_Batch: 9.621779441833496\n",
      "6e-07\n",
      "Epoch [145/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833132614592614e-07, Correlation: 0.9999999085626406, R2: 0.9999998166867325\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.110294342041016\n",
      "Gradient Norm_Of_Each_Mini_Batch: 24.00337028503418\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.868769645690918\n",
      "Gradient Norm_Batch: 9.668047904968262\n",
      "6e-07\n",
      "Epoch [146/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833072644785716e-07, Correlation: 0.9999999085688456, R2: 0.9999998166927508\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.213510513305664\n",
      "Gradient Norm_Of_Each_Mini_Batch: 11.742192268371582\n",
      "Gradient Norm_Of_Each_Mini_Batch: 13.567492485046387\n",
      "Gradient Norm_Batch: 9.613106727600098\n",
      "6e-07\n",
      "Epoch [147/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8330786133446964e-07, Correlation: 0.9999999085670161, R2: 0.999999816692146\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 27.223285675048828\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.045835494995117\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.771041870117188\n",
      "Gradient Norm_Batch: 10.047433853149414\n",
      "6e-07\n",
      "Epoch [148/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833183915778136e-07, Correlation: 0.9999999085718437, R2: 0.9999998166816042\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.973207473754883\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.961814880371094\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.6326265335083\n",
      "Gradient Norm_Batch: 9.584062576293945\n",
      "6e-07\n",
      "Epoch [149/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8330216278172884e-07, Correlation: 0.9999999085694279, R2: 0.9999998166978279\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.89215850830078\n",
      "Gradient Norm_Of_Each_Mini_Batch: 11.73615550994873\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.0161190032959\n",
      "Gradient Norm_Batch: 9.154255867004395\n",
      "6e-07\n",
      "Epoch [150/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8329103568248684e-07, Correlation: 0.9999999085620255, R2: 0.9999998167089634\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.568342208862305\n",
      "Gradient Norm_Of_Each_Mini_Batch: 13.85962200164795\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.663046836853027\n",
      "Gradient Norm_Batch: 9.441205024719238\n",
      "6e-07\n",
      "Epoch [151/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.83311286150456e-07, Correlation: 0.9999999085561413, R2: 0.9999998166887205\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.23472785949707\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.23269271850586\n",
      "Gradient Norm_Of_Each_Mini_Batch: 13.528672218322754\n",
      "Gradient Norm_Batch: 9.761476516723633\n",
      "6e-07\n",
      "Epoch [152/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8332441698021285e-07, Correlation: 0.9999999085599378, R2: 0.9999998166755929\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.26456642150879\n",
      "Gradient Norm_Of_Each_Mini_Batch: 28.335527420043945\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.827049255371094\n",
      "Gradient Norm_Batch: 9.122698783874512\n",
      "6e-07\n",
      "Epoch [153/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8328756823393633e-07, Correlation: 0.9999999085612558, R2: 0.9999998167124314\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 12.718175888061523\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.304031372070312\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.72812843322754\n",
      "Gradient Norm_Batch: 10.142511367797852\n",
      "6e-07\n",
      "Epoch [154/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8334921492169087e-07, Correlation: 0.9999999085646138, R2: 0.9999998166508133\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.013153076171875\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.457782745361328\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.034616470336914\n",
      "Gradient Norm_Batch: 10.000838279724121\n",
      "6e-07\n",
      "Epoch [155/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8333206242004962e-07, Correlation: 0.9999999085686543, R2: 0.9999998166679308\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 13.047253608703613\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.64725112915039\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.773113250732422\n",
      "Gradient Norm_Batch: 9.794647216796875\n",
      "6e-07\n",
      "Epoch [156/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8332791285047279e-07, Correlation: 0.9999999085638984, R2: 0.9999998166721015\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.90919303894043\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.566877365112305\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.73812484741211\n",
      "Gradient Norm_Batch: 8.838427543640137\n",
      "6e-07\n",
      "Epoch [157/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.832697904546876e-07, Correlation: 0.9999999085630618, R2: 0.9999998167302052\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.237154006958008\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.477087020874023\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.302091598510742\n",
      "Gradient Norm_Batch: 9.725784301757812\n",
      "6e-07\n",
      "Epoch [158/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8331292039874825e-07, Correlation: 0.9999999085681371, R2: 0.9999998166870923\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.9194393157959\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.59937286376953\n",
      "Gradient Norm_Of_Each_Mini_Batch: 13.271936416625977\n",
      "Gradient Norm_Batch: 10.630581855773926\n",
      "6e-07\n",
      "Epoch [159/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8336052676204417e-07, Correlation: 0.999999908569432, R2: 0.9999998166394762\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.741098403930664\n",
      "Gradient Norm_Of_Each_Mini_Batch: 12.551923751831055\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.15056324005127\n",
      "Gradient Norm_Batch: 10.27674388885498\n",
      "6e-07\n",
      "Epoch [160/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8333638251988305e-07, Correlation: 0.9999999085702039, R2: 0.999999816663608\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.395267486572266\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.283327102661133\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.688997268676758\n",
      "Gradient Norm_Batch: 10.775813102722168\n",
      "6e-07\n",
      "Epoch [161/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8337418339342548e-07, Correlation: 0.9999999085693073, R2: 0.9999998166258324\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.206951141357422\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.62763214111328\n",
      "Gradient Norm_Of_Each_Mini_Batch: 12.872993469238281\n",
      "Gradient Norm_Batch: 10.374411582946777\n",
      "6e-07\n",
      "Epoch [162/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8336879747948842e-07, Correlation: 0.9999999085618568, R2: 0.9999998166312056\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.02381134033203\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.91168212890625\n",
      "Gradient Norm_Of_Each_Mini_Batch: 9.949796676635742\n",
      "Gradient Norm_Batch: 10.638197898864746\n",
      "6e-07\n",
      "Epoch [163/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8337202334350877e-07, Correlation: 0.9999999085671372, R2: 0.9999998166279942\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.612539291381836\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.943559646606445\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.266563415527344\n",
      "Gradient Norm_Batch: 10.3449125289917\n",
      "6e-07\n",
      "Epoch [164/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8335749984998984e-07, Correlation: 0.9999999085648754, R2: 0.9999998166424896\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.92478084564209\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.379049301147461\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.7091007232666\n",
      "Gradient Norm_Batch: 9.835931777954102\n",
      "6e-07\n",
      "Epoch [165/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8332761442252377e-07, Correlation: 0.9999999085628583, R2: 0.9999998166723946\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.60537338256836\n",
      "Gradient Norm_Of_Each_Mini_Batch: 24.24205780029297\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.210905075073242\n",
      "Gradient Norm_Batch: 10.664620399475098\n",
      "6e-07\n",
      "Epoch [166/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.83362431016576e-07, Correlation: 0.9999999085712885, R2: 0.9999998166375773\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.771766662597656\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.845767974853516\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.47553062438965\n",
      "Gradient Norm_Batch: 10.135263442993164\n",
      "6e-07\n",
      "Epoch [167/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8334898754801543e-07, Correlation: 0.9999999085615929, R2: 0.9999998166510006\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.349388122558594\n",
      "Gradient Norm_Of_Each_Mini_Batch: 27.496923446655273\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.279600143432617\n",
      "Gradient Norm_Batch: 9.557522773742676\n",
      "6e-07\n",
      "Epoch [168/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833139009477236e-07, Correlation: 0.9999999085641619, R2: 0.9999998166861094\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.577207565307617\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.432971954345703\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.436356544494629\n",
      "Gradient Norm_Batch: 10.216024398803711\n",
      "6e-07\n",
      "Epoch [169/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833430758324539e-07, Correlation: 0.9999999085697934, R2: 0.9999998166569404\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 13.794432640075684\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.433809280395508\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.25865364074707\n",
      "Gradient Norm_Batch: 10.88070297241211\n",
      "6e-07\n",
      "Epoch [170/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833858789268561e-07, Correlation: 0.999999908569651, R2: 0.9999998166141325\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.930688858032227\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.49822235107422\n",
      "Gradient Norm_Of_Each_Mini_Batch: 12.784211158752441\n",
      "Gradient Norm_Batch: 10.769801139831543\n",
      "6e-07\n",
      "Epoch [171/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833783187521476e-07, Correlation: 0.9999999085680984, R2: 0.9999998166216661\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.946674346923828\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.6301212310791\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.828899383544922\n",
      "Gradient Norm_Batch: 9.8298921585083\n",
      "6e-07\n",
      "Epoch [172/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833343645785135e-07, Correlation: 0.9999999085619949, R2: 0.9999998166656295\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.067115783691406\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.027292251586914\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.425751686096191\n",
      "Gradient Norm_Batch: 9.598099708557129\n",
      "6e-07\n",
      "Epoch [173/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8331772366764199e-07, Correlation: 0.9999999085627298, R2: 0.9999998166822861\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.706274032592773\n",
      "Gradient Norm_Of_Each_Mini_Batch: 26.33493995666504\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.428743362426758\n",
      "Gradient Norm_Batch: 9.539373397827148\n",
      "6e-07\n",
      "Epoch [174/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8331050455344666e-07, Correlation: 0.9999999085645701, R2: 0.9999998166895023\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.203306198120117\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.723604202270508\n",
      "Gradient Norm_Of_Each_Mini_Batch: 13.911195755004883\n",
      "Gradient Norm_Batch: 9.804001808166504\n",
      "6e-07\n",
      "Epoch [175/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833197842415757e-07, Correlation: 0.9999999085677829, R2: 0.9999998166802128\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.635047912597656\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.519336700439453\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.99881935119629\n",
      "Gradient Norm_Batch: 10.304634094238281\n",
      "6e-07\n",
      "Epoch [176/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8334907281314372e-07, Correlation: 0.9999999085666933, R2: 0.9999998166509267\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.67804527282715\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.01616859436035\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.624773025512695\n",
      "Gradient Norm_Batch: 9.958810806274414\n",
      "6e-07\n",
      "Epoch [177/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833281118024388e-07, Correlation: 0.9999999085631565, R2: 0.9999998166718939\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.00636100769043\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.652719497680664\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.635486602783203\n",
      "Gradient Norm_Batch: 10.382881164550781\n",
      "6e-07\n",
      "Epoch [178/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8335981621930841e-07, Correlation: 0.9999999085655354, R2: 0.999999816640194\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.37408447265625\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.28434944152832\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.579221725463867\n",
      "Gradient Norm_Batch: 10.801207542419434\n",
      "6e-07\n",
      "Epoch [179/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8337165386128618e-07, Correlation: 0.9999999085715547, R2: 0.9999998166283578\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.336977005004883\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.730220794677734\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.994091033935547\n",
      "Gradient Norm_Batch: 10.457313537597656\n",
      "6e-07\n",
      "Epoch [180/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8335529716750898e-07, Correlation: 0.9999999085671604, R2: 0.9999998166446901\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.02352523803711\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.703859329223633\n",
      "Gradient Norm_Of_Each_Mini_Batch: 12.953441619873047\n",
      "Gradient Norm_Batch: 10.580989837646484\n",
      "6e-07\n",
      "Epoch [181/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833562492947749e-07, Correlation: 0.9999999085660618, R2: 0.9999998166437455\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.724727630615234\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.59134292602539\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.81089210510254\n",
      "Gradient Norm_Batch: 10.277179718017578\n",
      "6e-07\n",
      "Epoch [182/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8334065998715232e-07, Correlation: 0.9999999085677265, R2: 0.9999998166593275\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.00961685180664\n",
      "Gradient Norm_Of_Each_Mini_Batch: 13.59560489654541\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.844655990600586\n",
      "Gradient Norm_Batch: 11.142013549804688\n",
      "6e-07\n",
      "Epoch [183/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.83391222208229e-07, Correlation: 0.9999999085746575, R2: 0.9999998166087778\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.307472229003906\n",
      "Gradient Norm_Of_Each_Mini_Batch: 24.294570922851562\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.668712615966797\n",
      "Gradient Norm_Batch: 10.220166206359863\n",
      "6e-07\n",
      "Epoch [184/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8334566220801207e-07, Correlation: 0.9999999085669217, R2: 0.999999816654347\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 24.754514694213867\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.155502319335938\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.715940475463867\n",
      "Gradient Norm_Batch: 11.110502243041992\n",
      "6e-07\n",
      "Epoch [185/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8339321172788914e-07, Correlation: 0.9999999085701922, R2: 0.999999816606797\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 12.612403869628906\n",
      "Gradient Norm_Of_Each_Mini_Batch: 11.853896141052246\n",
      "Gradient Norm_Of_Each_Mini_Batch: 11.732600212097168\n",
      "Gradient Norm_Batch: 10.603809356689453\n",
      "6e-07\n",
      "Epoch [186/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8337115648137114e-07, Correlation: 0.9999999085675395, R2: 0.9999998166288453\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.106271743774414\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.96222496032715\n",
      "Gradient Norm_Of_Each_Mini_Batch: 12.26049518585205\n",
      "Gradient Norm_Batch: 10.37401008605957\n",
      "6e-07\n",
      "Epoch [187/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8335819618187088e-07, Correlation: 0.9999999085670207, R2: 0.9999998166418089\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.153474807739258\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.000452041625977\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.277433395385742\n",
      "Gradient Norm_Batch: 10.241936683654785\n",
      "6e-07\n",
      "Epoch [188/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8335076390485483e-07, Correlation: 0.9999999085663448, R2: 0.9999998166492446\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.17908477783203\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.326749801635742\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.87643051147461\n",
      "Gradient Norm_Batch: 10.332758903503418\n",
      "6e-07\n",
      "Epoch [189/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8335421714255062e-07, Correlation: 0.999999908562926, R2: 0.9999998166457771\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.26416015625\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.191783905029297\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.536954879760742\n",
      "Gradient Norm_Batch: 9.554571151733398\n",
      "6e-07\n",
      "Epoch [190/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8331762419165898e-07, Correlation: 0.9999999085598688, R2: 0.9999998166823619\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.207962036132812\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.483715057373047\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.689390182495117\n",
      "Gradient Norm_Batch: 9.900684356689453\n",
      "6e-07\n",
      "Epoch [191/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8332920603825187e-07, Correlation: 0.9999999085679386, R2: 0.9999998166707754\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 12.255121231079102\n",
      "Gradient Norm_Of_Each_Mini_Batch: 12.334202766418457\n",
      "Gradient Norm_Of_Each_Mini_Batch: 13.477044105529785\n",
      "Gradient Norm_Batch: 9.765318870544434\n",
      "6e-07\n",
      "Epoch [192/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8332816864585766e-07, Correlation: 0.9999999085639911, R2: 0.9999998166718104\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.444278717041016\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.399518966674805\n",
      "Gradient Norm_Of_Each_Mini_Batch: 12.514533996582031\n",
      "Gradient Norm_Batch: 8.965371131896973\n",
      "6e-07\n",
      "Epoch [193/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8328442763504427e-07, Correlation: 0.9999999085598038, R2: 0.9999998167155737\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.749528884887695\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.400686264038086\n",
      "Gradient Norm_Of_Each_Mini_Batch: 26.015506744384766\n",
      "Gradient Norm_Batch: 10.659114837646484\n",
      "6e-07\n",
      "Epoch [194/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8337534868351213e-07, Correlation: 0.9999999085640879, R2: 0.9999998166246563\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 12.26171588897705\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.256101608276367\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.17209243774414\n",
      "Gradient Norm_Batch: 10.194032669067383\n",
      "6e-07\n",
      "Epoch [195/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.83346614335278e-07, Correlation: 0.9999999085670094, R2: 0.9999998166533941\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.298830032348633\n",
      "Gradient Norm_Of_Each_Mini_Batch: 27.32095718383789\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.995031356811523\n",
      "Gradient Norm_Batch: 10.24542236328125\n",
      "6e-07\n",
      "Epoch [196/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833614931001648e-07, Correlation: 0.9999999085606046, R2: 0.9999998166384984\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.239192962646484\n",
      "Gradient Norm_Of_Each_Mini_Batch: 12.681787490844727\n",
      "Gradient Norm_Of_Each_Mini_Batch: 24.164932250976562\n",
      "Gradient Norm_Batch: 11.557220458984375\n",
      "6e-07\n",
      "Epoch [197/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8343413898946892e-07, Correlation: 0.9999999085697108, R2: 0.999999816565871\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.227806091308594\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.809642791748047\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.555522918701172\n",
      "Gradient Norm_Batch: 11.222999572753906\n",
      "6e-07\n",
      "Epoch [198/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8341624752338248e-07, Correlation: 0.9999999085657635, R2: 0.9999998165837406\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.443235397338867\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.438955307006836\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.668428421020508\n",
      "Gradient Norm_Batch: 10.516594886779785\n",
      "6e-07\n",
      "Epoch [199/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8337676976898365e-07, Correlation: 0.99999990855918, R2: 0.9999998166232235\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 13.143399238586426\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.86807632446289\n",
      "Gradient Norm_Of_Each_Mini_Batch: 12.86097526550293\n",
      "Gradient Norm_Batch: 10.384464263916016\n",
      "6e-07\n",
      "Epoch [200/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8336221785375528e-07, Correlation: 0.9999999085652971, R2: 0.9999998166377966\n",
      "Final gradient of the subproblem Core : 10.384464263916016\n",
      "(32, 32)\n",
      "---------------------------Testing with Best Lambda------------------------------\n",
      "Y Test Predicted:  [ 67.54932242  64.18957316 100.90629189 106.79439313  66.52893425\n",
      "  96.79614488 112.76400702  87.08126255  58.88496661  60.74048208\n",
      "  65.75349716  64.15717172  82.80149914  68.56409812 142.45877769\n",
      "  98.02001105  98.3530191   55.41634739  96.74724667  87.26443291\n",
      "  47.97826449  91.13721351  73.5649994   77.46465194  46.96432166\n",
      "  79.38007473  84.71051329  45.49457414  87.56687419  73.47977584\n",
      "  86.90812361  88.26721905 124.65312267  81.42688598  87.54740221\n",
      "  82.79810478  98.73494231  69.03779375  80.63840467  93.083423\n",
      "  68.40989766  87.28012217  62.04203939  57.34039195  56.0211765\n",
      "  89.30948951 110.54336349 134.61089234  43.26801663  74.48155072\n",
      "  96.2330459   87.89116763  79.23389713  90.72723155 125.88518093\n",
      " 160.37009356  60.51190922  86.88829167  89.57424276  85.49699538\n",
      "  79.97216107  95.54238185  57.09915505  50.36950424  70.86129715\n",
      " 131.2339601   94.51162181  83.16025244  62.9265213   84.10776468\n",
      "  76.25936689  96.48139847  77.08427207  65.79259289  71.77935616\n",
      "  65.14981366  61.02643632  68.75447131  66.41034881 104.95705502\n",
      "  79.74044071  91.80693167 110.44693258  62.2478728   57.61575206\n",
      "  98.12929327  67.83584845  72.12855294 127.85066488  63.30880296\n",
      " 103.69726848 150.07981006  67.41983032  92.38853205 123.67881106\n",
      "  46.91118671  64.66064646 108.22053867 101.13978055 113.37858312]\n",
      "Y Test Actual:  [ 67.54932471  64.19367989 100.89907103 106.80033802  66.5296898\n",
      "  96.80085769 112.75716496  87.08379176  58.86645104  60.75871896\n",
      "  65.76651181  64.14603363  82.80477245  68.55912818 142.45923938\n",
      "  98.00053671  98.36489423  55.40988723  96.73410604  87.25275894\n",
      "  47.97392502  91.14302569  73.57729565  77.46144622  46.97758628\n",
      "  79.38435102  84.7040345   45.4879342   87.55672081  73.49886432\n",
      "  86.90598491  88.25175908 124.63253198  81.43265772  87.55287141\n",
      "  82.8080788   98.72768177  69.0468243   80.62856336  93.08545506\n",
      "  68.40807412  87.26847367  62.04717769  57.33990441  56.01334774\n",
      "  89.31934273 110.55219975 134.61307846  43.26935217  74.47617341\n",
      "  96.21843559  87.89937403  79.21862917  90.71786386 125.8824493\n",
      " 160.38439066  60.50387741  86.87508783  89.5739495   85.50880057\n",
      "  79.97185552  95.54965657  57.09355445  50.35928763  70.86185745\n",
      " 131.24078613  94.50837386  83.15863626  62.93422433  84.09831139\n",
      "  76.26182003  96.48126457  77.07300387  65.77838739  71.76861932\n",
      "  65.14937109  61.0440168   68.74400314  66.41967439 104.95340089\n",
      "  79.75508697  91.80406117 110.44770514  62.24491669  57.60974417\n",
      "  98.13365195  67.82489044  72.14320497 127.85744229  63.30804812\n",
      " 103.70015697 150.07489827  67.42743244  92.3906977  123.66379363\n",
      "  46.92967844  64.66629945 108.22935093 101.14612067 113.38031778]\n",
      "Error Report on Testing _ With best Lambda\n",
      "SGD Learning Rate: 6e-07\n",
      "SGD_Alpha chosen for model:  2.5\n",
      "SGD_Test Normalized Estimation Error:  8.549679709709748e-09\n",
      "SGD_Test NMSE Loss:  1.1274309003218634e-08\n",
      "SGD_Test R2 Loss:  0.999999843688618\n",
      "SGD_Test Correlation:  0.9999999220235805\n",
      "Objective Function Values 1110.0198444332875\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final gradient: 10.384464263916016\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------------------Learning Rate 5e-07 -----------------------------------------\n",
      "Objective Function Value: 3408906.126530059\n",
      "--------------------------------------------------------------BCD iteration 0 --------------------------------------------------------------\n",
      "---------------------------------------------Sep 0 Factor 0 -------------------------------------------------\n",
      "Iteration: 0, Separation Rank: 0, Factor Matrix: 0, Objective Function Value: 4178689.296386482\n",
      "---------------------------------------------Sep 0 Factor 1 -------------------------------------------------\n",
      "Iteration: 0, Separation Rank: 0, Factor Matrix: 1, Objective Function Value: 3488977.6201287964\n",
      "---------------------------------------------Sep 1 Factor 0 -------------------------------------------------\n",
      "Iteration: 0, Separation Rank: 1, Factor Matrix: 0, Objective Function Value: 3092821.460810512\n",
      "---------------------------------------------Sep 1 Factor 1 -------------------------------------------------\n",
      "Iteration: 0, Separation Rank: 1, Factor Matrix: 1, Objective Function Value: 4026588.625912\n",
      "---------------------------------------------Core-------------------------------------------------\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 306513.15625\n",
      "Gradient Norm_Of_Each_Mini_Batch: 234753.5625\n",
      "Gradient Norm_Of_Each_Mini_Batch: 127494.0546875\n",
      "Gradient Norm_Batch: 500102.34375\n",
      "5e-07\n",
      "Epoch [1/200], Loss: 188800.3594, Gap to Optimality: 188800.3594, NMSE: 0.547329843044281, Correlation: 0.701149300946172, R2: 0.45267020833432703\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 171567.484375\n",
      "Gradient Norm_Of_Each_Mini_Batch: 205784.0625\n",
      "Gradient Norm_Of_Each_Mini_Batch: 80644.609375\n",
      "Gradient Norm_Batch: 342657.15625\n",
      "5e-07\n",
      "Epoch [2/200], Loss: 94727.6797, Gap to Optimality: 94727.6797, NMSE: 0.2746124565601349, Correlation: 0.890734070712534, R2: 0.7253875535146732\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 131566.640625\n",
      "Gradient Norm_Of_Each_Mini_Batch: 120184.765625\n",
      "Gradient Norm_Of_Each_Mini_Batch: 59088.75390625\n",
      "Gradient Norm_Batch: 240489.5\n",
      "5e-07\n",
      "Epoch [3/200], Loss: 50009.2656, Gap to Optimality: 50009.2656, NMSE: 0.14497295022010803, Correlation: 0.9487530014336688, R2: 0.855027045674527\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 84127.0078125\n",
      "Gradient Norm_Of_Each_Mini_Batch: 85562.390625\n",
      "Gradient Norm_Of_Each_Mini_Batch: 52143.06640625\n",
      "Gradient Norm_Batch: 172611.25\n",
      "5e-07\n",
      "Epoch [4/200], Loss: 27640.7559, Gap to Optimality: 27640.7559, NMSE: 0.08012590557336807, Correlation: 0.9721276453871222, R2: 0.919874089659706\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 73814.8203125\n",
      "Gradient Norm_Of_Each_Mini_Batch: 55697.9296875\n",
      "Gradient Norm_Of_Each_Mini_Batch: 32109.8828125\n",
      "Gradient Norm_Batch: 126645.796875\n",
      "5e-07\n",
      "Epoch [5/200], Loss: 15918.5566, Gap to Optimality: 15918.5566, NMSE: 0.04614265635609627, Correlation: 0.9835656748891305, R2: 0.9538573447895382\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 54441.89453125\n",
      "Gradient Norm_Of_Each_Mini_Batch: 41179.25390625\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22397.458984375\n",
      "Gradient Norm_Batch: 94816.953125\n",
      "5e-07\n",
      "Epoch [6/200], Loss: 9517.4795, Gap to Optimality: 9517.4795, NMSE: 0.02758547104895115, Correlation: 0.9897777005634485, R2: 0.9724145277249642\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 39814.6328125\n",
      "Gradient Norm_Of_Each_Mini_Batch: 35125.265625\n",
      "Gradient Norm_Of_Each_Mini_Batch: 13969.9931640625\n",
      "Gradient Norm_Batch: 72282.296875\n",
      "5e-07\n",
      "Epoch [7/200], Loss: 5876.1226, Gap to Optimality: 5876.1226, NMSE: 0.017028825357556343, Correlation: 0.993413274557211, R2: 0.9829711736916471\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 29749.015625\n",
      "Gradient Norm_Of_Each_Mini_Batch: 25452.900390625\n",
      "Gradient Norm_Of_Each_Mini_Batch: 13507.69140625\n",
      "Gradient Norm_Batch: 56084.03125\n",
      "5e-07\n",
      "Epoch [8/200], Loss: 3736.1326, Gap to Optimality: 3736.1326, NMSE: 0.01082472875714302, Correlation: 0.9956546673410174, R2: 0.9891752705325408\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22372.1953125\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23193.572265625\n",
      "Gradient Norm_Of_Each_Mini_Batch: 7576.84375\n",
      "Gradient Norm_Batch: 44133.8828125\n",
      "5e-07\n",
      "Epoch [9/200], Loss: 2431.7993, Gap to Optimality: 2431.7993, NMSE: 0.007043266203254461, Correlation: 0.9970796347112603, R2: 0.9929567344902066\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19105.123046875\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16582.01171875\n",
      "Gradient Norm_Of_Each_Mini_Batch: 7447.9404296875\n",
      "Gradient Norm_Batch: 35151.3828125\n",
      "5e-07\n",
      "Epoch [10/200], Loss: 1615.2266, Gap to Optimality: 1615.2266, NMSE: 0.004675867035984993, Correlation: 0.998004584562456, R2: 0.9953241328273892\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15037.3681640625\n",
      "Gradient Norm_Of_Each_Mini_Batch: 13028.5546875\n",
      "Gradient Norm_Of_Each_Mini_Batch: 6290.51904296875\n",
      "Gradient Norm_Batch: 28334.353515625\n",
      "5e-07\n",
      "Epoch [11/200], Loss: 1093.2107, Gap to Optimality: 1093.2107, NMSE: 0.0031624252442270517, Correlation: 0.9986192850070187, R2: 0.9968375747331606\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 11916.8984375\n",
      "Gradient Norm_Of_Each_Mini_Batch: 10296.5908203125\n",
      "Gradient Norm_Of_Each_Mini_Batch: 5611.046875\n",
      "Gradient Norm_Batch: 23062.5703125\n",
      "5e-07\n",
      "Epoch [12/200], Loss: 751.5107, Gap to Optimality: 751.5107, NMSE: 0.0021717462223023176, Correlation: 0.9990337720162663, R2: 0.9978282538979953\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 8422.51171875\n",
      "Gradient Norm_Of_Each_Mini_Batch: 9617.458984375\n",
      "Gradient Norm_Of_Each_Mini_Batch: 4599.26416015625\n",
      "Gradient Norm_Batch: 18926.677734375\n",
      "5e-07\n",
      "Epoch [13/200], Loss: 523.5862, Gap to Optimality: 523.5862, NMSE: 0.0015109216328710318, Correlation: 0.9993168989789909, R2: 0.998489078449126\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 7326.25732421875\n",
      "Gradient Norm_Of_Each_Mini_Batch: 7292.310546875\n",
      "Gradient Norm_Of_Each_Mini_Batch: 3850.37060546875\n",
      "Gradient Norm_Batch: 15656.779296875\n",
      "5e-07\n",
      "Epoch [14/200], Loss: 369.3516, Gap to Optimality: 369.3516, NMSE: 0.0010637405794113874, Correlation: 0.9995132057007177, R2: 0.9989362594703391\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 5547.95751953125\n",
      "Gradient Norm_Of_Each_Mini_Batch: 6196.56884765625\n",
      "Gradient Norm_Of_Each_Mini_Batch: 3486.0537109375\n",
      "Gradient Norm_Batch: 13032.21875\n",
      "5e-07\n",
      "Epoch [15/200], Loss: 263.3192, Gap to Optimality: 263.3192, NMSE: 0.0007563087274320424, Correlation: 0.999650220280547, R2: 0.9992436913643248\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 5721.45703125\n",
      "Gradient Norm_Of_Each_Mini_Batch: 4692.32763671875\n",
      "Gradient Norm_Of_Each_Mini_Batch: 2470.75244140625\n",
      "Gradient Norm_Batch: 10910.720703125\n",
      "5e-07\n",
      "Epoch [16/200], Loss: 189.4837, Gap to Optimality: 189.4837, NMSE: 0.0005422250833362341, Correlation: 0.9997473101458272, R2: 0.9994577749236382\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 4284.025390625\n",
      "Gradient Norm_Of_Each_Mini_Batch: 4632.5126953125\n",
      "Gradient Norm_Of_Each_Mini_Batch: 1778.646240234375\n",
      "Gradient Norm_Batch: 9173.4150390625\n",
      "5e-07\n",
      "Epoch [17/200], Loss: 137.4933, Gap to Optimality: 137.4933, NMSE: 0.00039147649658843875, Correlation: 0.9998162022558128, R2: 0.9996085235001156\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 3540.629150390625\n",
      "Gradient Norm_Of_Each_Mini_Batch: 3748.3212890625\n",
      "Gradient Norm_Of_Each_Mini_Batch: 1832.3170166015625\n",
      "Gradient Norm_Batch: 7747.4189453125\n",
      "5e-07\n",
      "Epoch [18/200], Loss: 100.6468, Gap to Optimality: 100.6468, NMSE: 0.0002846359566319734, Correlation: 0.9998656387624602, R2: 0.9997153640378541\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 3111.560791015625\n",
      "Gradient Norm_Of_Each_Mini_Batch: 2876.44091796875\n",
      "Gradient Norm_Of_Each_Mini_Batch: 1673.500244140625\n",
      "Gradient Norm_Batch: 6567.7197265625\n",
      "5e-07\n",
      "Epoch [19/200], Loss: 74.3082, Gap to Optimality: 74.3082, NMSE: 0.00020826169929932803, Correlation: 0.9999011234912552, R2: 0.9997917383158451\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 2301.34423828125\n",
      "Gradient Norm_Of_Each_Mini_Batch: 2698.466064453125\n",
      "Gradient Norm_Of_Each_Mini_Batch: 1490.3848876953125\n",
      "Gradient Norm_Batch: 5585.4619140625\n",
      "5e-07\n",
      "Epoch [20/200], Loss: 55.3269, Gap to Optimality: 55.3269, NMSE: 0.00015321985119953752, Correlation: 0.9999268906286275, R2: 0.9998467801519053\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 2289.196044921875\n",
      "Gradient Norm_Of_Each_Mini_Batch: 2145.673583984375\n",
      "Gradient Norm_Of_Each_Mini_Batch: 1056.2236328125\n",
      "Gradient Norm_Batch: 4764.99560546875\n",
      "5e-07\n",
      "Epoch [21/200], Loss: 41.5706, Gap to Optimality: 41.5706, NMSE: 0.00011332800204399973, Correlation: 0.9999457094358549, R2: 0.999886671999528\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 2087.489013671875\n",
      "Gradient Norm_Of_Each_Mini_Batch: 1760.3951416015625\n",
      "Gradient Norm_Of_Each_Mini_Batch: 894.5650634765625\n",
      "Gradient Norm_Batch: 4075.599853515625\n",
      "5e-07\n",
      "Epoch [22/200], Loss: 31.5313, Gap to Optimality: 31.5313, NMSE: 8.421397069469094e-05, Correlation: 0.9999595405151305, R2: 0.9999157860257344\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 1593.460693359375\n",
      "Gradient Norm_Of_Each_Mini_Batch: 1517.6224365234375\n",
      "Gradient Norm_Of_Each_Mini_Batch: 911.5519409179688\n",
      "Gradient Norm_Batch: 3495.45654296875\n",
      "5e-07\n",
      "Epoch [23/200], Loss: 24.1819, Gap to Optimality: 24.1819, NMSE: 6.289959856076166e-05, Correlation: 0.9999696880787426, R2: 0.9999371004042489\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 1613.224365234375\n",
      "Gradient Norm_Of_Each_Mini_Batch: 1176.643310546875\n",
      "Gradient Norm_Of_Each_Mini_Batch: 622.9946899414062\n",
      "Gradient Norm_Batch: 3004.918212890625\n",
      "5e-07\n",
      "Epoch [24/200], Loss: 18.7639, Gap to Optimality: 18.7639, NMSE: 4.718597847386263e-05, Correlation: 0.9999772011767349, R2: 0.9999528140199244\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 1142.6148681640625\n",
      "Gradient Norm_Of_Each_Mini_Batch: 1138.1702880859375\n",
      "Gradient Norm_Of_Each_Mini_Batch: 705.958251953125\n",
      "Gradient Norm_Batch: 2588.588623046875\n",
      "5e-07\n",
      "Epoch [25/200], Loss: 14.7537, Gap to Optimality: 14.7537, NMSE: 3.5554498026613146e-05, Correlation: 0.9999827741031606, R2: 0.999964445503052\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 1186.60498046875\n",
      "Gradient Norm_Of_Each_Mini_Batch: 956.0836791992188\n",
      "Gradient Norm_Of_Each_Mini_Batch: 518.2398681640625\n",
      "Gradient Norm_Batch: 2234.29150390625\n",
      "5e-07\n",
      "Epoch [26/200], Loss: 11.7678, Gap to Optimality: 11.7678, NMSE: 2.6893578251474537e-05, Correlation: 0.9999869414071538, R2: 0.9999731064216398\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 912.9190673828125\n",
      "Gradient Norm_Of_Each_Mini_Batch: 853.8407592773438\n",
      "Gradient Norm_Of_Each_Mini_Batch: 468.0044860839844\n",
      "Gradient Norm_Batch: 1932.9593505859375\n",
      "5e-07\n",
      "Epoch [27/200], Loss: 9.5439, Gap to Optimality: 9.5439, NMSE: 2.044217944785487e-05, Correlation: 0.9999900485464345, R2: 0.9999795578206913\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 765.6075439453125\n",
      "Gradient Norm_Of_Each_Mini_Batch: 690.1909790039062\n",
      "Gradient Norm_Of_Each_Mini_Batch: 477.545166015625\n",
      "Gradient Norm_Batch: 1675.8343505859375\n",
      "5e-07\n",
      "Epoch [28/200], Loss: 7.8780, Gap to Optimality: 7.8780, NMSE: 1.5609439287800342e-05, Correlation: 0.9999923868607444, R2: 0.9999843905608086\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 687.7098388671875\n",
      "Gradient Norm_Of_Each_Mini_Batch: 622.137451171875\n",
      "Gradient Norm_Of_Each_Mini_Batch: 371.9292907714844\n",
      "Gradient Norm_Batch: 1455.4776611328125\n",
      "5e-07\n",
      "Epoch [29/200], Loss: 6.6234, Gap to Optimality: 6.6234, NMSE: 1.1969513252552133e-05, Correlation: 0.9999941477606633, R2: 0.9999880304868871\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 635.1895141601562\n",
      "Gradient Norm_Of_Each_Mini_Batch: 533.9918212890625\n",
      "Gradient Norm_Of_Each_Mini_Batch: 298.93255615234375\n",
      "Gradient Norm_Batch: 1266.3768310546875\n",
      "5e-07\n",
      "Epoch [30/200], Loss: 5.6753, Gap to Optimality: 5.6753, NMSE: 9.218532795784995e-06, Correlation: 0.9999954864495672, R2: 0.9999907814664638\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 441.54559326171875\n",
      "Gradient Norm_Of_Each_Mini_Batch: 474.0515441894531\n",
      "Gradient Norm_Of_Each_Mini_Batch: 341.4759521484375\n",
      "Gradient Norm_Batch: 1104.2630615234375\n",
      "5e-07\n",
      "Epoch [31/200], Loss: 4.9581, Gap to Optimality: 4.9581, NMSE: 7.1375557126884814e-06, Correlation: 0.9999965016400925, R2: 0.9999928624448527\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 453.9331970214844\n",
      "Gradient Norm_Of_Each_Mini_Batch: 443.04693603515625\n",
      "Gradient Norm_Of_Each_Mini_Batch: 215.9923553466797\n",
      "Gradient Norm_Batch: 964.4261474609375\n",
      "5e-07\n",
      "Epoch [32/200], Loss: 4.4113, Gap to Optimality: 4.4113, NMSE: 5.550521109398687e-06, Correlation: 0.9999972765132137, R2: 0.9999944494790973\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 410.3374938964844\n",
      "Gradient Norm_Of_Each_Mini_Batch: 344.457763671875\n",
      "Gradient Norm_Of_Each_Mini_Batch: 232.48263549804688\n",
      "Gradient Norm_Batch: 843.6563720703125\n",
      "5e-07\n",
      "Epoch [33/200], Loss: 3.9935, Gap to Optimality: 3.9935, NMSE: 4.338060534792021e-06, Correlation: 0.9999978686734993, R2: 0.9999956619393263\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 354.6441345214844\n",
      "Gradient Norm_Of_Each_Mini_Batch: 311.2724914550781\n",
      "Gradient Norm_Of_Each_Mini_Batch: 182.39378356933594\n",
      "Gradient Norm_Batch: 739.288818359375\n",
      "5e-07\n",
      "Epoch [34/200], Loss: 3.6736, Gap to Optimality: 3.6736, NMSE: 3.4094841794285458e-06, Correlation: 0.9999983226886712, R2: 0.9999965905156056\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 317.31646728515625\n",
      "Gradient Norm_Of_Each_Mini_Batch: 285.5093688964844\n",
      "Gradient Norm_Of_Each_Mini_Batch: 135.2759246826172\n",
      "Gradient Norm_Batch: 648.91796875\n",
      "5e-07\n",
      "Epoch [35/200], Loss: 3.4277, Gap to Optimality: 3.4277, NMSE: 2.695516513995244e-06, Correlation: 0.9999986722974076, R2: 0.9999973044835112\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 238.68572998046875\n",
      "Gradient Norm_Of_Each_Mini_Batch: 278.8998718261719\n",
      "Gradient Norm_Of_Each_Mini_Batch: 147.52801513671875\n",
      "Gradient Norm_Batch: 570.4624633789062\n",
      "5e-07\n",
      "Epoch [36/200], Loss: 3.2380, Gap to Optimality: 3.2380, NMSE: 2.144756308553042e-06, Correlation: 0.9999989419909688, R2: 0.9999978552439812\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 206.0423126220703\n",
      "Gradient Norm_Of_Each_Mini_Batch: 252.73587036132812\n",
      "Gradient Norm_Of_Each_Mini_Batch: 118.03472137451172\n",
      "Gradient Norm_Batch: 502.1971435546875\n",
      "5e-07\n",
      "Epoch [37/200], Loss: 3.0912, Gap to Optimality: 3.0912, NMSE: 1.7183918998853187e-06, Correlation: 0.999999151433598, R2: 0.999998281608142\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 200.69766235351562\n",
      "Gradient Norm_Of_Each_Mini_Batch: 198.2956085205078\n",
      "Gradient Norm_Of_Each_Mini_Batch: 109.48258972167969\n",
      "Gradient Norm_Batch: 442.9049377441406\n",
      "5e-07\n",
      "Epoch [38/200], Loss: 2.9775, Gap to Optimality: 2.9775, NMSE: 1.3882208804716356e-06, Correlation: 0.999999314057773, R2: 0.9999986117791342\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 194.603759765625\n",
      "Gradient Norm_Of_Each_Mini_Batch: 186.52044677734375\n",
      "Gradient Norm_Of_Each_Mini_Batch: 90.79999542236328\n",
      "Gradient Norm_Batch: 391.12347412109375\n",
      "5e-07\n",
      "Epoch [39/200], Loss: 2.8889, Gap to Optimality: 2.8889, NMSE: 1.130882424149604e-06, Correlation: 0.9999994411736962, R2: 0.99999886911765\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 172.3759002685547\n",
      "Gradient Norm_Of_Each_Mini_Batch: 136.73849487304688\n",
      "Gradient Norm_Of_Each_Mini_Batch: 90.32608795166016\n",
      "Gradient Norm_Batch: 345.8509521484375\n",
      "5e-07\n",
      "Epoch [40/200], Loss: 2.8199, Gap to Optimality: 2.8199, NMSE: 9.303568617724522e-07, Correlation: 0.9999995397317251, R2: 0.9999990696431554\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 145.19871520996094\n",
      "Gradient Norm_Of_Each_Mini_Batch: 146.9779052734375\n",
      "Gradient Norm_Of_Each_Mini_Batch: 69.65568542480469\n",
      "Gradient Norm_Batch: 306.2077331542969\n",
      "5e-07\n",
      "Epoch [41/200], Loss: 2.7658, Gap to Optimality: 2.7658, NMSE: 7.733441407253849e-07, Correlation: 0.9999996172234789, R2: 0.9999992266558352\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 114.40200805664062\n",
      "Gradient Norm_Of_Each_Mini_Batch: 135.34889221191406\n",
      "Gradient Norm_Of_Each_Mini_Batch: 58.46453094482422\n",
      "Gradient Norm_Batch: 271.4466552734375\n",
      "5e-07\n",
      "Epoch [42/200], Loss: 2.7235, Gap to Optimality: 2.7235, NMSE: 6.502829137389199e-07, Correlation: 0.9999996779177648, R2: 0.9999993497170996\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 107.66285705566406\n",
      "Gradient Norm_Of_Each_Mini_Batch: 111.60662078857422\n",
      "Gradient Norm_Of_Each_Mini_Batch: 54.511295318603516\n",
      "Gradient Norm_Batch: 240.88345336914062\n",
      "5e-07\n",
      "Epoch [43/200], Loss: 2.6902, Gap to Optimality: 2.6902, NMSE: 5.535141553991707e-07, Correlation: 0.9999997256216858, R2: 0.9999994464859224\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 105.07068634033203\n",
      "Gradient Norm_Of_Each_Mini_Batch: 89.01067352294922\n",
      "Gradient Norm_Of_Each_Mini_Batch: 48.267887115478516\n",
      "Gradient Norm_Batch: 214.06973266601562\n",
      "5e-07\n",
      "Epoch [44/200], Loss: 2.6640, Gap to Optimality: 2.6640, NMSE: 4.773755790665746e-07, Correlation: 0.9999997632722145, R2: 0.9999995226243986\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 94.24736022949219\n",
      "Gradient Norm_Of_Each_Mini_Batch: 83.7107925415039\n",
      "Gradient Norm_Of_Each_Mini_Batch: 51.18484878540039\n",
      "Gradient Norm_Batch: 190.38421630859375\n",
      "5e-07\n",
      "Epoch [45/200], Loss: 2.6433, Gap to Optimality: 2.6433, NMSE: 4.17173453115538e-07, Correlation: 0.9999997929961504, R2: 0.9999995828265796\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 81.2043228149414\n",
      "Gradient Norm_Of_Each_Mini_Batch: 80.06797790527344\n",
      "Gradient Norm_Of_Each_Mini_Batch: 39.553009033203125\n",
      "Gradient Norm_Batch: 169.44857788085938\n",
      "5e-07\n",
      "Epoch [46/200], Loss: 2.6270, Gap to Optimality: 2.6270, NMSE: 3.6953815651941113e-07, Correlation: 0.9999998165078328, R2: 0.9999996304618463\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 77.10778045654297\n",
      "Gradient Norm_Of_Each_Mini_Batch: 70.70081329345703\n",
      "Gradient Norm_Of_Each_Mini_Batch: 41.1390495300293\n",
      "Gradient Norm_Batch: 150.96524047851562\n",
      "5e-07\n",
      "Epoch [47/200], Loss: 2.6140, Gap to Optimality: 2.6140, NMSE: 3.3175592761836015e-07, Correlation: 0.9999998351935463, R2: 0.9999996682440697\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 59.213783264160156\n",
      "Gradient Norm_Of_Each_Mini_Batch: 65.90648651123047\n",
      "Gradient Norm_Of_Each_Mini_Batch: 39.16175079345703\n",
      "Gradient Norm_Batch: 134.5126495361328\n",
      "5e-07\n",
      "Epoch [48/200], Loss: 2.6037, Gap to Optimality: 2.6037, NMSE: 3.0171776188581134e-07, Correlation: 0.999999849970748, R2: 0.9999996982822239\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 52.714664459228516\n",
      "Gradient Norm_Of_Each_Mini_Batch: 57.22737121582031\n",
      "Gradient Norm_Of_Each_Mini_Batch: 35.707618713378906\n",
      "Gradient Norm_Batch: 120.06963348388672\n",
      "5e-07\n",
      "Epoch [49/200], Loss: 2.5955, Gap to Optimality: 2.5955, NMSE: 2.779504200134397e-07, Correlation: 0.9999998617271109, R2: 0.9999997220495733\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 52.00149917602539\n",
      "Gradient Norm_Of_Each_Mini_Batch: 53.32152557373047\n",
      "Gradient Norm_Of_Each_Mini_Batch: 25.58277702331543\n",
      "Gradient Norm_Batch: 107.36085510253906\n",
      "5e-07\n",
      "Epoch [50/200], Loss: 2.5890, Gap to Optimality: 2.5890, NMSE: 2.5910026124620344e-07, Correlation: 0.9999998711128001, R2: 0.9999997408997273\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 51.46707534790039\n",
      "Gradient Norm_Of_Each_Mini_Batch: 45.03004837036133\n",
      "Gradient Norm_Of_Each_Mini_Batch: 24.018476486206055\n",
      "Gradient Norm_Batch: 96.02671813964844\n",
      "5e-07\n",
      "Epoch [51/200], Loss: 2.5838, Gap to Optimality: 2.5838, NMSE: 2.44038318442108e-07, Correlation: 0.9999998785717387, R2: 0.9999997559616539\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 41.312599182128906\n",
      "Gradient Norm_Of_Each_Mini_Batch: 40.75261688232422\n",
      "Gradient Norm_Of_Each_Mini_Batch: 25.08422088623047\n",
      "Gradient Norm_Batch: 86.02783966064453\n",
      "5e-07\n",
      "Epoch [52/200], Loss: 2.5797, Gap to Optimality: 2.5797, NMSE: 2.3204994192838058e-07, Correlation: 0.9999998845281652, R2: 0.9999997679500662\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 36.48493194580078\n",
      "Gradient Norm_Of_Each_Mini_Batch: 45.76994323730469\n",
      "Gradient Norm_Of_Each_Mini_Batch: 24.460355758666992\n",
      "Gradient Norm_Batch: 76.91930389404297\n",
      "5e-07\n",
      "Epoch [53/200], Loss: 2.5764, Gap to Optimality: 2.5764, NMSE: 2.2234377183849574e-07, Correlation: 0.9999998892730797, R2: 0.9999997776562374\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 38.44256591796875\n",
      "Gradient Norm_Of_Each_Mini_Batch: 40.33891677856445\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.24152183532715\n",
      "Gradient Norm_Batch: 68.90675354003906\n",
      "5e-07\n",
      "Epoch [54/200], Loss: 2.5737, Gap to Optimality: 2.5737, NMSE: 2.1462091126522864e-07, Correlation: 0.9999998930906175, R2: 0.9999997853790793\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 30.264968872070312\n",
      "Gradient Norm_Of_Each_Mini_Batch: 33.09174346923828\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.007204055786133\n",
      "Gradient Norm_Batch: 61.962074279785156\n",
      "5e-07\n",
      "Epoch [55/200], Loss: 2.5716, Gap to Optimality: 2.5716, NMSE: 2.0852564830420306e-07, Correlation: 0.9999998961321033, R2: 0.9999997914743372\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 28.83184814453125\n",
      "Gradient Norm_Of_Each_Mini_Batch: 29.144094467163086\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.480318069458008\n",
      "Gradient Norm_Batch: 55.712093353271484\n",
      "5e-07\n",
      "Epoch [56/200], Loss: 2.5700, Gap to Optimality: 2.5700, NMSE: 2.0361326846796146e-07, Correlation: 0.9999998985658703, R2: 0.9999997963867302\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 32.16569137573242\n",
      "Gradient Norm_Of_Each_Mini_Batch: 24.87704086303711\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.531646728515625\n",
      "Gradient Norm_Batch: 50.0357551574707\n",
      "5e-07\n",
      "Epoch [57/200], Loss: 2.5686, Gap to Optimality: 2.5686, NMSE: 1.9962668318385113e-07, Correlation: 0.9999999005188638, R2: 0.9999998003733166\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.243772506713867\n",
      "Gradient Norm_Of_Each_Mini_Batch: 28.432418823242188\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.828651428222656\n",
      "Gradient Norm_Batch: 45.10040283203125\n",
      "5e-07\n",
      "Epoch [58/200], Loss: 2.5675, Gap to Optimality: 2.5675, NMSE: 1.9643800897028996e-07, Correlation: 0.9999999021134318, R2: 0.9999998035619927\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 26.15139389038086\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.38027000427246\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.04678726196289\n",
      "Gradient Norm_Batch: 40.86500549316406\n",
      "5e-07\n",
      "Epoch [59/200], Loss: 2.5666, Gap to Optimality: 2.5666, NMSE: 1.9393658590161067e-07, Correlation: 0.9999999033689125, R2: 0.9999998060634341\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 32.28537368774414\n",
      "Gradient Norm_Of_Each_Mini_Batch: 26.100496292114258\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.592395782470703\n",
      "Gradient Norm_Batch: 37.06238555908203\n",
      "5e-07\n",
      "Epoch [60/200], Loss: 2.5659, Gap to Optimality: 2.5659, NMSE: 1.919161434216221e-07, Correlation: 0.9999999043776161, R2: 0.9999998080838354\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.758625984191895\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.76862907409668\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.070423126220703\n",
      "Gradient Norm_Batch: 33.361515045166016\n",
      "5e-07\n",
      "Epoch [61/200], Loss: 2.5654, Gap to Optimality: 2.5654, NMSE: 1.90210585060413e-07, Correlation: 0.9999999051951765, R2: 0.9999998097894245\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 24.65445899963379\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.141645431518555\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.067136764526367\n",
      "Gradient Norm_Batch: 30.515439987182617\n",
      "5e-07\n",
      "Epoch [62/200], Loss: 2.5649, Gap to Optimality: 2.5649, NMSE: 1.8891594777414866e-07, Correlation: 0.9999999058566711, R2: 0.9999998110840439\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.61157989501953\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.52646255493164\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.939481735229492\n",
      "Gradient Norm_Batch: 27.65077781677246\n",
      "5e-07\n",
      "Epoch [63/200], Loss: 2.5646, Gap to Optimality: 2.5646, NMSE: 1.8782648680826242e-07, Correlation: 0.9999999063765127, R2: 0.9999998121735284\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.992225646972656\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.072080612182617\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.115448951721191\n",
      "Gradient Norm_Batch: 25.03863525390625\n",
      "5e-07\n",
      "Epoch [64/200], Loss: 2.5643, Gap to Optimality: 2.5643, NMSE: 1.8694059633617144e-07, Correlation: 0.9999999067936368, R2: 0.9999998130594026\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.44764518737793\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.383121490478516\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.03968620300293\n",
      "Gradient Norm_Batch: 22.96338653564453\n",
      "5e-07\n",
      "Epoch [65/200], Loss: 2.5640, Gap to Optimality: 2.5640, NMSE: 1.8625340203470842e-07, Correlation: 0.9999999071363206, R2: 0.9999998137466124\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.69922637939453\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.312559127807617\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.541606903076172\n",
      "Gradient Norm_Batch: 21.120603561401367\n",
      "5e-07\n",
      "Epoch [66/200], Loss: 2.5638, Gap to Optimality: 2.5638, NMSE: 1.8568368886917597e-07, Correlation: 0.9999999074176512, R2: 0.9999998143163161\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.75414276123047\n",
      "Gradient Norm_Of_Each_Mini_Batch: 26.42062759399414\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.015552520751953\n",
      "Gradient Norm_Batch: 19.332834243774414\n",
      "5e-07\n",
      "Epoch [67/200], Loss: 2.5637, Gap to Optimality: 2.5637, NMSE: 1.85221082915632e-07, Correlation: 0.9999999076373006, R2: 0.9999998147789187\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.830127716064453\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.999092102050781\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.204621315002441\n",
      "Gradient Norm_Batch: 18.00574493408203\n",
      "5e-07\n",
      "Epoch [68/200], Loss: 2.5635, Gap to Optimality: 2.5635, NMSE: 1.8487247643861338e-07, Correlation: 0.9999999078152851, R2: 0.9999998151275237\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.213346481323242\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.044279098510742\n",
      "Gradient Norm_Of_Each_Mini_Batch: 11.774651527404785\n",
      "Gradient Norm_Batch: 16.938711166381836\n",
      "5e-07\n",
      "Epoch [69/200], Loss: 2.5634, Gap to Optimality: 2.5634, NMSE: 1.845958053081631e-07, Correlation: 0.9999999079603206, R2: 0.9999998154041898\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.731218338012695\n",
      "Gradient Norm_Of_Each_Mini_Batch: 24.106327056884766\n",
      "Gradient Norm_Of_Each_Mini_Batch: 11.658260345458984\n",
      "Gradient Norm_Batch: 16.089893341064453\n",
      "5e-07\n",
      "Epoch [70/200], Loss: 2.5634, Gap to Optimality: 2.5634, NMSE: 1.8436502102758823e-07, Correlation: 0.9999999080845628, R2: 0.9999998156349887\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.203481674194336\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.430889129638672\n",
      "Gradient Norm_Of_Each_Mini_Batch: 13.457357406616211\n",
      "Gradient Norm_Batch: 14.935761451721191\n",
      "5e-07\n",
      "Epoch [71/200], Loss: 2.5633, Gap to Optimality: 2.5633, NMSE: 1.8414591806958924e-07, Correlation: 0.999999908179728, R2: 0.9999998158540679\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.424745559692383\n",
      "Gradient Norm_Of_Each_Mini_Batch: 25.809974670410156\n",
      "Gradient Norm_Of_Each_Mini_Batch: 12.964251518249512\n",
      "Gradient Norm_Batch: 13.734710693359375\n",
      "5e-07\n",
      "Epoch [72/200], Loss: 2.5632, Gap to Optimality: 2.5632, NMSE: 1.839599548247861e-07, Correlation: 0.9999999082513108, R2: 0.9999998160400588\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.706005096435547\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.580154418945312\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.348559379577637\n",
      "Gradient Norm_Batch: 13.07275676727295\n",
      "5e-07\n",
      "Epoch [73/200], Loss: 2.5632, Gap to Optimality: 2.5632, NMSE: 1.8383508404440363e-07, Correlation: 0.9999999083130245, R2: 0.9999998161649218\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.9066104888916\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.764949798583984\n",
      "Gradient Norm_Of_Each_Mini_Batch: 13.962284088134766\n",
      "Gradient Norm_Batch: 12.953042030334473\n",
      "5e-07\n",
      "Epoch [74/200], Loss: 2.5632, Gap to Optimality: 2.5632, NMSE: 1.837616707689449e-07, Correlation: 0.9999999083666161, R2: 0.999999816238326\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.334640502929688\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.148361206054688\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.554418563842773\n",
      "Gradient Norm_Batch: 12.487099647521973\n",
      "5e-07\n",
      "Epoch [75/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.8368103837929084e-07, Correlation: 0.9999999084053184, R2: 0.9999998163189606\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.336326599121094\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.603191375732422\n",
      "Gradient Norm_Of_Each_Mini_Batch: 12.716251373291016\n",
      "Gradient Norm_Batch: 12.366528511047363\n",
      "5e-07\n",
      "Epoch [76/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.8363624576522852e-07, Correlation: 0.9999999084388769, R2: 0.9999998163637618\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.781343460083008\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.410632133483887\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.882734298706055\n",
      "Gradient Norm_Batch: 11.499091148376465\n",
      "5e-07\n",
      "Epoch [77/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.8355207487275038e-07, Correlation: 0.9999999084581289, R2: 0.999999816447919\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.404382705688477\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.756121635437012\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.33242416381836\n",
      "Gradient Norm_Batch: 11.882183074951172\n",
      "5e-07\n",
      "Epoch [78/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.8354243991325347e-07, Correlation: 0.9999999084886533, R2: 0.9999998164575811\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 24.681100845336914\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.8049373626709\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.12276840209961\n",
      "Gradient Norm_Batch: 11.701574325561523\n",
      "5e-07\n",
      "Epoch [79/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.8350770858432952e-07, Correlation: 0.9999999085044114, R2: 0.9999998164923112\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.857406616210938\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.540382385253906\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.11932373046875\n",
      "Gradient Norm_Batch: 11.709129333496094\n",
      "5e-07\n",
      "Epoch [80/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.8349500408021413e-07, Correlation: 0.9999999085167381, R2: 0.9999998165049887\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.937870025634766\n",
      "Gradient Norm_Of_Each_Mini_Batch: 24.615726470947266\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.739103317260742\n",
      "Gradient Norm_Batch: 11.187731742858887\n",
      "5e-07\n",
      "Epoch [81/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.8345741636949242e-07, Correlation: 0.9999999085243689, R2: 0.999999816542575\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.454458236694336\n",
      "Gradient Norm_Of_Each_Mini_Batch: 26.577238082885742\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.183544158935547\n",
      "Gradient Norm_Batch: 11.43552303314209\n",
      "5e-07\n",
      "Epoch [82/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.8346092645060708e-07, Correlation: 0.9999999085337983, R2: 0.9999998165390968\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.652544021606445\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.909191131591797\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.834365844726562\n",
      "Gradient Norm_Batch: 12.03015422821045\n",
      "5e-07\n",
      "Epoch [83/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.834855396509738e-07, Correlation: 0.9999999085455387, R2: 0.9999998165144421\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.049482345581055\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.92057991027832\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.208370208740234\n",
      "Gradient Norm_Batch: 11.155399322509766\n",
      "5e-07\n",
      "Epoch [84/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8342640828450385e-07, Correlation: 0.9999999085485669, R2: 0.999999816573599\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.504572868347168\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.77095603942871\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.828944206237793\n",
      "Gradient Norm_Batch: 11.17680835723877\n",
      "5e-07\n",
      "Epoch [85/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.834150396007317e-07, Correlation: 0.9999999085561705, R2: 0.9999998165849519\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 31.67334747314453\n",
      "Gradient Norm_Of_Each_Mini_Batch: 29.057382583618164\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.43512535095215\n",
      "Gradient Norm_Batch: 12.57561206817627\n",
      "5e-07\n",
      "Epoch [86/200], Loss: 2.5631, Gap to Optimality: 2.5631, NMSE: 1.834982583659439e-07, Correlation: 0.9999999085629537, R2: 0.9999998165017301\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.446401596069336\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.86240577697754\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.471782684326172\n",
      "Gradient Norm_Batch: 11.1749267578125\n",
      "5e-07\n",
      "Epoch [87/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.83408687348674e-07, Correlation: 0.9999999085618302, R2: 0.9999998165913196\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.501710891723633\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.959440231323242\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.564074516296387\n",
      "Gradient Norm_Batch: 10.166741371154785\n",
      "5e-07\n",
      "Epoch [88/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833502665249398e-07, Correlation: 0.9999999085609246, R2: 0.999999816649736\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.057300567626953\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.048322677612305\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.942975044250488\n",
      "Gradient Norm_Batch: 10.37201976776123\n",
      "5e-07\n",
      "Epoch [89/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833603704426423e-07, Correlation: 0.9999999085623393, R2: 0.9999998166396235\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.396068572998047\n",
      "Gradient Norm_Of_Each_Mini_Batch: 28.5994930267334\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.195383071899414\n",
      "Gradient Norm_Batch: 10.11468505859375\n",
      "5e-07\n",
      "Epoch [90/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8334843332468154e-07, Correlation: 0.9999999085600826, R2: 0.9999998166515564\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.908063888549805\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.063648223876953\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.53311538696289\n",
      "Gradient Norm_Batch: 9.849434852600098\n",
      "5e-07\n",
      "Epoch [91/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8333048501517624e-07, Correlation: 0.9999999085606848, R2: 0.9999998166695151\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.747608184814453\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.46364402770996\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.938148498535156\n",
      "Gradient Norm_Batch: 10.522379875183105\n",
      "5e-07\n",
      "Epoch [92/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8336285734221747e-07, Correlation: 0.999999908567006, R2: 0.999999816637118\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.310514450073242\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.035879135131836\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.63395118713379\n",
      "Gradient Norm_Batch: 10.068342208862305\n",
      "5e-07\n",
      "Epoch [93/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8334557694288378e-07, Correlation: 0.9999999085614822, R2: 0.9999998166544302\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.22546672821045\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.8417911529541\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.862716674804688\n",
      "Gradient Norm_Batch: 10.170166969299316\n",
      "5e-07\n",
      "Epoch [94/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8334618800963653e-07, Correlation: 0.9999999085640728, R2: 0.9999998166538083\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.144989013671875\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.8250789642334\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.54302978515625\n",
      "Gradient Norm_Batch: 11.176568031311035\n",
      "5e-07\n",
      "Epoch [95/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8341098950713786e-07, Correlation: 0.9999999085663972, R2: 0.9999998165890182\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.735200881958008\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.170312881469727\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.622669219970703\n",
      "Gradient Norm_Batch: 11.32651424407959\n",
      "5e-07\n",
      "Epoch [96/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8341232532748108e-07, Correlation: 0.999999908571756, R2: 0.9999998165876666\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 12.515901565551758\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.713712692260742\n",
      "Gradient Norm_Of_Each_Mini_Batch: 11.523319244384766\n",
      "Gradient Norm_Batch: 11.386195182800293\n",
      "5e-07\n",
      "Epoch [97/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.834235519027061e-07, Correlation: 0.9999999085684711, R2: 0.9999998165764515\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.0522403717041\n",
      "Gradient Norm_Of_Each_Mini_Batch: 24.95078468322754\n",
      "Gradient Norm_Of_Each_Mini_Batch: 11.657686233520508\n",
      "Gradient Norm_Batch: 10.593216896057129\n",
      "5e-07\n",
      "Epoch [98/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8336845641897526e-07, Correlation: 0.9999999085690701, R2: 0.9999998166315545\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 26.731887817382812\n",
      "Gradient Norm_Of_Each_Mini_Batch: 24.041624069213867\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.92388153076172\n",
      "Gradient Norm_Batch: 10.201864242553711\n",
      "5e-07\n",
      "Epoch [99/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8333842888296203e-07, Correlation: 0.9999999085664131, R2: 0.9999998166615759\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.64583396911621\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.394899368286133\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.636741638183594\n",
      "Gradient Norm_Batch: 10.3494873046875\n",
      "5e-07\n",
      "Epoch [100/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8335940410452167e-07, Correlation: 0.999999908566405, R2: 0.9999998166406051\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.09415626525879\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.798479080200195\n",
      "Gradient Norm_Of_Each_Mini_Batch: 12.040223121643066\n",
      "Gradient Norm_Batch: 10.762713432312012\n",
      "5e-07\n",
      "Epoch [101/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8336707796606788e-07, Correlation: 0.9999999085747844, R2: 0.9999998166329329\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 13.611930847167969\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.218326568603516\n",
      "Gradient Norm_Of_Each_Mini_Batch: 12.672491073608398\n",
      "Gradient Norm_Batch: 10.593214988708496\n",
      "5e-07\n",
      "Epoch [102/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8335673246383521e-07, Correlation: 0.9999999085748235, R2: 0.999999816643263\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.878192901611328\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.028282165527344\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.721271514892578\n",
      "Gradient Norm_Batch: 9.48659896850586\n",
      "5e-07\n",
      "Epoch [103/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833084724012224e-07, Correlation: 0.9999999085647285, R2: 0.9999998166915337\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.850231170654297\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.18042755126953\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.771121978759766\n",
      "Gradient Norm_Batch: 10.142152786254883\n",
      "5e-07\n",
      "Epoch [104/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833391394256978e-07, Correlation: 0.9999999085687584, R2: 0.9999998166608426\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.880125045776367\n",
      "Gradient Norm_Of_Each_Mini_Batch: 24.567588806152344\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.460676193237305\n",
      "Gradient Norm_Batch: 10.115070343017578\n",
      "5e-07\n",
      "Epoch [105/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833375620208244e-07, Correlation: 0.9999999085673548, R2: 0.9999998166624191\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.107677459716797\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.325843811035156\n",
      "Gradient Norm_Of_Each_Mini_Batch: 11.842452049255371\n",
      "Gradient Norm_Batch: 10.168010711669922\n",
      "5e-07\n",
      "Epoch [106/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8334291951305204e-07, Correlation: 0.9999999085666912, R2: 0.9999998166570755\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.358362197875977\n",
      "Gradient Norm_Of_Each_Mini_Batch: 25.967700958251953\n",
      "Gradient Norm_Of_Each_Mini_Batch: 13.886378288269043\n",
      "Gradient Norm_Batch: 10.191862106323242\n",
      "5e-07\n",
      "Epoch [107/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8334769436023635e-07, Correlation: 0.999999908567356, R2: 0.9999998166523055\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.079416275024414\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.26191520690918\n",
      "Gradient Norm_Of_Each_Mini_Batch: 12.75973892211914\n",
      "Gradient Norm_Batch: 10.283378601074219\n",
      "5e-07\n",
      "Epoch [108/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8334732487801375e-07, Correlation: 0.9999999085701353, R2: 0.9999998166526791\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.133586883544922\n",
      "Gradient Norm_Of_Each_Mini_Batch: 25.282461166381836\n",
      "Gradient Norm_Of_Each_Mini_Batch: 10.685559272766113\n",
      "Gradient Norm_Batch: 10.688082695007324\n",
      "5e-07\n",
      "Epoch [109/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8337395601975004e-07, Correlation: 0.9999999085703223, R2: 0.9999998166260368\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.613628387451172\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.164901733398438\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.963587760925293\n",
      "Gradient Norm_Batch: 10.853837966918945\n",
      "5e-07\n",
      "Epoch [110/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833750360447084e-07, Correlation: 0.9999999085739347, R2: 0.9999998166249586\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.404878616333008\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.0888729095459\n",
      "Gradient Norm_Of_Each_Mini_Batch: 11.466720581054688\n",
      "Gradient Norm_Batch: 10.585495948791504\n",
      "5e-07\n",
      "Epoch [111/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8336855589495826e-07, Correlation: 0.9999999085670983, R2: 0.999999816631458\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.496099472045898\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.97992706298828\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.97482681274414\n",
      "Gradient Norm_Batch: 10.041109085083008\n",
      "5e-07\n",
      "Epoch [112/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833367093695415e-07, Correlation: 0.9999999085675924, R2: 0.9999998166632904\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 27.160173416137695\n",
      "Gradient Norm_Of_Each_Mini_Batch: 29.789630889892578\n",
      "Gradient Norm_Of_Each_Mini_Batch: 24.363773345947266\n",
      "Gradient Norm_Batch: 9.703337669372559\n",
      "5e-07\n",
      "Epoch [113/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8331620310618746e-07, Correlation: 0.9999999085647813, R2: 0.9999998166837829\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.8195743560791\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.318246841430664\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.917207717895508\n",
      "Gradient Norm_Batch: 9.93225383758545\n",
      "5e-07\n",
      "Epoch [114/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8332356432892993e-07, Correlation: 0.9999999085657079, R2: 0.9999998166764394\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.436269760131836\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.306520462036133\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.66084861755371\n",
      "Gradient Norm_Batch: 10.491279602050781\n",
      "5e-07\n",
      "Epoch [115/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833614788893101e-07, Correlation: 0.9999999085599325, R2: 0.9999998166385169\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.869552612304688\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.742359161376953\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.817716598510742\n",
      "Gradient Norm_Batch: 10.266807556152344\n",
      "5e-07\n",
      "Epoch [116/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8335575191485987e-07, Correlation: 0.9999999085650674, R2: 0.9999998166442364\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.503939628601074\n",
      "Gradient Norm_Of_Each_Mini_Batch: 11.988845825195312\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.711206436157227\n",
      "Gradient Norm_Batch: 9.964556694030762\n",
      "5e-07\n",
      "Epoch [117/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8332924867081601e-07, Correlation: 0.9999999085689859, R2: 0.9999998166707661\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.271726608276367\n",
      "Gradient Norm_Of_Each_Mini_Batch: 26.995250701904297\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.94046974182129\n",
      "Gradient Norm_Batch: 9.902898788452148\n",
      "5e-07\n",
      "Epoch [118/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833242890825204e-07, Correlation: 0.9999999085701928, R2: 0.9999998166757078\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.799884796142578\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.3811092376709\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.883455276489258\n",
      "Gradient Norm_Batch: 10.590572357177734\n",
      "5e-07\n",
      "Epoch [119/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8335462925733736e-07, Correlation: 0.9999999085756067, R2: 0.9999998166453599\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.834331512451172\n",
      "Gradient Norm_Of_Each_Mini_Batch: 24.506629943847656\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.480642318725586\n",
      "Gradient Norm_Batch: 10.34101390838623\n",
      "5e-07\n",
      "Epoch [120/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833432463627105e-07, Correlation: 0.9999999085722314, R2: 0.9999998166567515\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.387170791625977\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.55706024169922\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.30294418334961\n",
      "Gradient Norm_Batch: 10.653796195983887\n",
      "5e-07\n",
      "Epoch [121/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8336365315008152e-07, Correlation: 0.9999999085724751, R2: 0.9999998166363243\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.074649810791016\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.593061447143555\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.893160820007324\n",
      "Gradient Norm_Batch: 9.7377347946167\n",
      "5e-07\n",
      "Epoch [122/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833132472484067e-07, Correlation: 0.9999999085698492, R2: 0.9999998166867484\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.20769691467285\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.19094467163086\n",
      "Gradient Norm_Of_Each_Mini_Batch: 10.148289680480957\n",
      "Gradient Norm_Batch: 9.860164642333984\n",
      "5e-07\n",
      "Epoch [123/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8331685680550436e-07, Correlation: 0.9999999085725131, R2: 0.9999998166831409\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.673471450805664\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.71050262451172\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.422588348388672\n",
      "Gradient Norm_Batch: 10.4635648727417\n",
      "5e-07\n",
      "Epoch [124/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8335059337459825e-07, Correlation: 0.9999999085744073, R2: 0.999999816649396\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.37108612060547\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.392309188842773\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.715048789978027\n",
      "Gradient Norm_Batch: 10.155939102172852\n",
      "5e-07\n",
      "Epoch [125/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8333594198338687e-07, Correlation: 0.9999999085715114, R2: 0.9999998166640587\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.592073440551758\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.679157257080078\n",
      "Gradient Norm_Of_Each_Mini_Batch: 12.235152244567871\n",
      "Gradient Norm_Batch: 10.059063911437988\n",
      "5e-07\n",
      "Epoch [126/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8333093976252712e-07, Correlation: 0.9999999085715621, R2: 0.9999998166690713\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.499155044555664\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.920963287353516\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.89265251159668\n",
      "Gradient Norm_Batch: 10.063874244689941\n",
      "5e-07\n",
      "Epoch [127/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8332733020542946e-07, Correlation: 0.9999999085739422, R2: 0.9999998166726741\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.664087295532227\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.3893985748291\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.525554656982422\n",
      "Gradient Norm_Batch: 10.287351608276367\n",
      "5e-07\n",
      "Epoch [128/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833453211474989e-07, Correlation: 0.9999999085719267, R2: 0.9999998166546683\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.999204635620117\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.51962661743164\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.914756774902344\n",
      "Gradient Norm_Batch: 9.7688570022583\n",
      "5e-07\n",
      "Epoch [129/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8331752471567597e-07, Correlation: 0.9999999085682699, R2: 0.9999998166824704\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 24.770851135253906\n",
      "Gradient Norm_Of_Each_Mini_Batch: 29.044971466064453\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.601675987243652\n",
      "Gradient Norm_Batch: 9.727602005004883\n",
      "5e-07\n",
      "Epoch [130/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8332264062337345e-07, Correlation: 0.9999999085635503, R2: 0.9999998166773508\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.024742126464844\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.86861228942871\n",
      "Gradient Norm_Of_Each_Mini_Batch: 11.385128021240234\n",
      "Gradient Norm_Batch: 10.069250106811523\n",
      "5e-07\n",
      "Epoch [131/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8333987839014299e-07, Correlation: 0.9999999085665113, R2: 0.9999998166601219\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.96173858642578\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.28066635131836\n",
      "Gradient Norm_Of_Each_Mini_Batch: 13.928314208984375\n",
      "Gradient Norm_Batch: 10.270678520202637\n",
      "5e-07\n",
      "Epoch [132/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8335450135964493e-07, Correlation: 0.9999999085653116, R2: 0.9999998166454949\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.685678482055664\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.705198287963867\n",
      "Gradient Norm_Of_Each_Mini_Batch: 13.515216827392578\n",
      "Gradient Norm_Batch: 9.719623565673828\n",
      "5e-07\n",
      "Epoch [133/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8332247009311686e-07, Correlation: 0.9999999085616538, R2: 0.9999998166775339\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.360645294189453\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.840025901794434\n",
      "Gradient Norm_Of_Each_Mini_Batch: 12.592808723449707\n",
      "Gradient Norm_Batch: 9.85269546508789\n",
      "5e-07\n",
      "Epoch [134/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833281260132935e-07, Correlation: 0.9999999085659184, R2: 0.9999998166718846\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.495988845825195\n",
      "Gradient Norm_Of_Each_Mini_Batch: 27.068143844604492\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.95439910888672\n",
      "Gradient Norm_Batch: 9.607291221618652\n",
      "5e-07\n",
      "Epoch [135/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833204379408926e-07, Correlation: 0.9999999085605094, R2: 0.9999998166795614\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.204146385192871\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.08951759338379\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.71347999572754\n",
      "Gradient Norm_Batch: 9.899331092834473\n",
      "5e-07\n",
      "Epoch [136/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8333655305013963e-07, Correlation: 0.999999908563929, R2: 0.9999998166634518\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.836090087890625\n",
      "Gradient Norm_Of_Each_Mini_Batch: 10.750421524047852\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.80548667907715\n",
      "Gradient Norm_Batch: 10.747668266296387\n",
      "5e-07\n",
      "Epoch [137/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8337759399855713e-07, Correlation: 0.9999999085711174, R2: 0.9999998166224139\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 25.714298248291016\n",
      "Gradient Norm_Of_Each_Mini_Batch: 26.34252166748047\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.90553092956543\n",
      "Gradient Norm_Batch: 10.410499572753906\n",
      "5e-07\n",
      "Epoch [138/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833486749092117e-07, Correlation: 0.9999999085711871, R2: 0.9999998166513379\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.354429244995117\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.068098068237305\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.88181495666504\n",
      "Gradient Norm_Batch: 10.763833045959473\n",
      "5e-07\n",
      "Epoch [139/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8337021856495994e-07, Correlation: 0.9999999085731011, R2: 0.9999998166297768\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.820146560668945\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.059592247009277\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.649425506591797\n",
      "Gradient Norm_Batch: 9.775527954101562\n",
      "5e-07\n",
      "Epoch [140/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833213900681585e-07, Correlation: 0.9999999085660618, R2: 0.9999998166786246\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.682109832763672\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.85805892944336\n",
      "Gradient Norm_Of_Each_Mini_Batch: 13.293474197387695\n",
      "Gradient Norm_Batch: 10.42176342010498\n",
      "5e-07\n",
      "Epoch [141/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8335703089178423e-07, Correlation: 0.9999999085705722, R2: 0.9999998166429779\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.621440887451172\n",
      "Gradient Norm_Of_Each_Mini_Batch: 27.78864097595215\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.215898513793945\n",
      "Gradient Norm_Batch: 10.697100639343262\n",
      "5e-07\n",
      "Epoch [142/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8336788798478665e-07, Correlation: 0.9999999085736709, R2: 0.9999998166321128\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.948715209960938\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.891883850097656\n",
      "Gradient Norm_Of_Each_Mini_Batch: 11.506903648376465\n",
      "Gradient Norm_Batch: 10.461833000183105\n",
      "5e-07\n",
      "Epoch [143/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8334601747937995e-07, Correlation: 0.9999999085766377, R2: 0.9999998166539829\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.617815017700195\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.102487564086914\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.004834175109863\n",
      "Gradient Norm_Batch: 10.8794584274292\n",
      "5e-07\n",
      "Epoch [144/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8336527318751905e-07, Correlation: 0.999999908577589, R2: 0.9999998166347079\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.08452606201172\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.290149688720703\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.058395385742188\n",
      "Gradient Norm_Batch: 10.290212631225586\n",
      "5e-07\n",
      "Epoch [145/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833459748468158e-07, Correlation: 0.9999999085709282, R2: 0.9999998166540189\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.387086868286133\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.848907470703125\n",
      "Gradient Norm_Of_Each_Mini_Batch: 11.736919403076172\n",
      "Gradient Norm_Batch: 9.981236457824707\n",
      "5e-07\n",
      "Epoch [146/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8333361140321358e-07, Correlation: 0.9999999085680525, R2: 0.9999998166663887\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 13.053316116333008\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.828863143920898\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.34010887145996\n",
      "Gradient Norm_Batch: 9.821701049804688\n",
      "5e-07\n",
      "Epoch [147/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8332471540816186e-07, Correlation: 0.9999999085667345, R2: 0.999999816675293\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 13.620485305786133\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.20512580871582\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.90582847595215\n",
      "Gradient Norm_Batch: 10.79340934753418\n",
      "5e-07\n",
      "Epoch [148/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8337318863359542e-07, Correlation: 0.9999999085738077, R2: 0.9999998166268038\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.082216262817383\n",
      "Gradient Norm_Of_Each_Mini_Batch: 11.677823066711426\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.3043212890625\n",
      "Gradient Norm_Batch: 10.856178283691406\n",
      "5e-07\n",
      "Epoch [149/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8337939877710596e-07, Correlation: 0.9999999085723275, R2: 0.9999998166206026\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.45756721496582\n",
      "Gradient Norm_Of_Each_Mini_Batch: 28.00795555114746\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.676050186157227\n",
      "Gradient Norm_Batch: 10.444567680358887\n",
      "5e-07\n",
      "Epoch [150/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8336069729230076e-07, Correlation: 0.9999999085683675, R2: 0.9999998166392959\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.752849578857422\n",
      "Gradient Norm_Of_Each_Mini_Batch: 12.890138626098633\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.200328826904297\n",
      "Gradient Norm_Batch: 10.750335693359375\n",
      "5e-07\n",
      "Epoch [151/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8337959772907197e-07, Correlation: 0.9999999085693075, R2: 0.9999998166203977\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.29230308532715\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.159475326538086\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.381336212158203\n",
      "Gradient Norm_Batch: 11.000606536865234\n",
      "5e-07\n",
      "Epoch [152/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8339483176532667e-07, Correlation: 0.9999999085703851, R2: 0.9999998166051792\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.431463241577148\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.258955001831055\n",
      "Gradient Norm_Of_Each_Mini_Batch: 12.368547439575195\n",
      "Gradient Norm_Batch: 10.874594688415527\n",
      "5e-07\n",
      "Epoch [153/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833853247035222e-07, Correlation: 0.9999999085707051, R2: 0.9999998166146726\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 28.113842010498047\n",
      "Gradient Norm_Of_Each_Mini_Batch: 28.463041305541992\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.180061340332031\n",
      "Gradient Norm_Batch: 10.937698364257812\n",
      "5e-07\n",
      "Epoch [154/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8338614893309568e-07, Correlation: 0.9999999085703631, R2: 0.9999998166138548\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.193132400512695\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.809967041015625\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.12481117248535\n",
      "Gradient Norm_Batch: 11.001718521118164\n",
      "5e-07\n",
      "Epoch [155/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8338619156565983e-07, Correlation: 0.9999999085737601, R2: 0.999999816613813\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 12.799762725830078\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.77726936340332\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.118425369262695\n",
      "Gradient Norm_Batch: 10.3175687789917\n",
      "5e-07\n",
      "Epoch [156/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8335013862724736e-07, Correlation: 0.9999999085688974, R2: 0.9999998166498666\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.402624130249023\n",
      "Gradient Norm_Of_Each_Mini_Batch: 10.158160209655762\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.792850494384766\n",
      "Gradient Norm_Batch: 9.769673347473145\n",
      "5e-07\n",
      "Epoch [157/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8332433171508455e-07, Correlation: 0.9999999085627967, R2: 0.9999998166756623\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.146452903747559\n",
      "Gradient Norm_Of_Each_Mini_Batch: 13.952252388000488\n",
      "Gradient Norm_Of_Each_Mini_Batch: 11.345297813415527\n",
      "Gradient Norm_Batch: 9.888214111328125\n",
      "5e-07\n",
      "Epoch [158/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833274581031219e-07, Correlation: 0.999999908565261, R2: 0.9999998166725357\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.712230682373047\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.824604034423828\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.291385650634766\n",
      "Gradient Norm_Batch: 10.475491523742676\n",
      "5e-07\n",
      "Epoch [159/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8335748563913512e-07, Correlation: 0.9999999085696848, R2: 0.9999998166425105\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 13.506889343261719\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.999862670898438\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.081356048583984\n",
      "Gradient Norm_Batch: 10.458259582519531\n",
      "5e-07\n",
      "Epoch [160/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833584093446916e-07, Correlation: 0.9999999085699756, R2: 0.9999998166415851\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.815725326538086\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.891517639160156\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.00946807861328\n",
      "Gradient Norm_Batch: 10.426312446594238\n",
      "5e-07\n",
      "Epoch [161/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8335050810946996e-07, Correlation: 0.9999999085739602, R2: 0.9999998166494778\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.658384323120117\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.50688934326172\n",
      "Gradient Norm_Of_Each_Mini_Batch: 11.783098220825195\n",
      "Gradient Norm_Batch: 10.318277359008789\n",
      "5e-07\n",
      "Epoch [162/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8334564799715736e-07, Correlation: 0.9999999085722044, R2: 0.9999998166543638\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.576332092285156\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.60022735595703\n",
      "Gradient Norm_Of_Each_Mini_Batch: 11.743631362915039\n",
      "Gradient Norm_Batch: 9.735660552978516\n",
      "5e-07\n",
      "Epoch [163/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8330780449105077e-07, Correlation: 0.9999999085733375, R2: 0.9999998166921931\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.333274841308594\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.010339736938477\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.425691604614258\n",
      "Gradient Norm_Batch: 10.358298301696777\n",
      "5e-07\n",
      "Epoch [164/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8334191054236726e-07, Correlation: 0.999999908575584, R2: 0.999999816658079\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 24.12407875061035\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.236385345458984\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.126218795776367\n",
      "Gradient Norm_Batch: 10.447999954223633\n",
      "5e-07\n",
      "Epoch [165/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833457901057045e-07, Correlation: 0.9999999085750313, R2: 0.9999998166542085\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.74072265625\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.945615768432617\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.334091186523438\n",
      "Gradient Norm_Batch: 9.771203994750977\n",
      "5e-07\n",
      "Epoch [166/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8331786577618914e-07, Correlation: 0.9999999085696936, R2: 0.9999998166821241\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 11.776078224182129\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.55222511291504\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.723825454711914\n",
      "Gradient Norm_Batch: 10.228379249572754\n",
      "5e-07\n",
      "Epoch [167/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833425926633936e-07, Correlation: 0.9999999085716612, R2: 0.9999998166574106\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.498294830322266\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.152904510498047\n",
      "Gradient Norm_Of_Each_Mini_Batch: 13.933318138122559\n",
      "Gradient Norm_Batch: 10.259758949279785\n",
      "5e-07\n",
      "Epoch [168/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833409157825372e-07, Correlation: 0.999999908572689, R2: 0.999999816659081\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.177658081054688\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.713016510009766\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.090410232543945\n",
      "Gradient Norm_Batch: 10.790644645690918\n",
      "5e-07\n",
      "Epoch [169/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8337698293180438e-07, Correlation: 0.9999999085717637, R2: 0.9999998166230007\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.587682723999023\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.291961669921875\n",
      "Gradient Norm_Of_Each_Mini_Batch: 13.389272689819336\n",
      "Gradient Norm_Batch: 10.761469841003418\n",
      "5e-07\n",
      "Epoch [170/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833681295693168e-07, Correlation: 0.9999999085722525, R2: 0.9999998166318728\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.346004486083984\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.45378303527832\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.803816795349121\n",
      "Gradient Norm_Batch: 10.4181489944458\n",
      "5e-07\n",
      "Epoch [171/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8334714013690245e-07, Correlation: 0.9999999085715551, R2: 0.9999998166528739\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.803050994873047\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.933950424194336\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.180797576904297\n",
      "Gradient Norm_Batch: 10.282586097717285\n",
      "5e-07\n",
      "Epoch [172/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8333332718611928e-07, Correlation: 0.9999999085742818, R2: 0.9999998166666563\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.496837615966797\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.40496063232422\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.26676368713379\n",
      "Gradient Norm_Batch: 10.353519439697266\n",
      "5e-07\n",
      "Epoch [173/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8333953732962982e-07, Correlation: 0.9999999085712631, R2: 0.9999998166604697\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.53687858581543\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.145675659179688\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.431611061096191\n",
      "Gradient Norm_Batch: 10.402315139770508\n",
      "5e-07\n",
      "Epoch [174/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.83346813287244e-07, Correlation: 0.9999999085693593, R2: 0.9999998166531829\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 23.737768173217773\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.343143463134766\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.768594741821289\n",
      "Gradient Norm_Batch: 10.024334907531738\n",
      "5e-07\n",
      "Epoch [175/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8332542595089762e-07, Correlation: 0.999999908568943, R2: 0.9999998166745725\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.956655502319336\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.642629623413086\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.41167640686035\n",
      "Gradient Norm_Batch: 10.42611312866211\n",
      "5e-07\n",
      "Epoch [176/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833521423577622e-07, Correlation: 0.9999999085721268, R2: 0.9999998166478422\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.636295318603516\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.152137756347656\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.730127334594727\n",
      "Gradient Norm_Batch: 10.50501823425293\n",
      "5e-07\n",
      "Epoch [177/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8335721563289553e-07, Correlation: 0.9999999085726777, R2: 0.9999998166427791\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.707475662231445\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.708221435546875\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.593721389770508\n",
      "Gradient Norm_Batch: 9.733770370483398\n",
      "5e-07\n",
      "Epoch [178/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833193152833701e-07, Correlation: 0.9999999085665959, R2: 0.9999998166806948\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.961488723754883\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.791412353515625\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.081128120422363\n",
      "Gradient Norm_Batch: 10.453567504882812\n",
      "5e-07\n",
      "Epoch [179/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833504654769058e-07, Correlation: 0.9999999085737953, R2: 0.9999998166495245\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.078866004943848\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.352680206298828\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.04415225982666\n",
      "Gradient Norm_Batch: 10.757871627807617\n",
      "5e-07\n",
      "Epoch [180/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8337365759180102e-07, Correlation: 0.9999999085724581, R2: 0.9999998166263342\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.118154525756836\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.404075622558594\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.4431095123291\n",
      "Gradient Norm_Batch: 10.380645751953125\n",
      "5e-07\n",
      "Epoch [181/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8335927620682924e-07, Correlation: 0.9999999085675351, R2: 0.9999998166407149\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.48823356628418\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.02468490600586\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.642960548400879\n",
      "Gradient Norm_Batch: 10.223738670349121\n",
      "5e-07\n",
      "Epoch [182/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8334669960040628e-07, Correlation: 0.9999999085681122, R2: 0.9999998166532827\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.949146270751953\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.490720748901367\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.241790771484375\n",
      "Gradient Norm_Batch: 10.856785774230957\n",
      "5e-07\n",
      "Epoch [183/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8338674578899372e-07, Correlation: 0.999999908568159, R2: 0.9999998166132424\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.459491729736328\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.421852111816406\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.257455825805664\n",
      "Gradient Norm_Batch: 10.948542594909668\n",
      "5e-07\n",
      "Epoch [184/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8339521545840398e-07, Correlation: 0.9999999085670789, R2: 0.9999998166047926\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.57809829711914\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.754024505615234\n",
      "Gradient Norm_Of_Each_Mini_Batch: 9.345230102539062\n",
      "Gradient Norm_Batch: 11.252224922180176\n",
      "5e-07\n",
      "Epoch [185/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8340938368055504e-07, Correlation: 0.9999999085716369, R2: 0.9999998165905979\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.202991485595703\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.894026756286621\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.430871963500977\n",
      "Gradient Norm_Batch: 11.600947380065918\n",
      "5e-07\n",
      "Epoch [186/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8342446139740787e-07, Correlation: 0.9999999085725915, R2: 0.9999998165755194\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.483312606811523\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.773529052734375\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.614179611206055\n",
      "Gradient Norm_Batch: 11.049660682678223\n",
      "5e-07\n",
      "Epoch [187/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8338646157189942e-07, Correlation: 0.9999999085745012, R2: 0.9999998166135395\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.11587142944336\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.287212371826172\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.837867736816406\n",
      "Gradient Norm_Batch: 10.706281661987305\n",
      "5e-07\n",
      "Epoch [188/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8336986329359206e-07, Correlation: 0.9999999085718109, R2: 0.9999998166301253\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.525009155273438\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.472627639770508\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.724024772644043\n",
      "Gradient Norm_Batch: 10.119292259216309\n",
      "5e-07\n",
      "Epoch [189/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8334003470954485e-07, Correlation: 0.9999999085675101, R2: 0.9999998166599722\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 17.876577377319336\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.7565975189209\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.269487380981445\n",
      "Gradient Norm_Batch: 11.015050888061523\n",
      "5e-07\n",
      "Epoch [190/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.833913074733573e-07, Correlation: 0.9999999085710317, R2: 0.9999998166086892\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.919178009033203\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.823299407958984\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.64234733581543\n",
      "Gradient Norm_Batch: 10.646708488464355\n",
      "5e-07\n",
      "Epoch [191/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8337134122248244e-07, Correlation: 0.9999999085701213, R2: 0.9999998166286518\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.72130584716797\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.53087615966797\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.730229377746582\n",
      "Gradient Norm_Batch: 10.63520622253418\n",
      "5e-07\n",
      "Epoch [192/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8337236440402194e-07, Correlation: 0.9999999085678078, R2: 0.9999998166276409\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 13.747425079345703\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.123130798339844\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.985523223876953\n",
      "Gradient Norm_Batch: 10.569575309753418\n",
      "5e-07\n",
      "Epoch [193/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8336837115384697e-07, Correlation: 0.9999999085684751, R2: 0.9999998166316434\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.052289962768555\n",
      "Gradient Norm_Of_Each_Mini_Batch: 20.70667266845703\n",
      "Gradient Norm_Of_Each_Mini_Batch: 10.953473091125488\n",
      "Gradient Norm_Batch: 10.288521766662598\n",
      "5e-07\n",
      "Epoch [194/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8335417450998648e-07, Correlation: 0.9999999085666141, R2: 0.9999998166458142\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 12.396733283996582\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.800124168395996\n",
      "Gradient Norm_Of_Each_Mini_Batch: 12.971301078796387\n",
      "Gradient Norm_Batch: 10.208934783935547\n",
      "5e-07\n",
      "Epoch [195/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8334769436023635e-07, Correlation: 0.9999999085670097, R2: 0.9999998166523199\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 15.033376693725586\n",
      "Gradient Norm_Of_Each_Mini_Batch: 14.4700345993042\n",
      "Gradient Norm_Of_Each_Mini_Batch: 12.619415283203125\n",
      "Gradient Norm_Batch: 10.462800025939941\n",
      "5e-07\n",
      "Epoch [196/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8336562845888693e-07, Correlation: 0.9999999085664848, R2: 0.999999816634364\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.497068405151367\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.156845092773438\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.18751335144043\n",
      "Gradient Norm_Batch: 10.700711250305176\n",
      "5e-07\n",
      "Epoch [197/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.83375604478897e-07, Correlation: 0.9999999085684435, R2: 0.9999998166244028\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 21.738122940063477\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.36216163635254\n",
      "Gradient Norm_Of_Each_Mini_Batch: 18.0436954498291\n",
      "Gradient Norm_Batch: 11.191977500915527\n",
      "5e-07\n",
      "Epoch [198/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8341221164064336e-07, Correlation: 0.9999999085656249, R2: 0.9999998165877891\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.98282241821289\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.39798927307129\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.662899017333984\n",
      "Gradient Norm_Batch: 11.321544647216797\n",
      "5e-07\n",
      "Epoch [199/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8341772545227286e-07, Correlation: 0.9999999085620244, R2: 0.9999998165822833\n",
      "256\n",
      "Gradient Norm_Of_Each_Mini_Batch: 19.952871322631836\n",
      "Gradient Norm_Of_Each_Mini_Batch: 22.971542358398438\n",
      "Gradient Norm_Of_Each_Mini_Batch: 16.966737747192383\n",
      "Gradient Norm_Batch: 11.040863037109375\n",
      "5e-07\n",
      "Epoch [200/200], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.8340558938234608e-07, Correlation: 0.9999999085626934, R2: 0.9999998165944217\n",
      "Final gradient of the subproblem Core : 11.040863037109375\n",
      "(32, 32)\n",
      "---------------------------Testing with Best Lambda------------------------------\n",
      "Y Test Predicted:  [ 67.54939938  64.18962924 100.90628986 106.79435596  66.52892515\n",
      "  96.79613506 112.76391867  87.08129328  58.88510877  60.74050324\n",
      "  65.75358548  64.15717416  82.80161674  68.56408112 142.4587587\n",
      "  98.01992366  98.35294349  55.41634971  96.74718732  87.26446024\n",
      "  47.97821193  91.1372831   73.56516284  77.46463462  46.96439354\n",
      "  79.38016217  84.71051278  45.49464971  87.56685193  73.47972544\n",
      "  86.90815103  88.26716265 124.65323654  81.42691374  87.54743982\n",
      "  82.7981815   98.73491083  69.03780612  80.63831638  93.08352023\n",
      "  68.4099123   87.28010953  62.04202692  57.34037371  56.02116969\n",
      "  89.30939576 110.54329008 134.61091227  43.26809932  74.48155995\n",
      "  96.23300332  87.89120531  79.23392242  90.72719709 125.88509926\n",
      " 160.37013322  60.51190493  86.8882539   89.57416472  85.49698669\n",
      "  79.97210112  95.54232588  57.09918337  50.36941448  70.86130128\n",
      " 131.23396815  94.51150797  83.16022638  62.92665567  84.10782245\n",
      "  76.25922921  96.48128714  77.08422335  65.79259682  71.7793311\n",
      "  65.14985132  61.02643921  68.75457416  66.41030302 104.95702947\n",
      "  79.7405432   91.80695418 110.44688696  62.24808958  57.61574472\n",
      "  98.12928389  67.83596853  72.12858318 127.85058816  63.30881231\n",
      " 103.69744106 150.07982158  67.4198303   92.38854535 123.67886478\n",
      "  46.91113875  64.66066445 108.22051921 101.13982425 113.37855847]\n",
      "Y Test Actual:  [ 67.54932471  64.19367989 100.89907103 106.80033802  66.5296898\n",
      "  96.80085769 112.75716496  87.08379176  58.86645104  60.75871896\n",
      "  65.76651181  64.14603363  82.80477245  68.55912818 142.45923938\n",
      "  98.00053671  98.36489423  55.40988723  96.73410604  87.25275894\n",
      "  47.97392502  91.14302569  73.57729565  77.46144622  46.97758628\n",
      "  79.38435102  84.7040345   45.4879342   87.55672081  73.49886432\n",
      "  86.90598491  88.25175908 124.63253198  81.43265772  87.55287141\n",
      "  82.8080788   98.72768177  69.0468243   80.62856336  93.08545506\n",
      "  68.40807412  87.26847367  62.04717769  57.33990441  56.01334774\n",
      "  89.31934273 110.55219975 134.61307846  43.26935217  74.47617341\n",
      "  96.21843559  87.89937403  79.21862917  90.71786386 125.8824493\n",
      " 160.38439066  60.50387741  86.87508783  89.5739495   85.50880057\n",
      "  79.97185552  95.54965657  57.09355445  50.35928763  70.86185745\n",
      " 131.24078613  94.50837386  83.15863626  62.93422433  84.09831139\n",
      "  76.26182003  96.48126457  77.07300387  65.77838739  71.76861932\n",
      "  65.14937109  61.0440168   68.74400314  66.41967439 104.95340089\n",
      "  79.75508697  91.80406117 110.44770514  62.24491669  57.60974417\n",
      "  98.13365195  67.82489044  72.14320497 127.85744229  63.30804812\n",
      " 103.70015697 150.07489827  67.42743244  92.3906977  123.66379363\n",
      "  46.92967844  64.66629945 108.22935093 101.14612067 113.38031778]\n",
      "Error Report on Testing _ With best Lambda\n",
      "SGD Learning Rate: 5e-07\n",
      "SGD_Alpha chosen for model:  2.5\n",
      "SGD_Test Normalized Estimation Error:  8.64643786420714e-09\n",
      "SGD_Test NMSE Loss:  1.1263053117928117e-08\n",
      "SGD_Test R2 Loss:  0.999999843844674\n",
      "SGD_Test Correlation:  0.9999999221027122\n",
      "Objective Function Values 1110.0198548006592\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final gradient: 11.040863037109375\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learning_rates = [0.000001,0.0000009,0.0000008,0.0000007,0.0000006,0.0000005]\n",
    "\n",
    "\n",
    "for idx,lr in enumerate (learning_rates):   \n",
    "      \n",
    "    try:\n",
    "      print('')\n",
    "      print('-------------------------------------Learning Rate',lr,'-----------------------------------------')\n",
    "      lsr_tensor_SGD = copy.deepcopy(lsr_tensor_initializer)\n",
    "      learning_rate = lr\n",
    "      epochs = 200\n",
    "      batch_size = 256\n",
    "\n",
    "      momentum = 0\n",
    "      nesterov = False\n",
    "      decay_factor = 0\n",
    "      hypers = {'max_iter': 1, 'threshold': 1e-4, 'ranks': tuple(tensor_mode_ranks), 'separation_rank': separation_rank,'learning_rate':learning_rate,'epochs':epochs,'batch_size': batch_size, 'momentum':momentum, 'nesterov': nesterov, 'decay_factor': decay_factor}\n",
    "\n",
    "      normalized_estimation_error_SGD, test_nmse_loss_SGD, test_R2_loss_SGD, test_correlation_SGD, objective_function_values_SGD,gradient_values_SGD,iterate_differences_SGD,epoch_level_gradients_SGD,epoch_level_function,tensor_iteration_SGD,factor_core_iterate_SGD = train_test_sgd(X_train, Y_train, X_test, Y_test, lambda1, hypers, Y_train_mean,lsr_tensor_SGD,B_tensored,intercept= False)\n",
    "\n",
    "    \n",
    "      #Get current time and store in variable\n",
    "      formatted_time = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "      max_iter = hypers['max_iter']\n",
    "      \n",
    "      \n",
    "      if platform.system() ==  'Windows':\n",
    "        formatted_time = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "        tensor_dimensions_str = \"_\".join(map(str, tensor_dimensions))\n",
    "        tensor_mode_ranks_str = \"_\".join(map(str, tensor_mode_ranks))\n",
    "        pkl_file = rf\"D:\\Tensor Based ML for Neuro Imaging\\INSPIRE_CAHBHIR\\Python Scripts\\LSR-Tensor-Ridge-Regression_All_Data\\Platform_For_Experimenmts\\With_New_Dataset\\Core_Tensor_Isolated_Experiments\\SGD_learning_rate_{learning_rate}_batch_size_{batch_size}_decay_{decay_factor}_intercept5_ExecutionTime{formatted_time}_n_train_{n_train}_n_test_{n_test}_tensor_dimension_{tensor_dimensions}_tensor_mode_ranks_{tensor_mode_ranks}_separation_rank_{separation_rank}_max_iter_{max_iter}.pkl\" \n",
    "      elif platform.system() == 'Darwin': \n",
    "        pkl_file = f\"/Users/lakrama/Neuro Project Codes/LSR-Tensor-Ridge-Regression_All_Data/Platforms for Experments/Data/Experiments_Isolating_Sub_Problems/Final lr search/After Indentifinig Batch Size Issue/Gradient Descent/SGD_learning_rate_{learning_rate}_batch_size_{batch_size}_decay_{decay_factor}_intercept5_,ExecutionTime{formatted_time}, n_train_{n_train},n_test_{n_test}, tensor_dimensions:{tensor_dimensions}, tensor_mode_= ranks:{tensor_mode_ranks}, separation_rank:{separation_rank}, max_iter={max_iter}.pkl\"\n",
    "\n",
    "      #with open(pkl_file, \"wb\") as file:\n",
    "        #dill.dump((lsr_tensor_SGD,lambda1, normalized_estimation_error_SGD, test_nmse_loss_SGD, test_R2_loss_SGD, test_correlation_SGD, objective_function_values_SGD,gradient_values_SGD, iterate_differences_SGD,epoch_level_gradients_SGD,epoch_level_function,tensor_iteration_SGD,factor_core_iterate_SGD), file)\n",
    "\n",
    "\n",
    "      print(\"Error Report on Testing _ With best Lambda\")\n",
    "      print(\"SGD Learning Rate:\", learning_rate)\n",
    "      print(\"SGD_Alpha chosen for model: \", lambda1)\n",
    "      print(\"SGD_Test Normalized Estimation Error: \", normalized_estimation_error_SGD)\n",
    "      print(\"SGD_Test NMSE Loss: \", test_nmse_loss_SGD)\n",
    "      print(\"SGD_Test R2 Loss: \", test_R2_loss_SGD)\n",
    "      print(\"SGD_Test Correlation: \", test_correlation_SGD)\n",
    "      print(\"Objective Function Values\", objective_function_values_SGD[0,1,2])\n",
    "\n",
    "      # Looking at the  variation of function values within a BCD iteration\n",
    "\n",
    "      import matplotlib.pyplot as plt\n",
    "\n",
    "      # Create a line plot\n",
    "      plt.plot(epoch_level_gradients_SGD[0,1,2,:], marker='o')\n",
    "\n",
    "      # Add title and labels\n",
    "      plt.title('Gradient Value')\n",
    "      plt.xlabel('X-axis')\n",
    "      plt.yscale('log')\n",
    "      plt.ylabel('Y-axis')\n",
    "\n",
    "      plt.show()\n",
    "\n",
    "      print(f'final gradient: {epoch_level_gradients_SGD[0,1,2,-1]}')\n",
    "      # fucnion value\n",
    "\n",
    "      # Create a line plot\n",
    "      plt.plot(epoch_level_function[0,1,2,:], marker='o')\n",
    "\n",
    "      # Add title and labels\n",
    "      plt.title('Function Value')\n",
    "      plt.xlabel('X-axis')\n",
    "      plt.yscale('log') \n",
    "      plt.ylabel('Y-axis')\n",
    "\n",
    "      plt.show()\n",
    "\n",
    "    except Exception as e:\n",
    "      print(f\"An error occurred for learning rate {lr}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------------------Learning Rate 1e-06 -----------------------------------------\n",
      "Objective Function Value: 1110.019831221592\n",
      "--------------------------------------------------------------BCD iteration 0 --------------------------------------------------------------\n",
      "---------------------------------------------Sep 0 Factor 0 -------------------------------------------------\n",
      "Iteration: 0, Separation Rank: 0, Factor Matrix: 0, Objective Function Value: 1110.019831221592\n",
      "---------------------------------------------Sep 0 Factor 1 -------------------------------------------------\n",
      "Iteration: 0, Separation Rank: 0, Factor Matrix: 1, Objective Function Value: 1110.019831221592\n",
      "---------------------------------------------Sep 1 Factor 0 -------------------------------------------------\n",
      "Iteration: 0, Separation Rank: 1, Factor Matrix: 0, Objective Function Value: 1110.019831221592\n",
      "---------------------------------------------Sep 1 Factor 1 -------------------------------------------------\n",
      "Iteration: 0, Separation Rank: 1, Factor Matrix: 1, Objective Function Value: 1110.019831221592\n",
      "---------------------------------------------Core-------------------------------------------------\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 9.50236988067627\n",
      "Gradient Norm_Batch: 3.564272165298462\n",
      "Epoch [1/500], Loss: 2.5630, Gap to Optimality: 2.5630, NMSE: 1.829952225307352e-07, Correlation: 0.9999999085699717, R2: 0.9999998170047794\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 3.5614025592803955\n",
      "Gradient Norm_Batch: 1.933480143547058\n",
      "Epoch [2/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8292368508809886e-07, Correlation: 0.9999999085797779, R2: 0.9999998170763215\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 1.932608962059021\n",
      "Gradient Norm_Batch: 1.1938570737838745\n",
      "Epoch [3/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8289360070866678e-07, Correlation: 0.9999999085868618, R2: 0.9999998171064062\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 1.1957570314407349\n",
      "Gradient Norm_Batch: 0.7732241153717041\n",
      "Epoch [4/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8287965986019117e-07, Correlation: 0.9999999085902636, R2: 0.9999998171203373\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.7720290422439575\n",
      "Gradient Norm_Batch: 0.5205817818641663\n",
      "Epoch [5/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828741744702711e-07, Correlation: 0.9999999085915413, R2: 0.9999998171258201\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.5198414921760559\n",
      "Gradient Norm_Batch: 0.35985857248306274\n",
      "Epoch [6/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8287288128249202e-07, Correlation: 0.9999999085913446, R2: 0.9999998171271225\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.3593295216560364\n",
      "Gradient Norm_Batch: 0.2447555512189865\n",
      "Epoch [7/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286611691564758e-07, Correlation: 0.9999999085941118, R2: 0.9999998171338719\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.24370785057544708\n",
      "Gradient Norm_Batch: 0.1716136336326599\n",
      "Epoch [8/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286428371538932e-07, Correlation: 0.9999999085947587, R2: 0.9999998171357156\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.16973161697387695\n",
      "Gradient Norm_Batch: 0.12362036854028702\n",
      "Epoch [9/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828629621059008e-07, Correlation: 0.9999999085952819, R2: 0.9999998171370144\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.12188378721475601\n",
      "Gradient Norm_Batch: 0.08869388699531555\n",
      "Epoch [10/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286215208718204e-07, Correlation: 0.9999999085955908, R2: 0.9999998171378519\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.0889706239104271\n",
      "Gradient Norm_Batch: 0.06488817185163498\n",
      "Epoch [11/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828652074209458e-07, Correlation: 0.9999999085938241, R2: 0.9999998171347982\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.0636126771569252\n",
      "Gradient Norm_Batch: 0.051429636776447296\n",
      "Epoch [12/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286799274847e-07, Correlation: 0.999999908592512, R2: 0.9999998171319981\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.04940935596823692\n",
      "Gradient Norm_Batch: 0.04597235098481178\n",
      "Epoch [13/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8287018122009613e-07, Correlation: 0.9999999085914413, R2: 0.9999998171298183\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.042213015258312225\n",
      "Gradient Norm_Batch: 0.045634858310222626\n",
      "Epoch [14/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828702522743697e-07, Correlation: 0.9999999085914306, R2: 0.9999998171297602\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.0402626171708107\n",
      "Gradient Norm_Batch: 0.047030918300151825\n",
      "Epoch [15/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828695985750528e-07, Correlation: 0.9999999085917667, R2: 0.9999998171303928\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.04014680162072182\n",
      "Gradient Norm_Batch: 0.04353673383593559\n",
      "Epoch [16/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286739589257195e-07, Correlation: 0.9999999085928356, R2: 0.9999998171326027\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.038586296141147614\n",
      "Gradient Norm_Batch: 0.044387780129909515\n",
      "Epoch [17/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286797853761527e-07, Correlation: 0.9999999085925457, R2: 0.9999998171320201\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.03877308964729309\n",
      "Gradient Norm_Batch: 0.04409610480070114\n",
      "Epoch [18/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286755221197382e-07, Correlation: 0.9999999085927762, R2: 0.999999817132456\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.03899069130420685\n",
      "Gradient Norm_Batch: 0.04281553253531456\n",
      "Epoch [19/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828674385251361e-07, Correlation: 0.9999999085928047, R2: 0.9999998171325729\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.03932887688279152\n",
      "Gradient Norm_Batch: 0.043204568326473236\n",
      "Epoch [20/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286615954821173e-07, Correlation: 0.9999999085935481, R2: 0.9999998171338403\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.04104551300406456\n",
      "Gradient Norm_Batch: 0.028619758784770966\n",
      "Epoch [21/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828670690429135e-07, Correlation: 0.9999999085929093, R2: 0.9999998171329282\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.024405555799603462\n",
      "Gradient Norm_Batch: 0.028513222932815552\n",
      "Epoch [22/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286561953573255e-07, Correlation: 0.9999999085936512, R2: 0.9999998171343687\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.022791961207985878\n",
      "Gradient Norm_Batch: 0.027418194338679314\n",
      "Epoch [23/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286530689692881e-07, Correlation: 0.9999999085938169, R2: 0.9999998171346983\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.021601984277367592\n",
      "Gradient Norm_Batch: 0.02718273736536503\n",
      "Epoch [24/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.82865946385391e-07, Correlation: 0.9999999085934962, R2: 0.9999998171340528\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020966066047549248\n",
      "Gradient Norm_Batch: 0.027126872912049294\n",
      "Epoch [25/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286610270479287e-07, Correlation: 0.9999999085934188, R2: 0.9999998171338994\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02110273949801922\n",
      "Gradient Norm_Batch: 0.027095261961221695\n",
      "Epoch [26/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286607428308344e-07, Correlation: 0.9999999085934227, R2: 0.9999998171339064\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020955653861165047\n",
      "Gradient Norm_Batch: 0.027099445462226868\n",
      "Epoch [27/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286620218077587e-07, Correlation: 0.9999999085933722, R2: 0.9999998171338036\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020931914448738098\n",
      "Gradient Norm_Batch: 0.027107037603855133\n",
      "Epoch [28/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286620218077587e-07, Correlation: 0.9999999085933615, R2: 0.9999998171337833\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020862380042672157\n",
      "Gradient Norm_Batch: 0.027100764214992523\n",
      "Epoch [29/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286620218077587e-07, Correlation: 0.9999999085933631, R2: 0.9999998171337865\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020824342966079712\n",
      "Gradient Norm_Batch: 0.02714274451136589\n",
      "Epoch [30/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286628744590416e-07, Correlation: 0.9999999085933239, R2: 0.9999998171337067\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020823156461119652\n",
      "Gradient Norm_Batch: 0.02714274451136589\n",
      "Epoch [31/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286628744590416e-07, Correlation: 0.9999999085933239, R2: 0.9999998171337067\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020841818302869797\n",
      "Gradient Norm_Batch: 0.02714274451136589\n",
      "Epoch [32/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286628744590416e-07, Correlation: 0.9999999085933239, R2: 0.9999998171337067\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02084404230117798\n",
      "Gradient Norm_Batch: 0.02714274451136589\n",
      "Epoch [33/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286628744590416e-07, Correlation: 0.9999999085933239, R2: 0.9999998171337067\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020845811814069748\n",
      "Gradient Norm_Batch: 0.02714274451136589\n",
      "Epoch [34/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286628744590416e-07, Correlation: 0.9999999085933239, R2: 0.9999998171337067\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020846115425229073\n",
      "Gradient Norm_Batch: 0.02714274451136589\n",
      "Epoch [35/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286628744590416e-07, Correlation: 0.9999999085933239, R2: 0.9999998171337067\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020839426666498184\n",
      "Gradient Norm_Batch: 0.02714274451136589\n",
      "Epoch [36/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286628744590416e-07, Correlation: 0.9999999085933239, R2: 0.9999998171337067\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020844509825110435\n",
      "Gradient Norm_Batch: 0.02714274451136589\n",
      "Epoch [37/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286628744590416e-07, Correlation: 0.9999999085933239, R2: 0.9999998171337067\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02083936333656311\n",
      "Gradient Norm_Batch: 0.02714274451136589\n",
      "Epoch [38/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286628744590416e-07, Correlation: 0.9999999085933239, R2: 0.9999998171337067\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020845727995038033\n",
      "Gradient Norm_Batch: 0.02714274451136589\n",
      "Epoch [39/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286628744590416e-07, Correlation: 0.9999999085933239, R2: 0.9999998171337067\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02084800973534584\n",
      "Gradient Norm_Batch: 0.02714274451136589\n",
      "Epoch [40/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286628744590416e-07, Correlation: 0.9999999085933239, R2: 0.9999998171337067\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.0208438653498888\n",
      "Gradient Norm_Batch: 0.02714274451136589\n",
      "Epoch [41/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286628744590416e-07, Correlation: 0.9999999085933239, R2: 0.9999998171337067\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020846985280513763\n",
      "Gradient Norm_Batch: 0.02714274451136589\n",
      "Epoch [42/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286628744590416e-07, Correlation: 0.9999999085933239, R2: 0.9999998171337067\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02084537409245968\n",
      "Gradient Norm_Batch: 0.02714274451136589\n",
      "Epoch [43/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286628744590416e-07, Correlation: 0.9999999085933239, R2: 0.9999998171337067\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020844046026468277\n",
      "Gradient Norm_Batch: 0.02714274451136589\n",
      "Epoch [44/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286628744590416e-07, Correlation: 0.9999999085933239, R2: 0.9999998171337067\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02084607258439064\n",
      "Gradient Norm_Batch: 0.02714274451136589\n",
      "Epoch [45/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286628744590416e-07, Correlation: 0.9999999085933239, R2: 0.9999998171337067\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020840482786297798\n",
      "Gradient Norm_Batch: 0.02714274451136589\n",
      "Epoch [46/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286628744590416e-07, Correlation: 0.9999999085933239, R2: 0.9999998171337067\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020847517997026443\n",
      "Gradient Norm_Batch: 0.02714274451136589\n",
      "Epoch [47/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286628744590416e-07, Correlation: 0.9999999085933239, R2: 0.9999998171337067\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02084493264555931\n",
      "Gradient Norm_Batch: 0.02714274451136589\n",
      "Epoch [48/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286628744590416e-07, Correlation: 0.9999999085933239, R2: 0.9999998171337067\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020845821127295494\n",
      "Gradient Norm_Batch: 0.02714274451136589\n",
      "Epoch [49/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286628744590416e-07, Correlation: 0.9999999085933239, R2: 0.9999998171337067\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.0208445992320776\n",
      "Gradient Norm_Batch: 0.02714274451136589\n",
      "Epoch [50/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286628744590416e-07, Correlation: 0.9999999085933239, R2: 0.9999998171337067\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020846592262387276\n",
      "Gradient Norm_Batch: 0.02714274451136589\n",
      "Epoch [51/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286628744590416e-07, Correlation: 0.9999999085933239, R2: 0.9999998171337067\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020838990807533264\n",
      "Gradient Norm_Batch: 0.02714274451136589\n",
      "Epoch [52/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286628744590416e-07, Correlation: 0.9999999085933239, R2: 0.9999998171337067\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020845213904976845\n",
      "Gradient Norm_Batch: 0.02714274451136589\n",
      "Epoch [53/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286628744590416e-07, Correlation: 0.9999999085933239, R2: 0.9999998171337067\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020842300727963448\n",
      "Gradient Norm_Batch: 0.02714274451136589\n",
      "Epoch [54/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286628744590416e-07, Correlation: 0.9999999085933239, R2: 0.9999998171337067\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020838798955082893\n",
      "Gradient Norm_Batch: 0.02714274451136589\n",
      "Epoch [55/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286628744590416e-07, Correlation: 0.9999999085933239, R2: 0.9999998171337067\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02084408886730671\n",
      "Gradient Norm_Batch: 0.02714274451136589\n",
      "Epoch [56/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286628744590416e-07, Correlation: 0.9999999085933239, R2: 0.9999998171337067\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020842649042606354\n",
      "Gradient Norm_Batch: 0.02714274451136589\n",
      "Epoch [57/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286628744590416e-07, Correlation: 0.9999999085933239, R2: 0.9999998171337067\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02084537222981453\n",
      "Gradient Norm_Batch: 0.02714274451136589\n",
      "Epoch [58/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286628744590416e-07, Correlation: 0.9999999085933239, R2: 0.9999998171337067\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020845750346779823\n",
      "Gradient Norm_Batch: 0.02714274451136589\n",
      "Epoch [59/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286628744590416e-07, Correlation: 0.9999999085933239, R2: 0.9999998171337067\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020839707925915718\n",
      "Gradient Norm_Batch: 0.02714274451136589\n",
      "Epoch [60/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286628744590416e-07, Correlation: 0.9999999085933239, R2: 0.9999998171337067\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020840521901845932\n",
      "Gradient Norm_Batch: 0.02714274451136589\n",
      "Epoch [61/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286628744590416e-07, Correlation: 0.9999999085933239, R2: 0.9999998171337067\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020845312625169754\n",
      "Gradient Norm_Batch: 0.02714274451136589\n",
      "Epoch [62/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286628744590416e-07, Correlation: 0.9999999085933239, R2: 0.9999998171337067\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020843910053372383\n",
      "Gradient Norm_Batch: 0.02714274451136589\n",
      "Epoch [63/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286628744590416e-07, Correlation: 0.9999999085933239, R2: 0.9999998171337067\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020845843479037285\n",
      "Gradient Norm_Batch: 0.02714274451136589\n",
      "Epoch [64/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286628744590416e-07, Correlation: 0.9999999085933239, R2: 0.9999998171337067\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020841939374804497\n",
      "Gradient Norm_Batch: 0.02714274451136589\n",
      "Epoch [65/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286628744590416e-07, Correlation: 0.9999999085933239, R2: 0.9999998171337067\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020846262574195862\n",
      "Gradient Norm_Batch: 0.02714274451136589\n",
      "Epoch [66/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286628744590416e-07, Correlation: 0.9999999085933239, R2: 0.9999998171337067\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.0208441074937582\n",
      "Gradient Norm_Batch: 0.02714274451136589\n",
      "Epoch [67/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286628744590416e-07, Correlation: 0.9999999085933239, R2: 0.9999998171337067\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02084304578602314\n",
      "Gradient Norm_Batch: 0.02714274451136589\n",
      "Epoch [68/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286628744590416e-07, Correlation: 0.9999999085933239, R2: 0.9999998171337067\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02084248512983322\n",
      "Gradient Norm_Batch: 0.02714274451136589\n",
      "Epoch [69/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286628744590416e-07, Correlation: 0.9999999085933239, R2: 0.9999998171337067\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020845603197813034\n",
      "Gradient Norm_Batch: 0.02714274451136589\n",
      "Epoch [70/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286628744590416e-07, Correlation: 0.9999999085933239, R2: 0.9999998171337067\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020844699814915657\n",
      "Gradient Norm_Batch: 0.02714274451136589\n",
      "Epoch [71/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286628744590416e-07, Correlation: 0.9999999085933239, R2: 0.9999998171337067\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020843038335442543\n",
      "Gradient Norm_Batch: 0.02714274451136589\n",
      "Epoch [72/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286628744590416e-07, Correlation: 0.9999999085933239, R2: 0.9999998171337067\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020840948447585106\n",
      "Gradient Norm_Batch: 0.02714274451136589\n",
      "Epoch [73/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286628744590416e-07, Correlation: 0.9999999085933239, R2: 0.9999998171337067\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020846666768193245\n",
      "Gradient Norm_Batch: 0.02714274451136589\n",
      "Epoch [74/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286628744590416e-07, Correlation: 0.9999999085933239, R2: 0.9999998171337067\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020842555910348892\n",
      "Gradient Norm_Batch: 0.02714274451136589\n",
      "Epoch [75/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286628744590416e-07, Correlation: 0.9999999085933239, R2: 0.9999998171337067\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02084391377866268\n",
      "Gradient Norm_Batch: 0.02714274451136589\n",
      "Epoch [76/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286628744590416e-07, Correlation: 0.9999999085933239, R2: 0.9999998171337067\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020839491859078407\n",
      "Gradient Norm_Batch: 0.02714274451136589\n",
      "Epoch [77/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286628744590416e-07, Correlation: 0.9999999085933239, R2: 0.9999998171337067\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02084559202194214\n",
      "Gradient Norm_Batch: 0.02714274451136589\n",
      "Epoch [78/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286628744590416e-07, Correlation: 0.9999999085933239, R2: 0.9999998171337067\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020845232531428337\n",
      "Gradient Norm_Batch: 0.02714274451136589\n",
      "Epoch [79/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286628744590416e-07, Correlation: 0.9999999085933239, R2: 0.9999998171337067\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020840559154748917\n",
      "Gradient Norm_Batch: 0.02714274451136589\n",
      "Epoch [80/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286628744590416e-07, Correlation: 0.9999999085933239, R2: 0.9999998171337067\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02084105648100376\n",
      "Gradient Norm_Batch: 0.02714274451136589\n",
      "Epoch [81/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286628744590416e-07, Correlation: 0.9999999085933239, R2: 0.9999998171337067\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02084408327937126\n",
      "Gradient Norm_Batch: 0.02714274451136589\n",
      "Epoch [82/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286628744590416e-07, Correlation: 0.9999999085933239, R2: 0.9999998171337067\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020839519798755646\n",
      "Gradient Norm_Batch: 0.02714274451136589\n",
      "Epoch [83/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286628744590416e-07, Correlation: 0.9999999085933239, R2: 0.9999998171337067\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020843077450990677\n",
      "Gradient Norm_Batch: 0.027143023908138275\n",
      "Epoch [84/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286628744590416e-07, Correlation: 0.9999999085933291, R2: 0.9999998171337157\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02084161527454853\n",
      "Gradient Norm_Batch: 0.027143023908138275\n",
      "Epoch [85/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286628744590416e-07, Correlation: 0.9999999085933291, R2: 0.9999998171337157\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020842984318733215\n",
      "Gradient Norm_Batch: 0.027143023908138275\n",
      "Epoch [86/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286628744590416e-07, Correlation: 0.9999999085933291, R2: 0.9999998171337157\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020839698612689972\n",
      "Gradient Norm_Batch: 0.027143023908138275\n",
      "Epoch [87/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286628744590416e-07, Correlation: 0.9999999085933291, R2: 0.9999998171337157\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020842621102929115\n",
      "Gradient Norm_Batch: 0.027143023908138275\n",
      "Epoch [88/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286628744590416e-07, Correlation: 0.9999999085933291, R2: 0.9999998171337157\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02084077149629593\n",
      "Gradient Norm_Batch: 0.027143023908138275\n",
      "Epoch [89/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286628744590416e-07, Correlation: 0.9999999085933291, R2: 0.9999998171337157\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020844457671046257\n",
      "Gradient Norm_Batch: 0.027143023908138275\n",
      "Epoch [90/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286628744590416e-07, Correlation: 0.9999999085933291, R2: 0.9999998171337157\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020843427628278732\n",
      "Gradient Norm_Batch: 0.027143023908138275\n",
      "Epoch [91/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286628744590416e-07, Correlation: 0.9999999085933291, R2: 0.9999998171337157\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02084271050989628\n",
      "Gradient Norm_Batch: 0.027143023908138275\n",
      "Epoch [92/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286628744590416e-07, Correlation: 0.9999999085933291, R2: 0.9999998171337157\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020845796912908554\n",
      "Gradient Norm_Batch: 0.027143023908138275\n",
      "Epoch [93/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286628744590416e-07, Correlation: 0.9999999085933291, R2: 0.9999998171337157\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020848682150244713\n",
      "Gradient Norm_Batch: 0.027143023908138275\n",
      "Epoch [94/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286628744590416e-07, Correlation: 0.9999999085933291, R2: 0.9999998171337157\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020841587334871292\n",
      "Gradient Norm_Batch: 0.027143023908138275\n",
      "Epoch [95/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286628744590416e-07, Correlation: 0.9999999085933291, R2: 0.9999998171337157\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02084418758749962\n",
      "Gradient Norm_Batch: 0.027143023908138275\n",
      "Epoch [96/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286628744590416e-07, Correlation: 0.9999999085933291, R2: 0.9999998171337157\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020845914259552956\n",
      "Gradient Norm_Batch: 0.027143023908138275\n",
      "Epoch [97/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286628744590416e-07, Correlation: 0.9999999085933291, R2: 0.9999998171337157\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020846981555223465\n",
      "Gradient Norm_Batch: 0.027143023908138275\n",
      "Epoch [98/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286628744590416e-07, Correlation: 0.9999999085933291, R2: 0.9999998171337157\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020843999460339546\n",
      "Gradient Norm_Batch: 0.027143023908138275\n",
      "Epoch [99/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286628744590416e-07, Correlation: 0.9999999085933291, R2: 0.9999998171337157\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02084513194859028\n",
      "Gradient Norm_Batch: 0.027143023908138275\n",
      "Epoch [100/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286628744590416e-07, Correlation: 0.9999999085933291, R2: 0.9999998171337157\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020842138677835464\n",
      "Gradient Norm_Batch: 0.027143023908138275\n",
      "Epoch [101/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286628744590416e-07, Correlation: 0.9999999085933291, R2: 0.9999998171337157\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020843680948019028\n",
      "Gradient Norm_Batch: 0.027143023908138275\n",
      "Epoch [102/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286628744590416e-07, Correlation: 0.9999999085933291, R2: 0.9999998171337157\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02084280364215374\n",
      "Gradient Norm_Batch: 0.027143023908138275\n",
      "Epoch [103/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286628744590416e-07, Correlation: 0.9999999085933291, R2: 0.9999998171337157\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02084868773818016\n",
      "Gradient Norm_Batch: 0.027143023908138275\n",
      "Epoch [104/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286628744590416e-07, Correlation: 0.9999999085933291, R2: 0.9999998171337157\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020843692123889923\n",
      "Gradient Norm_Batch: 0.02713089995086193\n",
      "Epoch [105/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828663300784683e-07, Correlation: 0.9999999085933052, R2: 0.9999998171336688\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02084299549460411\n",
      "Gradient Norm_Batch: 0.02713089995086193\n",
      "Epoch [106/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828663300784683e-07, Correlation: 0.9999999085933052, R2: 0.9999998171336688\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02084260806441307\n",
      "Gradient Norm_Batch: 0.02713089995086193\n",
      "Epoch [107/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828663300784683e-07, Correlation: 0.9999999085933052, R2: 0.9999998171336688\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02083994634449482\n",
      "Gradient Norm_Batch: 0.02713089995086193\n",
      "Epoch [108/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828663300784683e-07, Correlation: 0.9999999085933052, R2: 0.9999998171336688\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02084207534790039\n",
      "Gradient Norm_Batch: 0.02713089995086193\n",
      "Epoch [109/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828663300784683e-07, Correlation: 0.9999999085933052, R2: 0.9999998171336688\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020842423662543297\n",
      "Gradient Norm_Batch: 0.02713089995086193\n",
      "Epoch [110/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828663300784683e-07, Correlation: 0.9999999085933052, R2: 0.9999998171336688\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02084285020828247\n",
      "Gradient Norm_Batch: 0.02713089995086193\n",
      "Epoch [111/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828663300784683e-07, Correlation: 0.9999999085933052, R2: 0.9999998171336688\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02084202878177166\n",
      "Gradient Norm_Batch: 0.02713089995086193\n",
      "Epoch [112/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828663300784683e-07, Correlation: 0.9999999085933052, R2: 0.9999998171336688\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020840685814619064\n",
      "Gradient Norm_Batch: 0.02713089995086193\n",
      "Epoch [113/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828663300784683e-07, Correlation: 0.9999999085933052, R2: 0.9999998171336688\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020840179175138474\n",
      "Gradient Norm_Batch: 0.02713089995086193\n",
      "Epoch [114/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828663300784683e-07, Correlation: 0.9999999085933052, R2: 0.9999998171336688\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020845701918005943\n",
      "Gradient Norm_Batch: 0.02713089995086193\n",
      "Epoch [115/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828663300784683e-07, Correlation: 0.9999999085933052, R2: 0.9999998171336688\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020846933126449585\n",
      "Gradient Norm_Batch: 0.02713089995086193\n",
      "Epoch [116/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828663300784683e-07, Correlation: 0.9999999085933052, R2: 0.9999998171336688\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020843269303441048\n",
      "Gradient Norm_Batch: 0.02713089995086193\n",
      "Epoch [117/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828663300784683e-07, Correlation: 0.9999999085933052, R2: 0.9999998171336688\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02084476314485073\n",
      "Gradient Norm_Batch: 0.02713089995086193\n",
      "Epoch [118/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828663300784683e-07, Correlation: 0.9999999085933052, R2: 0.9999998171336688\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020844275131821632\n",
      "Gradient Norm_Batch: 0.02713089995086193\n",
      "Epoch [119/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828663300784683e-07, Correlation: 0.9999999085933052, R2: 0.9999998171336688\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020842591300606728\n",
      "Gradient Norm_Batch: 0.02713089995086193\n",
      "Epoch [120/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828663300784683e-07, Correlation: 0.9999999085933052, R2: 0.9999998171336688\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020845016464591026\n",
      "Gradient Norm_Batch: 0.02713089995086193\n",
      "Epoch [121/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828663300784683e-07, Correlation: 0.9999999085933052, R2: 0.9999998171336688\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020848959684371948\n",
      "Gradient Norm_Batch: 0.02713089995086193\n",
      "Epoch [122/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828663300784683e-07, Correlation: 0.9999999085933052, R2: 0.9999998171336688\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02084769867360592\n",
      "Gradient Norm_Batch: 0.02713089995086193\n",
      "Epoch [123/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828663300784683e-07, Correlation: 0.9999999085933052, R2: 0.9999998171336688\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020847517997026443\n",
      "Gradient Norm_Batch: 0.02713089995086193\n",
      "Epoch [124/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828663300784683e-07, Correlation: 0.9999999085933052, R2: 0.9999998171336688\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020842716097831726\n",
      "Gradient Norm_Batch: 0.02713089995086193\n",
      "Epoch [125/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828663300784683e-07, Correlation: 0.9999999085933052, R2: 0.9999998171336688\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020841918885707855\n",
      "Gradient Norm_Batch: 0.02713089995086193\n",
      "Epoch [126/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828663300784683e-07, Correlation: 0.9999999085933052, R2: 0.9999998171336688\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02084360644221306\n",
      "Gradient Norm_Batch: 0.02713089995086193\n",
      "Epoch [127/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828663300784683e-07, Correlation: 0.9999999085933052, R2: 0.9999998171336688\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020841918885707855\n",
      "Gradient Norm_Batch: 0.02713089995086193\n",
      "Epoch [128/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828663300784683e-07, Correlation: 0.9999999085933052, R2: 0.9999998171336688\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020842671394348145\n",
      "Gradient Norm_Batch: 0.02713089995086193\n",
      "Epoch [129/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828663300784683e-07, Correlation: 0.9999999085933052, R2: 0.9999998171336688\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020841801539063454\n",
      "Gradient Norm_Batch: 0.02713089995086193\n",
      "Epoch [130/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828663300784683e-07, Correlation: 0.9999999085933052, R2: 0.9999998171336688\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020843572914600372\n",
      "Gradient Norm_Batch: 0.02713089995086193\n",
      "Epoch [131/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828663300784683e-07, Correlation: 0.9999999085933052, R2: 0.9999998171336688\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020845819264650345\n",
      "Gradient Norm_Batch: 0.02713089995086193\n",
      "Epoch [132/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828663300784683e-07, Correlation: 0.9999999085933052, R2: 0.9999998171336688\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020841682329773903\n",
      "Gradient Norm_Batch: 0.02713089995086193\n",
      "Epoch [133/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828663300784683e-07, Correlation: 0.9999999085933052, R2: 0.9999998171336688\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020841257646679878\n",
      "Gradient Norm_Batch: 0.02713089995086193\n",
      "Epoch [134/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828663300784683e-07, Correlation: 0.9999999085933052, R2: 0.9999998171336688\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020837251096963882\n",
      "Gradient Norm_Batch: 0.02713089995086193\n",
      "Epoch [135/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828663300784683e-07, Correlation: 0.9999999085933052, R2: 0.9999998171336688\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020843319594860077\n",
      "Gradient Norm_Batch: 0.02713089995086193\n",
      "Epoch [136/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828663300784683e-07, Correlation: 0.9999999085933052, R2: 0.9999998171336688\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020843125879764557\n",
      "Gradient Norm_Batch: 0.02713089995086193\n",
      "Epoch [137/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828663300784683e-07, Correlation: 0.9999999085933052, R2: 0.9999998171336688\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020842837169766426\n",
      "Gradient Norm_Batch: 0.02713089995086193\n",
      "Epoch [138/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828663300784683e-07, Correlation: 0.9999999085933052, R2: 0.9999998171336688\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020843403413891792\n",
      "Gradient Norm_Batch: 0.027215180918574333\n",
      "Epoch [139/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286648639787018e-07, Correlation: 0.9999999085932286, R2: 0.9999998171335129\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020843425765633583\n",
      "Gradient Norm_Batch: 0.027215180918574333\n",
      "Epoch [140/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286648639787018e-07, Correlation: 0.9999999085932286, R2: 0.9999998171335129\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020842567086219788\n",
      "Gradient Norm_Batch: 0.027215180918574333\n",
      "Epoch [141/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286648639787018e-07, Correlation: 0.9999999085932286, R2: 0.9999998171335129\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020847659558057785\n",
      "Gradient Norm_Batch: 0.027215180918574333\n",
      "Epoch [142/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286648639787018e-07, Correlation: 0.9999999085932286, R2: 0.9999998171335129\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020842092111706734\n",
      "Gradient Norm_Batch: 0.027215180918574333\n",
      "Epoch [143/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286648639787018e-07, Correlation: 0.9999999085932286, R2: 0.9999998171335129\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020842496305704117\n",
      "Gradient Norm_Batch: 0.027215180918574333\n",
      "Epoch [144/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286648639787018e-07, Correlation: 0.9999999085932286, R2: 0.9999998171335129\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02084920182824135\n",
      "Gradient Norm_Batch: 0.027215180918574333\n",
      "Epoch [145/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286648639787018e-07, Correlation: 0.9999999085932286, R2: 0.9999998171335129\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02084546536207199\n",
      "Gradient Norm_Batch: 0.027215180918574333\n",
      "Epoch [146/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286648639787018e-07, Correlation: 0.9999999085932286, R2: 0.9999998171335129\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02084072306752205\n",
      "Gradient Norm_Batch: 0.027215180918574333\n",
      "Epoch [147/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286648639787018e-07, Correlation: 0.9999999085932286, R2: 0.9999998171335129\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.0208468995988369\n",
      "Gradient Norm_Batch: 0.027215180918574333\n",
      "Epoch [148/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286648639787018e-07, Correlation: 0.9999999085932286, R2: 0.9999998171335129\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02083810791373253\n",
      "Gradient Norm_Batch: 0.027215180918574333\n",
      "Epoch [149/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286648639787018e-07, Correlation: 0.9999999085932286, R2: 0.9999998171335129\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020846422761678696\n",
      "Gradient Norm_Batch: 0.027215180918574333\n",
      "Epoch [150/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286648639787018e-07, Correlation: 0.9999999085932286, R2: 0.9999998171335129\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02084333822131157\n",
      "Gradient Norm_Batch: 0.027215180918574333\n",
      "Epoch [151/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286648639787018e-07, Correlation: 0.9999999085932286, R2: 0.9999998171335129\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02084653079509735\n",
      "Gradient Norm_Batch: 0.027215180918574333\n",
      "Epoch [152/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286648639787018e-07, Correlation: 0.9999999085932286, R2: 0.9999998171335129\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02084515057504177\n",
      "Gradient Norm_Batch: 0.027215180918574333\n",
      "Epoch [153/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286648639787018e-07, Correlation: 0.9999999085932286, R2: 0.9999998171335129\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020845182240009308\n",
      "Gradient Norm_Batch: 0.027215180918574333\n",
      "Epoch [154/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286648639787018e-07, Correlation: 0.9999999085932286, R2: 0.9999998171335129\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02084120362997055\n",
      "Gradient Norm_Batch: 0.027215180918574333\n",
      "Epoch [155/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286648639787018e-07, Correlation: 0.9999999085932286, R2: 0.9999998171335129\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020842133089900017\n",
      "Gradient Norm_Batch: 0.027215180918574333\n",
      "Epoch [156/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286648639787018e-07, Correlation: 0.9999999085932286, R2: 0.9999998171335129\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020845161750912666\n",
      "Gradient Norm_Batch: 0.027215180918574333\n",
      "Epoch [157/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286648639787018e-07, Correlation: 0.9999999085932286, R2: 0.9999998171335129\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02084069326519966\n",
      "Gradient Norm_Batch: 0.027215180918574333\n",
      "Epoch [158/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286648639787018e-07, Correlation: 0.9999999085932286, R2: 0.9999998171335129\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02084311470389366\n",
      "Gradient Norm_Batch: 0.027215180918574333\n",
      "Epoch [159/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286648639787018e-07, Correlation: 0.9999999085932286, R2: 0.9999998171335129\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02084248512983322\n",
      "Gradient Norm_Batch: 0.027215180918574333\n",
      "Epoch [160/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286648639787018e-07, Correlation: 0.9999999085932286, R2: 0.9999998171335129\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02084246650338173\n",
      "Gradient Norm_Batch: 0.027215180918574333\n",
      "Epoch [161/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286648639787018e-07, Correlation: 0.9999999085932286, R2: 0.9999998171335129\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020847270265221596\n",
      "Gradient Norm_Batch: 0.027215180918574333\n",
      "Epoch [162/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286648639787018e-07, Correlation: 0.9999999085932286, R2: 0.9999998171335129\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020844057202339172\n",
      "Gradient Norm_Batch: 0.027215180918574333\n",
      "Epoch [163/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286648639787018e-07, Correlation: 0.9999999085932286, R2: 0.9999998171335129\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02084348350763321\n",
      "Gradient Norm_Batch: 0.027215180918574333\n",
      "Epoch [164/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286648639787018e-07, Correlation: 0.9999999085932286, R2: 0.9999998171335129\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02084418572485447\n",
      "Gradient Norm_Batch: 0.027215180918574333\n",
      "Epoch [165/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286648639787018e-07, Correlation: 0.9999999085932286, R2: 0.9999998171335129\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02084856480360031\n",
      "Gradient Norm_Batch: 0.027215180918574333\n",
      "Epoch [166/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286648639787018e-07, Correlation: 0.9999999085932286, R2: 0.9999998171335129\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020841628313064575\n",
      "Gradient Norm_Batch: 0.027215180918574333\n",
      "Epoch [167/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286648639787018e-07, Correlation: 0.9999999085932286, R2: 0.9999998171335129\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02084510400891304\n",
      "Gradient Norm_Batch: 0.027215180918574333\n",
      "Epoch [168/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286648639787018e-07, Correlation: 0.9999999085932286, R2: 0.9999998171335129\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02084093540906906\n",
      "Gradient Norm_Batch: 0.027215180918574333\n",
      "Epoch [169/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286648639787018e-07, Correlation: 0.9999999085932286, R2: 0.9999998171335129\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020844554528594017\n",
      "Gradient Norm_Batch: 0.027215180918574333\n",
      "Epoch [170/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286648639787018e-07, Correlation: 0.9999999085932286, R2: 0.9999998171335129\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020843258127570152\n",
      "Gradient Norm_Batch: 0.027215180918574333\n",
      "Epoch [171/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286648639787018e-07, Correlation: 0.9999999085932286, R2: 0.9999998171335129\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020842425525188446\n",
      "Gradient Norm_Batch: 0.027215180918574333\n",
      "Epoch [172/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286648639787018e-07, Correlation: 0.9999999085932286, R2: 0.9999998171335129\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02084534242749214\n",
      "Gradient Norm_Batch: 0.027215180918574333\n",
      "Epoch [173/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286648639787018e-07, Correlation: 0.9999999085932286, R2: 0.9999998171335129\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.0208404753357172\n",
      "Gradient Norm_Batch: 0.027215180918574333\n",
      "Epoch [174/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286648639787018e-07, Correlation: 0.9999999085932286, R2: 0.9999998171335129\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020845521241426468\n",
      "Gradient Norm_Batch: 0.027215180918574333\n",
      "Epoch [175/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286648639787018e-07, Correlation: 0.9999999085932286, R2: 0.9999998171335129\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020841944962739944\n",
      "Gradient Norm_Batch: 0.027215180918574333\n",
      "Epoch [176/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286648639787018e-07, Correlation: 0.9999999085932286, R2: 0.9999998171335129\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020842164754867554\n",
      "Gradient Norm_Batch: 0.027215180918574333\n",
      "Epoch [177/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286648639787018e-07, Correlation: 0.9999999085932286, R2: 0.9999998171335129\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.0208441074937582\n",
      "Gradient Norm_Batch: 0.027215180918574333\n",
      "Epoch [178/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286648639787018e-07, Correlation: 0.9999999085932286, R2: 0.9999998171335129\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020843220874667168\n",
      "Gradient Norm_Batch: 0.027215180918574333\n",
      "Epoch [179/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286648639787018e-07, Correlation: 0.9999999085932286, R2: 0.9999998171335129\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02084534801542759\n",
      "Gradient Norm_Batch: 0.027215180918574333\n",
      "Epoch [180/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286648639787018e-07, Correlation: 0.9999999085932286, R2: 0.9999998171335129\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020839601755142212\n",
      "Gradient Norm_Batch: 0.027215180918574333\n",
      "Epoch [181/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286648639787018e-07, Correlation: 0.9999999085932286, R2: 0.9999998171335129\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02084134705364704\n",
      "Gradient Norm_Batch: 0.027215180918574333\n",
      "Epoch [182/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286648639787018e-07, Correlation: 0.9999999085932286, R2: 0.9999998171335129\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020842693746089935\n",
      "Gradient Norm_Batch: 0.027215180918574333\n",
      "Epoch [183/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286648639787018e-07, Correlation: 0.9999999085932286, R2: 0.9999998171335129\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02084825001657009\n",
      "Gradient Norm_Batch: 0.027215180918574333\n",
      "Epoch [184/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286648639787018e-07, Correlation: 0.9999999085932286, R2: 0.9999998171335129\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020848529413342476\n",
      "Gradient Norm_Batch: 0.027215180918574333\n",
      "Epoch [185/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286648639787018e-07, Correlation: 0.9999999085932286, R2: 0.9999998171335129\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02084398828446865\n",
      "Gradient Norm_Batch: 0.027215180918574333\n",
      "Epoch [186/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286648639787018e-07, Correlation: 0.9999999085932286, R2: 0.9999998171335129\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02083921618759632\n",
      "Gradient Norm_Batch: 0.027215180918574333\n",
      "Epoch [187/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286648639787018e-07, Correlation: 0.9999999085932286, R2: 0.9999998171335129\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020845241844654083\n",
      "Gradient Norm_Batch: 0.027215180918574333\n",
      "Epoch [188/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286648639787018e-07, Correlation: 0.9999999085932286, R2: 0.9999998171335129\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020846934989094734\n",
      "Gradient Norm_Batch: 0.027215180918574333\n",
      "Epoch [189/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286648639787018e-07, Correlation: 0.9999999085932286, R2: 0.9999998171335129\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020842695608735085\n",
      "Gradient Norm_Batch: 0.027215180918574333\n",
      "Epoch [190/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286648639787018e-07, Correlation: 0.9999999085932286, R2: 0.9999998171335129\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02084384299814701\n",
      "Gradient Norm_Batch: 0.027215180918574333\n",
      "Epoch [191/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286648639787018e-07, Correlation: 0.9999999085932286, R2: 0.9999998171335129\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020843276754021645\n",
      "Gradient Norm_Batch: 0.027215180918574333\n",
      "Epoch [192/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286648639787018e-07, Correlation: 0.9999999085932286, R2: 0.9999998171335129\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020843449980020523\n",
      "Gradient Norm_Batch: 0.027215180918574333\n",
      "Epoch [193/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286648639787018e-07, Correlation: 0.9999999085932286, R2: 0.9999998171335129\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020842401310801506\n",
      "Gradient Norm_Batch: 0.02721456065773964\n",
      "Epoch [194/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286648639787018e-07, Correlation: 0.9999999085932278, R2: 0.9999998171335119\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020844921469688416\n",
      "Gradient Norm_Batch: 0.02721456065773964\n",
      "Epoch [195/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286648639787018e-07, Correlation: 0.9999999085932278, R2: 0.9999998171335119\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.0208426546305418\n",
      "Gradient Norm_Batch: 0.02721456065773964\n",
      "Epoch [196/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286648639787018e-07, Correlation: 0.9999999085932278, R2: 0.9999998171335119\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02084200829267502\n",
      "Gradient Norm_Batch: 0.02721456065773964\n",
      "Epoch [197/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286648639787018e-07, Correlation: 0.9999999085932278, R2: 0.9999998171335119\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020841671153903008\n",
      "Gradient Norm_Batch: 0.02721456065773964\n",
      "Epoch [198/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286648639787018e-07, Correlation: 0.9999999085932278, R2: 0.9999998171335119\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02084210515022278\n",
      "Gradient Norm_Batch: 0.02721456065773964\n",
      "Epoch [199/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286648639787018e-07, Correlation: 0.9999999085932278, R2: 0.9999998171335119\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020845316350460052\n",
      "Gradient Norm_Batch: 0.02721456065773964\n",
      "Epoch [200/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286648639787018e-07, Correlation: 0.9999999085932278, R2: 0.9999998171335119\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02084384113550186\n",
      "Gradient Norm_Batch: 0.02721456065773964\n",
      "Epoch [201/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286648639787018e-07, Correlation: 0.9999999085932278, R2: 0.9999998171335119\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02084382250905037\n",
      "Gradient Norm_Batch: 0.02721456065773964\n",
      "Epoch [202/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286648639787018e-07, Correlation: 0.9999999085932278, R2: 0.9999998171335119\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020847616717219353\n",
      "Gradient Norm_Batch: 0.02721456065773964\n",
      "Epoch [203/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286648639787018e-07, Correlation: 0.9999999085932278, R2: 0.9999998171335119\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02084367349743843\n",
      "Gradient Norm_Batch: 0.02721456065773964\n",
      "Epoch [204/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286648639787018e-07, Correlation: 0.9999999085932278, R2: 0.9999998171335119\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020843520760536194\n",
      "Gradient Norm_Batch: 0.02721456065773964\n",
      "Epoch [205/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286648639787018e-07, Correlation: 0.9999999085932278, R2: 0.9999998171335119\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020845109596848488\n",
      "Gradient Norm_Batch: 0.02721456065773964\n",
      "Epoch [206/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286648639787018e-07, Correlation: 0.9999999085932278, R2: 0.9999998171335119\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020844822749495506\n",
      "Gradient Norm_Batch: 0.02721456065773964\n",
      "Epoch [207/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286648639787018e-07, Correlation: 0.9999999085932278, R2: 0.9999998171335119\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02083875797688961\n",
      "Gradient Norm_Batch: 0.02721456065773964\n",
      "Epoch [208/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286648639787018e-07, Correlation: 0.9999999085932278, R2: 0.9999998171335119\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020846573635935783\n",
      "Gradient Norm_Batch: 0.02721565216779709\n",
      "Epoch [209/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286648639787018e-07, Correlation: 0.999999908593229, R2: 0.9999998171335139\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02084546536207199\n",
      "Gradient Norm_Batch: 0.02721565216779709\n",
      "Epoch [210/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286648639787018e-07, Correlation: 0.999999908593229, R2: 0.9999998171335139\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02084897644817829\n",
      "Gradient Norm_Batch: 0.02721565216779709\n",
      "Epoch [211/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286648639787018e-07, Correlation: 0.999999908593229, R2: 0.9999998171335139\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02084772288799286\n",
      "Gradient Norm_Batch: 0.02721565216779709\n",
      "Epoch [212/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286648639787018e-07, Correlation: 0.999999908593229, R2: 0.9999998171335139\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02084476314485073\n",
      "Gradient Norm_Batch: 0.02721565216779709\n",
      "Epoch [213/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286648639787018e-07, Correlation: 0.999999908593229, R2: 0.9999998171335139\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02084263041615486\n",
      "Gradient Norm_Batch: 0.02721565216779709\n",
      "Epoch [214/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286648639787018e-07, Correlation: 0.999999908593229, R2: 0.9999998171335139\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020843062549829483\n",
      "Gradient Norm_Batch: 0.02721565216779709\n",
      "Epoch [215/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286648639787018e-07, Correlation: 0.999999908593229, R2: 0.9999998171335139\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020850123837590218\n",
      "Gradient Norm_Batch: 0.02721565216779709\n",
      "Epoch [216/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286648639787018e-07, Correlation: 0.999999908593229, R2: 0.9999998171335139\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020839504897594452\n",
      "Gradient Norm_Batch: 0.02721565216779709\n",
      "Epoch [217/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286648639787018e-07, Correlation: 0.999999908593229, R2: 0.9999998171335139\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020841872319579124\n",
      "Gradient Norm_Batch: 0.02721565216779709\n",
      "Epoch [218/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286648639787018e-07, Correlation: 0.999999908593229, R2: 0.9999998171335139\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020841440185904503\n",
      "Gradient Norm_Batch: 0.02721565216779709\n",
      "Epoch [219/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286648639787018e-07, Correlation: 0.999999908593229, R2: 0.9999998171335139\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020845923572778702\n",
      "Gradient Norm_Batch: 0.02721565216779709\n",
      "Epoch [220/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286648639787018e-07, Correlation: 0.999999908593229, R2: 0.9999998171335139\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02084353007376194\n",
      "Gradient Norm_Batch: 0.02721565216779709\n",
      "Epoch [221/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286648639787018e-07, Correlation: 0.999999908593229, R2: 0.9999998171335139\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020845049992203712\n",
      "Gradient Norm_Batch: 0.02721565216779709\n",
      "Epoch [222/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286648639787018e-07, Correlation: 0.999999908593229, R2: 0.9999998171335139\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020846553146839142\n",
      "Gradient Norm_Batch: 0.02721565216779709\n",
      "Epoch [223/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286648639787018e-07, Correlation: 0.999999908593229, R2: 0.9999998171335139\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020841436460614204\n",
      "Gradient Norm_Batch: 0.02721565216779709\n",
      "Epoch [224/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286648639787018e-07, Correlation: 0.999999908593229, R2: 0.9999998171335139\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020844338461756706\n",
      "Gradient Norm_Batch: 0.02721565216779709\n",
      "Epoch [225/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286648639787018e-07, Correlation: 0.999999908593229, R2: 0.9999998171335139\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020842621102929115\n",
      "Gradient Norm_Batch: 0.02721565216779709\n",
      "Epoch [226/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286648639787018e-07, Correlation: 0.999999908593229, R2: 0.9999998171335139\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02084393985569477\n",
      "Gradient Norm_Batch: 0.02721565216779709\n",
      "Epoch [227/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286648639787018e-07, Correlation: 0.999999908593229, R2: 0.9999998171335139\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020843029022216797\n",
      "Gradient Norm_Batch: 0.02721565216779709\n",
      "Epoch [228/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286648639787018e-07, Correlation: 0.999999908593229, R2: 0.9999998171335139\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020841874182224274\n",
      "Gradient Norm_Batch: 0.02721565216779709\n",
      "Epoch [229/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286648639787018e-07, Correlation: 0.999999908593229, R2: 0.9999998171335139\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020842822268605232\n",
      "Gradient Norm_Batch: 0.02721565216779709\n",
      "Epoch [230/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286648639787018e-07, Correlation: 0.999999908593229, R2: 0.9999998171335139\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02084309235215187\n",
      "Gradient Norm_Batch: 0.02721565216779709\n",
      "Epoch [231/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286648639787018e-07, Correlation: 0.999999908593229, R2: 0.9999998171335139\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020839828997850418\n",
      "Gradient Norm_Batch: 0.02721565216779709\n",
      "Epoch [232/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286648639787018e-07, Correlation: 0.999999908593229, R2: 0.9999998171335139\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02084214985370636\n",
      "Gradient Norm_Batch: 0.02721565216779709\n",
      "Epoch [233/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286648639787018e-07, Correlation: 0.999999908593229, R2: 0.9999998171335139\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020842669531702995\n",
      "Gradient Norm_Batch: 0.02721565216779709\n",
      "Epoch [234/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286648639787018e-07, Correlation: 0.999999908593229, R2: 0.9999998171335139\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02084566093981266\n",
      "Gradient Norm_Batch: 0.02721565216779709\n",
      "Epoch [235/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286648639787018e-07, Correlation: 0.999999908593229, R2: 0.9999998171335139\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020843505859375\n",
      "Gradient Norm_Batch: 0.02721565216779709\n",
      "Epoch [236/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286648639787018e-07, Correlation: 0.999999908593229, R2: 0.9999998171335139\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020846642553806305\n",
      "Gradient Norm_Batch: 0.02721565216779709\n",
      "Epoch [237/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286648639787018e-07, Correlation: 0.999999908593229, R2: 0.9999998171335139\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020840855315327644\n",
      "Gradient Norm_Batch: 0.02721565216779709\n",
      "Epoch [238/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286648639787018e-07, Correlation: 0.999999908593229, R2: 0.9999998171335139\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02084309235215187\n",
      "Gradient Norm_Batch: 0.02721565216779709\n",
      "Epoch [239/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286648639787018e-07, Correlation: 0.999999908593229, R2: 0.9999998171335139\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02084474079310894\n",
      "Gradient Norm_Batch: 0.02721565216779709\n",
      "Epoch [240/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286648639787018e-07, Correlation: 0.999999908593229, R2: 0.9999998171335139\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020845463499426842\n",
      "Gradient Norm_Batch: 0.02721565216779709\n",
      "Epoch [241/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286648639787018e-07, Correlation: 0.999999908593229, R2: 0.9999998171335139\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020849641412496567\n",
      "Gradient Norm_Batch: 0.02721565216779709\n",
      "Epoch [242/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286648639787018e-07, Correlation: 0.999999908593229, R2: 0.9999998171335139\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020839257165789604\n",
      "Gradient Norm_Batch: 0.02721565216779709\n",
      "Epoch [243/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286648639787018e-07, Correlation: 0.999999908593229, R2: 0.9999998171335139\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020842647179961205\n",
      "Gradient Norm_Batch: 0.02721565216779709\n",
      "Epoch [244/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286648639787018e-07, Correlation: 0.999999908593229, R2: 0.9999998171335139\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020842356607317924\n",
      "Gradient Norm_Batch: 0.02721565216779709\n",
      "Epoch [245/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286648639787018e-07, Correlation: 0.999999908593229, R2: 0.9999998171335139\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020847463980317116\n",
      "Gradient Norm_Batch: 0.02721565216779709\n",
      "Epoch [246/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286648639787018e-07, Correlation: 0.999999908593229, R2: 0.9999998171335139\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020848777145147324\n",
      "Gradient Norm_Batch: 0.02721565216779709\n",
      "Epoch [247/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.8286648639787018e-07, Correlation: 0.999999908593229, R2: 0.9999998171335139\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020846668630838394\n",
      "Gradient Norm_Batch: 0.027211835607886314\n",
      "Epoch [248/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932259, R2: 0.9999998171335079\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02084437943994999\n",
      "Gradient Norm_Batch: 0.027211835607886314\n",
      "Epoch [249/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932259, R2: 0.9999998171335079\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020843183621764183\n",
      "Gradient Norm_Batch: 0.027211835607886314\n",
      "Epoch [250/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932259, R2: 0.9999998171335079\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02084590680897236\n",
      "Gradient Norm_Batch: 0.027211835607886314\n",
      "Epoch [251/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932259, R2: 0.9999998171335079\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020845016464591026\n",
      "Gradient Norm_Batch: 0.027211835607886314\n",
      "Epoch [252/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932259, R2: 0.9999998171335079\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02084236592054367\n",
      "Gradient Norm_Batch: 0.027211835607886314\n",
      "Epoch [253/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932259, R2: 0.9999998171335079\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020846469327807426\n",
      "Gradient Norm_Batch: 0.027211835607886314\n",
      "Epoch [254/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932259, R2: 0.9999998171335079\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020844509825110435\n",
      "Gradient Norm_Batch: 0.027211835607886314\n",
      "Epoch [255/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932259, R2: 0.9999998171335079\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020845727995038033\n",
      "Gradient Norm_Batch: 0.027211835607886314\n",
      "Epoch [256/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932259, R2: 0.9999998171335079\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020845433697104454\n",
      "Gradient Norm_Batch: 0.02721007540822029\n",
      "Epoch [257/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932209, R2: 0.9999998171334967\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020847193896770477\n",
      "Gradient Norm_Batch: 0.02721007540822029\n",
      "Epoch [258/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932209, R2: 0.9999998171334967\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020841922610998154\n",
      "Gradient Norm_Batch: 0.02721007540822029\n",
      "Epoch [259/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932209, R2: 0.9999998171334967\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02084297314286232\n",
      "Gradient Norm_Batch: 0.02721007540822029\n",
      "Epoch [260/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932209, R2: 0.9999998171334967\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020845212042331696\n",
      "Gradient Norm_Batch: 0.02721007540822029\n",
      "Epoch [261/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932209, R2: 0.9999998171334967\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020844558253884315\n",
      "Gradient Norm_Batch: 0.02721007540822029\n",
      "Epoch [262/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932209, R2: 0.9999998171334967\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02084072306752205\n",
      "Gradient Norm_Batch: 0.02721007540822029\n",
      "Epoch [263/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932209, R2: 0.9999998171334967\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020842231810092926\n",
      "Gradient Norm_Batch: 0.02721007540822029\n",
      "Epoch [264/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932209, R2: 0.9999998171334967\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020843960344791412\n",
      "Gradient Norm_Batch: 0.02721007540822029\n",
      "Epoch [265/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932209, R2: 0.9999998171334967\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02083914540708065\n",
      "Gradient Norm_Batch: 0.02721007540822029\n",
      "Epoch [266/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932209, R2: 0.9999998171334967\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02084575593471527\n",
      "Gradient Norm_Batch: 0.02721007540822029\n",
      "Epoch [267/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932209, R2: 0.9999998171334967\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020841311663389206\n",
      "Gradient Norm_Batch: 0.02721007540822029\n",
      "Epoch [268/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932209, R2: 0.9999998171334967\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020846424624323845\n",
      "Gradient Norm_Batch: 0.02721007540822029\n",
      "Epoch [269/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932209, R2: 0.9999998171334967\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020844943821430206\n",
      "Gradient Norm_Batch: 0.02721007540822029\n",
      "Epoch [270/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932209, R2: 0.9999998171334967\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02084553800523281\n",
      "Gradient Norm_Batch: 0.02721007540822029\n",
      "Epoch [271/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932209, R2: 0.9999998171334967\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020843563601374626\n",
      "Gradient Norm_Batch: 0.02721007540822029\n",
      "Epoch [272/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932209, R2: 0.9999998171334967\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020840918645262718\n",
      "Gradient Norm_Batch: 0.02721007540822029\n",
      "Epoch [273/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932209, R2: 0.9999998171334967\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02084091305732727\n",
      "Gradient Norm_Batch: 0.02721007540822029\n",
      "Epoch [274/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932209, R2: 0.9999998171334967\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02084355242550373\n",
      "Gradient Norm_Batch: 0.02721007540822029\n",
      "Epoch [275/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932209, R2: 0.9999998171334967\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020847423002123833\n",
      "Gradient Norm_Batch: 0.02721007540822029\n",
      "Epoch [276/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932209, R2: 0.9999998171334967\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020841969177126884\n",
      "Gradient Norm_Batch: 0.02721007540822029\n",
      "Epoch [277/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932209, R2: 0.9999998171334967\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020843591541051865\n",
      "Gradient Norm_Batch: 0.02721007540822029\n",
      "Epoch [278/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932209, R2: 0.9999998171334967\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020842792466282845\n",
      "Gradient Norm_Batch: 0.02721007540822029\n",
      "Epoch [279/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932209, R2: 0.9999998171334967\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020839765667915344\n",
      "Gradient Norm_Batch: 0.02721007540822029\n",
      "Epoch [280/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932209, R2: 0.9999998171334967\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020840641111135483\n",
      "Gradient Norm_Batch: 0.02721007540822029\n",
      "Epoch [281/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932209, R2: 0.9999998171334967\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020836498588323593\n",
      "Gradient Norm_Batch: 0.02721007540822029\n",
      "Epoch [282/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932209, R2: 0.9999998171334967\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020843733102083206\n",
      "Gradient Norm_Batch: 0.02721007540822029\n",
      "Epoch [283/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932209, R2: 0.9999998171334967\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020844493061304092\n",
      "Gradient Norm_Batch: 0.02721007540822029\n",
      "Epoch [284/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932209, R2: 0.9999998171334967\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02084219641983509\n",
      "Gradient Norm_Batch: 0.02721007540822029\n",
      "Epoch [285/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932209, R2: 0.9999998171334967\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020842762663960457\n",
      "Gradient Norm_Batch: 0.02721007540822029\n",
      "Epoch [286/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932209, R2: 0.9999998171334967\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020842961966991425\n",
      "Gradient Norm_Batch: 0.02721007540822029\n",
      "Epoch [287/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932209, R2: 0.9999998171334967\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020843300968408585\n",
      "Gradient Norm_Batch: 0.02721007540822029\n",
      "Epoch [288/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932209, R2: 0.9999998171334967\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02084389701485634\n",
      "Gradient Norm_Batch: 0.02721007540822029\n",
      "Epoch [289/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932209, R2: 0.9999998171334967\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020841652527451515\n",
      "Gradient Norm_Batch: 0.02721007540822029\n",
      "Epoch [290/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932209, R2: 0.9999998171334967\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02084197662770748\n",
      "Gradient Norm_Batch: 0.02721007540822029\n",
      "Epoch [291/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932209, R2: 0.9999998171334967\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020844997838139534\n",
      "Gradient Norm_Batch: 0.02721007540822029\n",
      "Epoch [292/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932209, R2: 0.9999998171334967\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020843705162405968\n",
      "Gradient Norm_Batch: 0.02721007540822029\n",
      "Epoch [293/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932209, R2: 0.9999998171334967\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020843230187892914\n",
      "Gradient Norm_Batch: 0.02721007540822029\n",
      "Epoch [294/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932209, R2: 0.9999998171334967\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02084428071975708\n",
      "Gradient Norm_Batch: 0.02721007540822029\n",
      "Epoch [295/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932209, R2: 0.9999998171334967\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020849695429205894\n",
      "Gradient Norm_Batch: 0.02721007540822029\n",
      "Epoch [296/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932209, R2: 0.9999998171334967\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02084476687014103\n",
      "Gradient Norm_Batch: 0.02721007540822029\n",
      "Epoch [297/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932209, R2: 0.9999998171334967\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020841922610998154\n",
      "Gradient Norm_Batch: 0.02721007540822029\n",
      "Epoch [298/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932209, R2: 0.9999998171334967\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02084672637283802\n",
      "Gradient Norm_Batch: 0.02721007540822029\n",
      "Epoch [299/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932209, R2: 0.9999998171334967\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020845772698521614\n",
      "Gradient Norm_Batch: 0.02721007540822029\n",
      "Epoch [300/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932209, R2: 0.9999998171334967\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02084203250706196\n",
      "Gradient Norm_Batch: 0.02721007540822029\n",
      "Epoch [301/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932209, R2: 0.9999998171334967\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020844269543886185\n",
      "Gradient Norm_Batch: 0.02721007540822029\n",
      "Epoch [302/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932209, R2: 0.9999998171334967\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02084191143512726\n",
      "Gradient Norm_Batch: 0.02721007540822029\n",
      "Epoch [303/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932209, R2: 0.9999998171334967\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02084328979253769\n",
      "Gradient Norm_Batch: 0.02721007540822029\n",
      "Epoch [304/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932209, R2: 0.9999998171334967\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020842300727963448\n",
      "Gradient Norm_Batch: 0.02721007540822029\n",
      "Epoch [305/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932209, R2: 0.9999998171334967\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020844915881752968\n",
      "Gradient Norm_Batch: 0.02721007540822029\n",
      "Epoch [306/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932209, R2: 0.9999998171334967\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020838238298892975\n",
      "Gradient Norm_Batch: 0.02721007540822029\n",
      "Epoch [307/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932209, R2: 0.9999998171334967\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02084353007376194\n",
      "Gradient Norm_Batch: 0.02721007540822029\n",
      "Epoch [308/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932209, R2: 0.9999998171334967\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020840277895331383\n",
      "Gradient Norm_Batch: 0.02721007540822029\n",
      "Epoch [309/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932209, R2: 0.9999998171334967\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020832818001508713\n",
      "Gradient Norm_Batch: 0.02721007540822029\n",
      "Epoch [310/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932209, R2: 0.9999998171334967\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020848041400313377\n",
      "Gradient Norm_Batch: 0.02721007540822029\n",
      "Epoch [311/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932209, R2: 0.9999998171334967\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020845631137490273\n",
      "Gradient Norm_Batch: 0.02721007540822029\n",
      "Epoch [312/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932209, R2: 0.9999998171334967\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020844167098402977\n",
      "Gradient Norm_Batch: 0.02721007540822029\n",
      "Epoch [313/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932209, R2: 0.9999998171334967\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020842937752604485\n",
      "Gradient Norm_Batch: 0.02721007540822029\n",
      "Epoch [314/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932209, R2: 0.9999998171334967\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02084777131676674\n",
      "Gradient Norm_Batch: 0.02721007540822029\n",
      "Epoch [315/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932209, R2: 0.9999998171334967\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020842311903834343\n",
      "Gradient Norm_Batch: 0.02721007540822029\n",
      "Epoch [316/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932209, R2: 0.9999998171334967\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020843062549829483\n",
      "Gradient Norm_Batch: 0.02721007540822029\n",
      "Epoch [317/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932209, R2: 0.9999998171334967\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.0208401158452034\n",
      "Gradient Norm_Batch: 0.02721007540822029\n",
      "Epoch [318/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932209, R2: 0.9999998171334967\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020844534039497375\n",
      "Gradient Norm_Batch: 0.02721007540822029\n",
      "Epoch [319/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932209, R2: 0.9999998171334967\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02084018662571907\n",
      "Gradient Norm_Batch: 0.02721007540822029\n",
      "Epoch [320/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932209, R2: 0.9999998171334967\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020843906328082085\n",
      "Gradient Norm_Batch: 0.02721007540822029\n",
      "Epoch [321/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932209, R2: 0.9999998171334967\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020841823890805244\n",
      "Gradient Norm_Batch: 0.02721007540822029\n",
      "Epoch [322/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932209, R2: 0.9999998171334967\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020847590640187263\n",
      "Gradient Norm_Batch: 0.02721007540822029\n",
      "Epoch [323/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932209, R2: 0.9999998171334967\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02084611915051937\n",
      "Gradient Norm_Batch: 0.02721007540822029\n",
      "Epoch [324/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932209, R2: 0.9999998171334967\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020846497267484665\n",
      "Gradient Norm_Batch: 0.02721007540822029\n",
      "Epoch [325/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932209, R2: 0.9999998171334967\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020844263955950737\n",
      "Gradient Norm_Batch: 0.02721007540822029\n",
      "Epoch [326/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932209, R2: 0.9999998171334967\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.0208435095846653\n",
      "Gradient Norm_Batch: 0.02721007540822029\n",
      "Epoch [327/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932209, R2: 0.9999998171334967\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020845288410782814\n",
      "Gradient Norm_Batch: 0.02721007540822029\n",
      "Epoch [328/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932209, R2: 0.9999998171334967\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020850766450166702\n",
      "Gradient Norm_Batch: 0.02721007540822029\n",
      "Epoch [329/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932209, R2: 0.9999998171334967\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020843487232923508\n",
      "Gradient Norm_Batch: 0.02721007540822029\n",
      "Epoch [330/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932209, R2: 0.9999998171334967\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.0208441074937582\n",
      "Gradient Norm_Batch: 0.02721007540822029\n",
      "Epoch [331/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932209, R2: 0.9999998171334967\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020839115604758263\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [332/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020838476717472076\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [333/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02084074169397354\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [334/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020838553085923195\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [335/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020841768011450768\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [336/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020841015502810478\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [337/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020839985460042953\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [338/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02083996869623661\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [339/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020840704441070557\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [340/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020839931443333626\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [341/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020838791504502296\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [342/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020838864147663116\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [343/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020837411284446716\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [344/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020836787298321724\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [345/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020839020609855652\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [346/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020840760320425034\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [347/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020842665806412697\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [348/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020845666527748108\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [349/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020838746801018715\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [350/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02083824761211872\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [351/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020841427147388458\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [352/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02083825320005417\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [353/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02083563432097435\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [354/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020837102085351944\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [355/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020839855074882507\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [356/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020839445292949677\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [357/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020838581025600433\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [358/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020843731239438057\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [359/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020839674398303032\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [360/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020839326083660126\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [361/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02084191143512726\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [362/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020848164334893227\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [363/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020837945863604546\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [364/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020840520039200783\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [365/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020841803401708603\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [366/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02084064483642578\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [367/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.0208418108522892\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [368/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02084088884294033\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [369/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020844655111432076\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [370/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02084038034081459\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [371/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020840734243392944\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [372/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.0208426546305418\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [373/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020839637145400047\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [374/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02083968184888363\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [375/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02084065042436123\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [376/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020841626450419426\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [377/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020837802439928055\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [378/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020837221294641495\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [379/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02083965763449669\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [380/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020840149372816086\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [381/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02084258198738098\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [382/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02084122598171234\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [383/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020842012017965317\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [384/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020835034549236298\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [385/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02083686552941799\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [386/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.0208396278321743\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [387/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02083655633032322\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [388/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020845215767621994\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [389/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020840611308813095\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [390/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020840395241975784\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [391/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.0208370890468359\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [392/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02083880826830864\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [393/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020840493962168694\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [394/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020837990567088127\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [395/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020837103947997093\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [396/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020838728174567223\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [397/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020839840173721313\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [398/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020834987983107567\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [399/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020840350538492203\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [400/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02083982527256012\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [401/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020837221294641495\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [402/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020842760801315308\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [403/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020836351439356804\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [404/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020841823890805244\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [405/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02084130235016346\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [406/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02084379829466343\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [407/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020842012017965317\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [408/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020840933546423912\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [409/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02084343507885933\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [410/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.0208389051258564\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [411/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02083881013095379\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [412/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02083587273955345\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [413/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020838597789406776\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [414/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020838527008891106\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [415/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02084277756512165\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [416/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020839335396885872\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [417/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02083560824394226\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [418/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02083904668688774\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [419/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02083837240934372\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [420/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02083902433514595\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [421/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020839346572756767\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [422/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020838694646954536\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [423/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020838366821408272\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [424/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020841524004936218\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [425/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02083803527057171\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [426/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02083751931786537\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [427/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02084105648100376\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [428/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020840780809521675\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [429/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020838800817728043\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [430/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020834963768720627\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [431/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020836595445871353\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [432/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02084122784435749\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [433/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020843546837568283\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [434/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020835187286138535\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [435/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020837394520640373\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [436/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020837334915995598\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [437/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020841633901000023\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [438/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02083858661353588\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [439/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020839281380176544\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [440/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020839672535657883\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [441/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020841939374804497\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [442/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02084343135356903\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [443/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02083643153309822\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [444/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020839735865592957\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [445/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020836109295487404\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [446/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.0208370853215456\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [447/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020843980833888054\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [448/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020842518657445908\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [449/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020837800577282906\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [450/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020837608724832535\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [451/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02084266021847725\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [452/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020839743316173553\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [453/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020842116326093674\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [454/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020842861384153366\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [455/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020842529833316803\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [456/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020840920507907867\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [457/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02083655074238777\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [458/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020838947966694832\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [459/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020838718861341476\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [460/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02083861641585827\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [461/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020837366580963135\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [462/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020837312564253807\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [463/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.0208416897803545\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [464/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020833643153309822\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [465/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020840439945459366\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [466/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020836567506194115\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [467/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020839562639594078\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [468/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020836971700191498\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [469/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02084173820912838\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [470/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020837675780057907\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [471/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020839877426624298\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [472/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020835302770137787\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [473/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020839368924498558\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [474/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020837964490056038\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [475/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.0208425335586071\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [476/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02083708718419075\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [477/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02084064856171608\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [478/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020839592441916466\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [479/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02083609439432621\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [480/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020840128883719444\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [481/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02084117941558361\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [482/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020839422941207886\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [483/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020838098600506783\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [484/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020839227363467216\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [485/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020837584510445595\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [486/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020837320014834404\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [487/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020836517214775085\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [488/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.02084370143711567\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [489/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020839007571339607\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [490/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020836837589740753\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [491/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020833903923630714\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [492/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020835796371102333\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [493/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020839033648371696\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [494/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020836366340517998\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [495/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020836282521486282\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [496/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020838728174567223\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [497/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020837197080254555\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [498/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020839078351855278\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [499/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "650\n",
      "Gradient Norm_Of_Each_Mini_Batch: 0.020833807066082954\n",
      "Gradient Norm_Batch: 0.027209386229515076\n",
      "Epoch [500/500], Loss: 2.5629, Gap to Optimality: 2.5629, NMSE: 1.828665148195796e-07, Correlation: 0.9999999085932211, R2: 0.9999998171334971\n",
      "Final gradient of the subproblem Core : 0.027209386229515076\n",
      "The threshold activated1.6802046957309358e-05\n",
      "(32, 32)\n",
      "---------------------------Testing with Best Lambda------------------------------\n",
      "Y Test Predicted:  [ 67.54952084  64.18941303 100.9065354  106.79474319  66.52853718\n",
      "  96.79609753 112.76440447  87.08130038  58.88453415  60.74017178\n",
      "  65.75309846  64.15694524  82.80165689  68.56392956 142.45952736\n",
      "  98.02027629  98.35343278  55.41583541  96.74739603  87.26441568\n",
      "  47.97747612  91.13718838  73.56468442  77.46457664  46.96363592\n",
      "  79.38007367  84.71050135  45.49422176  87.56694605  73.47968867\n",
      "  86.90814973  88.26726981 124.65371489  81.42676791  87.54746196\n",
      "  82.79838609  98.73512456  69.03734531  80.63862824  93.0836955\n",
      "  68.40957824  87.28010795  62.04184167  57.33991767  56.0207063\n",
      "  89.30946842 110.54390014 134.61163804  43.26755011  74.48143084\n",
      "  96.23321808  87.89117218  79.23396778  90.72731726 125.8857718\n",
      " 160.37131911  60.51156622  86.88820016  89.57430836  85.4968726\n",
      "  79.97211634  95.54243441  57.0987802   50.36895218  70.86110165\n",
      " 131.23465709  94.51177414  83.16015609  62.92637828  84.10811176\n",
      "  76.25925799  96.48163442  77.08412144  65.79255477  71.7790571\n",
      "  65.14943256  61.02618608  68.75447753  66.40998267 104.95740224\n",
      "  79.74021344  91.80686631 110.44728616  62.24775532  57.61545194\n",
      "  98.12944     67.83567884  72.12848434 127.85112826  63.30844826\n",
      " 103.6976849  150.08097042  67.4195969   92.38851485 123.67940397\n",
      "  46.91062055  64.66048612 108.22095911 101.14010799 113.37888858]\n",
      "Y Test Actual:  [ 67.54932471  64.19367989 100.89907103 106.80033802  66.5296898\n",
      "  96.80085769 112.75716496  87.08379176  58.86645104  60.75871896\n",
      "  65.76651181  64.14603363  82.80477245  68.55912818 142.45923938\n",
      "  98.00053671  98.36489423  55.40988723  96.73410604  87.25275894\n",
      "  47.97392502  91.14302569  73.57729565  77.46144622  46.97758628\n",
      "  79.38435102  84.7040345   45.4879342   87.55672081  73.49886432\n",
      "  86.90598491  88.25175908 124.63253198  81.43265772  87.55287141\n",
      "  82.8080788   98.72768177  69.0468243   80.62856336  93.08545506\n",
      "  68.40807412  87.26847367  62.04717769  57.33990441  56.01334774\n",
      "  89.31934273 110.55219975 134.61307846  43.26935217  74.47617341\n",
      "  96.21843559  87.89937403  79.21862917  90.71786386 125.8824493\n",
      " 160.38439066  60.50387741  86.87508783  89.5739495   85.50880057\n",
      "  79.97185552  95.54965657  57.09355445  50.35928763  70.86185745\n",
      " 131.24078613  94.50837386  83.15863626  62.93422433  84.09831139\n",
      "  76.26182003  96.48126457  77.07300387  65.77838739  71.76861932\n",
      "  65.14937109  61.0440168   68.74400314  66.41967439 104.95340089\n",
      "  79.75508697  91.80406117 110.44770514  62.24491669  57.60974417\n",
      "  98.13365195  67.82489044  72.14320497 127.85744229  63.30804812\n",
      " 103.70015697 150.07489827  67.42743244  92.3906977  123.66379363\n",
      "  46.92967844  64.66629945 108.22935093 101.14612067 113.38031778]\n",
      "Error Report on Testing _ With best Lambda\n",
      "SGD_Alpha chosen for model:  2.5\n",
      "SGD_Test Normalized Estimation Error:  7.3964851034045534e-09\n",
      "SGD_Test NMSE Loss:  1.1333328687966223e-08\n",
      "SGD_Test R2 Loss:  0.9999998428703463\n",
      "SGD_Test Correlation:  0.9999999218323866\n",
      "Objective Function Values 1110.0197569359127\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final gradient: 0.027209386229515076\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAoIAAAHFCAYAAABmRZhcAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAABe7klEQVR4nO3de1hVVcI/8O/hHG4iYOAgtzihMQhCyEUM0DHFQbFAx26mJlP59lJ4QdK0+M1gl9Gpd2ymRsMXMwsvI74zUkoqYiAmZTiAI0opioVxkcEUUEw5h/X7w+Ho8QCeo8Ayz/fzPOd5ZO2191p7y+P5uvZeayuEEAJEREREZHYsZHeAiIiIiORgECQiIiIyUwyCRERERGaKQZCIiIjITDEIEhEREZkpBkEiIiIiM8UgSERERGSmGASJiIiIzBSDIBEREZGZYhAkIrrORx99BIVC0eln4cKFUvu2adMm/OUvf+l0m0KhwNKlS/u0P7/5zW9ga2uL8+fPd1lnxowZsLS0xJkzZ4w+roxzITJXKtkdICK6E61btw5Dhw7VK3N3d5fUm6s2bdqEI0eOIDk52WDbV199BU9Pzz7tz3PPPYdPPvkEmzZtwosvvmiwvampCdnZ2XjkkUcwaNCgPu0bERmHQZCIqBMBAQEICwuT3Q2jPfjgg33eZmxsLNzd3fHhhx92GgT/9re/4dKlS3juuef6vG9EZBzeGiYiMlFXty7vu+8+/Pa3v9X93HGbuaCgAC+88AIGDhwIZ2dnTJ06FbW1tQb7b9q0CREREejfvz/69++P4cOHY+3atQCAhx56CJ999hm+//57vdvV3fXpyJEjmDx5Mu655x7Y2Nhg+PDh+Pjjj/Xq7N27FwqFAn/729+QmpoKd3d3ODg4YPz48Th27Fi310GpVCIhIQElJSUoLy832L5u3Tq4ubkhNjYW//73v/Hiiy/C398f/fv3h4uLC8aNG4cvvvii2zYAYOnSpXrn2qHj+n733Xd65VlZWYiIiICdnR369++PCRMmoKys7KbtEJkjBkEiok5otVpoNBq9z62aPXs2LC0tsWnTJrz99tvYu3cvZs6cqVfn97//PWbMmAF3d3d89NFHyM7ORkJCAr7//nsAwPvvv4+oqCi4urriq6++0n26cuzYMURGRuLo0aN47733sHXrVvj7++O3v/0t3n77bYP6r776Kr7//nt88MEHyMjIQGVlJeLi4qDVars9t2effRYKhQIffvihXnlFRQWKi4uRkJAApVKJH3/8EQCQlpaGzz77DOvWrcPgwYPx0EMPYe/evcZcRqMsW7YMTz31FPz9/bFlyxasX78eLS0tGD16NCoqKnqsHaK7hiAiIp1169YJAJ1+2trahBBCABBpaWkG+6rVapGQkGBwrBdffFGv3ttvvy0AiLq6OiGEEFVVVUKpVIoZM2Z027eHH35YqNXqTrfd2Kdp06YJa2trUV1drVcvNjZW9OvXT5w/f14IIURBQYEAICZNmqRXb8uWLQKA+Oqrr7rtkxBCjBkzRgwcOFBcuXJFV/bSSy8JAOL48eOd7qPRaERbW5uIjo4Wv/nNb7o9l7S0NNHZ11XH9T116pQQQojq6mqhUqnE3Llz9eq1tLQIV1dX8cQTT9z0XIjMDUcEiYg6kZmZiYMHD+p9VKpbe6w6Pj5e7+cHHngAAHSjfXl5edBqtUhKSrq9Tl8nPz8f0dHRuPfee/XKf/vb36K1tdVgNPFmfezOc889h8bGRmzbtg0AoNFosGHDBowePRo+Pj66eqtXr0ZISAhsbGygUqlgaWmJzz//HN98880tneONcnNzodFoMGvWLL2RXBsbG4wZM6ZHRx6J7hYMgkREnfDz80NYWJje51Y5Ozvr/WxtbQ0AuHTpEgDg3//+NwD06Kzfs2fPws3NzaC8Y+bz2bNnTepjdx577DE4Ojpi3bp1AIAdO3bgzJkzepNE3nnnHbzwwgsYOXIk/vGPf+DAgQM4ePAgJk6caFQbxuhYombEiBGwtLTU+2RlZaGxsbFH2iG6m3DWMBGRiaytrXH58mWD8hvDlbF+8YtfAAB++OEHgxG8W+Xs7Iy6ujqD8o5JKgMHDuyRdgDA1tYWTz31FNasWYO6ujp8+OGHsLe3x+OPP66rs2HDBjz00ENIT0/X27elpeWmx7exsQEAXL58WRdQARgEu45z+vvf/w61Wn3L50NkTjgiSERkovvuuw+HDx/WK8vPz8eFCxdu6XgxMTFQKpUGIelG1tbWRo+eRUdHIz8/32B2cmZmJvr169fjy80899xz0Gq1+J//+R/s2LED06ZNQ79+/XTbFQqFXogDgMOHD3c74aXDfffdp6t/ve3bt+v9PGHCBKhUKpw8edJgNPd2R3WJ7lYcESQiMtHTTz+N3/3ud/j973+PMWPGoKKiAitXroSjo+MtHe++++7Dq6++ijfeeAOXLl3CU089BUdHR1RUVKCxsRGvvfYaACAwMBBbt25Feno6QkNDYWFh0WW4SUtLQ05ODsaOHYvf//73cHJywsaNG/HZZ5/h7bffvuW+diUsLAwPPPAA/vKXv0AIYbB24COPPII33ngDaWlpGDNmDI4dO4bXX38d3t7eN52RPWnSJDg5OeG5557D66+/DpVKhY8++ginT5/Wq3fffffh9ddfR2pqKqqqqjBx4kTcc889OHPmDIqLi2FnZ6e7lkR0FYMgEZGJFi1ahObmZnz00Uf405/+hPDwcGzZsgWTJ0++5WO+/vrr8PHxwV//+lfMmDEDKpUKPj4+mDdvnq7O/PnzcfToUbz66qtoamqCEAJCiE6P5+vriy+//BKvvvoqkpKScOnSJfj5+WHdunV6ax32pOeeew7z58+Hv78/Ro4cqbctNTUVra2tWLt2Ld5++234+/tj9erVyM7OvukkDgcHB+zatQvJycmYOXMmBgwYgNmzZyM2NhazZ8/Wq/vKK6/A398f7777Lv72t7/h8uXLcHV1xYgRI5CYmNjTp0z0s6cQXf0rQkRERER3NT4jSERERGSmGASJiIiIzBSDIBEREZGZYhAkIiIiMlMMgkRERERmikGQiIiIyExxHUHqVnt7O2pra2Fvbw+FQiG7O0RERGQEIQRaWlrg7u4OC4uux/0YBKlbtbW1PfbuUyIiIupbp0+fhqenZ5fbGQSpW/b29gCu/iI5ODhI7g0REREZo7m5Gffee6/ue7wrDILUrY7bwQ4ODgyCREREPzM3e6yLk0WIiIiIzBSDIBEREZGZYhAkIiIiMlMMgkRERERmikGQiIiIyEwxCBIRERGZKQZBIiIiIjPFIEhERERkphgEiYiIiMwU3yxCfU7bLlB86kc0tPwEF3sbhHs7QWnR/crnRERE1PMYBKlP7TpSh9e2V6Cu6SddmZujDdLi/DExwE1iz4iIiMwPbw1Tn9l1pA4vbCjVC4EAUN/0E17YUIpdR+ok9YyIiMg8MQhSn9C2C7y2vQKik20dZa9tr4C2vbMaRERE1BsYBKlPFJ/60WAk8HoCQF3TTyg+9WPfdYqIiMjMMQhSn2ho6ToE3ko9IiIiun0MgtQnXOxterQeERER3T4GQeoT4d5OcHO0QVeLxChwdfZwuLdTX3aLiIjIrDEIUp9QWiiQFucPAAZhsOPntDh/ridIRETUhxgEqc9MDHBD+swQuDrq3/51dbRB+swQriNIRETUx7igNPWpiQFu+LW/K0b+YQ8aL17BG5OHYfpINUcCiYiIJOCIIPU5pYUCNlZKAMAwD0eGQCIiIkkYBEkKxX+yn+D60URERNIwCJIUCt0UESZBIiIiWRgESQoLjggSERFJxyBIUij+c2+YrxYmIiKSh0GQpNDdGOaQIBERkTQMgiRHx61hub0gIiIyawyCJMW1EUGp3SAiIjJrDIJmJicnB76+vvDx8cEHH3wgrR8dzwgKjgkSERFJwzeLmBGNRoOUlBQUFBTAwcEBISEhmDp1KpycnPq8LxZcPYaIiEg6jgiakeLiYgwbNgweHh6wt7fHpEmTkJubK6UvHesIctYwERGRPNKD4PLlyzFixAjY29vDxcUFU6ZMwbFjx7rdZ+nSpVAoFHofV1dXg3o1NTWYOXMmnJ2d0a9fPwwfPhwlJSVGt9vS0oLk5GSo1WrY2toiMjISBw8evKW+3K59+/YhLi4O7u7uUCgU+OSTTzqt9/7778Pb2xs2NjYIDQ3FF198odtWW1sLDw8P3c+enp6oqanp8b4aQ/dmEQ4JEhERSSM9CBYWFiIpKQkHDhxAXl4eNBoNYmJicPHixW73GzZsGOrq6nSf8vJyve3nzp1DVFQULC0tsXPnTlRUVGDFihUYMGCA0e3Onj0beXl5WL9+PcrLyxETE4Px48cbhKeb9eV6RUVFaGtrMyj/9ttvUV9f3+V+Fy9eRFBQEFauXNllnaysLCQnJyM1NRVlZWUYPXo0YmNjUV1dDaDzpVo6ntWThZNFiIiIJBJ3mIaGBgFAFBYWdlknLS1NBAUFdXucxYsXi1GjRt1yu62trUKpVIqcnBy9ekFBQSI1NdWkvnTQarUiKChIPPbYY0Kj0ejKjx07JlxdXcVbb71l1HEAiOzsbIPy8PBwkZiYqFc2dOhQsWTJEiGEEEVFRWLKlCm6bfPmzRMbN27stI2VK1cKPz8/8ctf/lIAEE1NTUb1zVgT/7JPqBfniL3HGnr0uERERCREU1OTUd/f0kcEb9TU1AQAN53AUFlZCXd3d3h7e2PatGmoqqrS275t2zaEhYXh8ccfh4uLC4KDg7FmzRqj29VoNNBqtbCxsdGrZ2tri/3795vUlw4WFhbYsWMHysrKMGvWLLS3t+PkyZMYN24c4uPj8fLLL3d7zt25cuUKSkpKEBMTo1ceExODL7/8EgAQHh6OI0eOoKamBi0tLdixYwcmTJjQ6fGSkpJQUVFhcCu8p1x7xRyHBImIiGS5o4KgEAIpKSkYNWoUAgICuqw3cuRIZGZmIjc3F2vWrEF9fT0iIyNx9uxZXZ2qqiqkp6fDx8cHubm5SExMxLx585CZmWlUu/b29oiIiMAbb7yB2tpaaLVabNiwAV9//TXq6upM6sv13N3dkZ+fj6KiIkyfPh3jxo1DdHQ0Vq9efauXDQDQ2NgIrVaLQYMG6ZUPGjRId8tZpVJhxYoVGDt2LIKDg7Fo0SI4OzvfVru3SsEFpYmIiKS7o5aPmTNnDg4fPmww4naj2NhY3Z8DAwMRERGBIUOG4OOPP0ZKSgoAoL29HWFhYVi2bBkAIDg4GEePHkV6ejpmzZplVLvr16/Hs88+Cw8PDyiVSoSEhGD69OkoLS01qS838vLyQmZmJsaMGYPBgwdj7dq1Pfas3o3HEULolcXHxyM+Pr5H2rodHbOGOSJIREQkzx0zIjh37lxs27YNBQUF8PT0NGlfOzs7BAYGorKyUlfm5uYGf39/vXp+fn66iRPGtDtkyBAUFhbiwoULOH36NIqLi9HW1gZvb2+T+nKjM2fO4Pnnn0dcXBxaW1uxYMECU063UwMHDoRSqTSYcNLQ0GAwSngn0I0IMgcSERFJIz0ICiEwZ84cbN26Ffn5+d2GrK5cvnwZ33zzDdzc3HRlUVFRBsvBHD9+HGq12uR27ezs4ObmhnPnziE3NxeTJ082qS/Xa2xsRHR0NPz8/HRtb9myBQsXLjTllA1YWVkhNDQUeXl5euV5eXmIjIy8rWP3Br5ijoiISD7pt4aTkpKwadMmfPrpp7C3t9eNaDk6OsLW1hYrV65EdnY2Pv/8c90+CxcuRFxcHLy8vNDQ0IA333wTzc3NSEhI0NVZsGABIiMjsWzZMjzxxBMoLi5GRkYGMjIyjGoXAHJzcyGEgK+vL06cOIFFixbB19cXzzzzjEl96dDe3o6JEydCrVYjKysLKpUKfn5+2LNnD8aOHQsPD48uRwcvXLiAEydO6H4+deoUDh06BCcnJ3h5eQEAUlJS8PTTTyMsLAwRERHIyMhAdXU1EhMTb+nvplfpXjFHRERE0vTy7OWbwtUsYPBZt26dEOLq8ixqtVpvnyeffFK4ubkJS0tL4e7uLqZOnSqOHj1qcOzt27eLgIAAYW1tLYYOHSoyMjKMblcIIbKyssTgwYOFlZWVcHV1FUlJSeL8+fO31JcOu3fvFpcuXTIoLysrE9XV1V3uV1BQ0Gl/ExIS9OqtWrVKqNVqYWVlJUJCQrpdhscYxk4/N9WUVfuFenGOyD1S16PHJSIiIuO/vxVC8OYcda25uRmOjo5oamqCg4NDjx136vtFKK0+j/99OhQThvX8m1iIiIjMmbHf39KfESTz1DGTmf8PISIikodBkKTgZBEiIiL5GARJCi4oTUREJB+DIElxbUFpyR0hIiIyYwyCJMW1EUEmQSIiIlkYBEmKjiDYzhxIREQkDYMgScF3DRMREcnHIEhSdIwIEhERkTwMgiSF7hlBDggSERFJwyBIUljo3jXMJEhERCQLgyBJxRFBIiIieRgESYqOV8xx1jAREZE8DIIkxbVXzDEJEhERycIgSFLwFXNERETyMQiSFLrVY5gEiYiIpGEQJCk4a5iIiEg+BkGSgusIEhERyccgSJJw1jAREZFsDIIkxbXJIkyCREREsjAIkhTXlo+R2g0iIiKzxiBIUlybLEJERESyMAiSFAoOCRIREUnHIEhScEFpIiIi+RgESQpFx6xhThsmIiKShkGQ5OCIIBERkXQMgiQFHxEkIiKSj0GQpOCsYSIiIvkYBEmKa6+YYxQkIiKShUGQpFDcvAoRERH1MgZBkkKh6HjXMEcEiYiIZGEQJCk4WYSIiEg+BkGSg8vHEBERSccgSFLoZg0zCRIREUnDIEhS6G4Nc0yQiIhIGgZBkuLa8jFy+0FERGTOGARJio53DXMdQSIiInkYBM1MTk4OfH194ePjgw8++EBaPzgiSEREJJ9Kdgeo72g0GqSkpKCgoAAODg4ICQnB1KlT4eTk1Od9UfAVc0RERNJxRNCMFBcXY9iwYfDw8IC9vT0mTZqE3NxcKX3hiCAREZF80oPg8uXLMWLECNjb28PFxQVTpkzBsWPHut1n6dKlUCgUeh9XV1eDejU1NZg5cyacnZ3Rr18/DB8+HCUlJUa329LSguTkZKjVatja2iIyMhIHDx68pb7crn379iEuLg7u7u5QKBT45JNPOq33/vvvw9vbGzY2NggNDcUXX3yh21ZbWwsPDw/dz56enqipqenxvhqDs4aJiIjkkx4ECwsLkZSUhAMHDiAvLw8ajQYxMTG4ePFit/sNGzYMdXV1uk95ebne9nPnziEqKgqWlpbYuXMnKioqsGLFCgwYMMDodmfPno28vDysX78e5eXliImJwfjx4w3C0836cr2ioiK0tbUZlH/77beor6/vcr+LFy8iKCgIK1eu7LJOVlYWkpOTkZqairKyMowePRqxsbGorq4G0PnEjI5btH2to9l25kAiIiJ5xB2moaFBABCFhYVd1klLSxNBQUHdHmfx4sVi1KhRt9xua2urUCqVIicnR69eUFCQSE1NNakvHbRarQgKChKPPfaY0Gg0uvJjx44JV1dX8dZbbxl1HAAiOzvboDw8PFwkJibqlQ0dOlQsWbJECCFEUVGRmDJlim7bvHnzxMaNG7ttq6mpSQAQTU1NRvXNWP8vu1yoF+eIFbnf9uhxiYiIyPjvb+kjgjdqamoCgJtOYKisrIS7uzu8vb0xbdo0VFVV6W3ftm0bwsLC8Pjjj8PFxQXBwcFYs2aN0e1qNBpotVrY2Njo1bO1tcX+/ftN6ksHCwsL7NixA2VlZZg1axba29tx8uRJjBs3DvHx8Xj55Ze7PefuXLlyBSUlJYiJidErj4mJwZdffgkACA8Px5EjR1BTU4OWlhbs2LEDEyZM6PR4q1atgr+/P0aMGHHLfeqOgq+YIyIiku6OCoJCCKSkpGDUqFEICAjost7IkSORmZmJ3NxcrFmzBvX19YiMjMTZs2d1daqqqpCeng4fHx/k5uYiMTER8+bNQ2ZmplHt2tvbIyIiAm+88QZqa2uh1WqxYcMGfP3116irqzOpL9dzd3dHfn4+ioqKMH36dIwbNw7R0dFYvXr1rV42AEBjYyO0Wi0GDRqkVz5o0CDdLWeVSoUVK1Zg7NixCA4OxqJFi+Ds7Nzp8ZKSklBRUWHwTGRP4SvmiIiI5Lujlo+ZM2cODh8+bDDidqPY2FjdnwMDAxEREYEhQ4bg448/RkpKCgCgvb0dYWFhWLZsGQAgODgYR48eRXp6OmbNmmVUu+vXr8ezzz4LDw8PKJVKhISEYPr06SgtLTWpLzfy8vJCZmYmxowZg8GDB2Pt2rU99qzejccRQuiVxcfHIz4+vkfa6gmcLEJERCTPHTMiOHfuXGzbtg0FBQXw9PQ0aV87OzsEBgaisrJSV+bm5gZ/f3+9en5+frqJE8a0O2TIEBQWFuLChQs4ffo0iouL0dbWBm9vb5P6cqMzZ87g+eefR1xcHFpbW7FgwQJTTrdTAwcOhFKpNJhw0tDQYDBKeCfg8jFERETySQ+CQgjMmTMHW7duRX5+frchqyuXL1/GN998Azc3N11ZVFSUwXIwx48fh1qtNrldOzs7uLm54dy5c8jNzcXkyZNN6sv1GhsbER0dDT8/P13bW7ZswcKFC005ZQNWVlYIDQ1FXl6eXnleXh4iIyNv69i9oeMVc5w1TEREJI/0W8NJSUnYtGkTPv30U9jb2+tGtBwdHWFra4uVK1ciOzsbn3/+uW6fhQsXIi4uDl5eXmhoaMCbb76J5uZmJCQk6OosWLAAkZGRWLZsGZ544gkUFxcjIyMDGRkZRrULALm5uRBCwNfXFydOnMCiRYvg6+uLZ555xqS+dGhvb8fEiROhVquRlZUFlUoFPz8/7NmzB2PHjoWHh0eXo4MXLlzAiRMndD+fOnUKhw4dgpOTE7y8vAAAKSkpePrppxEWFoaIiAhkZGSguroaiYmJt/R305uuTRZhEiQiIpKm1+cv3wSuThw1+Kxbt04IcXV5FrVarbfPk08+Kdzc3ISlpaVwd3cXU6dOFUePHjU49vbt20VAQICwtrYWQ4cOFRkZGUa3K4QQWVlZYvDgwcLKykq4urqKpKQkcf78+VvqS4fdu3eLS5cuGZSXlZWJ6urqLvcrKCjotL8JCQl69VatWiXUarWwsrISISEh3S7DY4zeWj7mje1HhXpxjlj2WUWPHpeIiIiM//5WCMGntKhrzc3NcHR0RFNTExwcHHrsuMt2fIOMfVV4/leD8eokvx47LhERERn//S39GUEyT7pXzPH/IURERNIwCJIcnDVMREQkHYMgScFZw0RERPIxCJIUnDVMREQkH4MgSWHBW8NERETSMQiSFAr0zCv1iIiI6NYxCJIU114xxyFBIiIiWRgESQrd8jFSe0FERGTeGARJDkXHrGFGQSIiIlkYBEmKawtKS+0GERGRWWMQJCks/jMiyBxIREQkD4MgSaHg8jFERETSMQiSFNcWj2ESJCIikoVBkKToGBFsb5fbDyIiInPGIEhSKHTPCHJEkIiISBYGQZKCzwgSERHJxyBIUnS8Yo45kIiISB4GQZKCI4JERETyMQiSFNdeMcckSEREJAuDIEnBEUEiIiL5GARJCt0zgkyCRERE0jAIkhS6EUG53SAiIjJrDIIkhW4dQSZBIiIiaRgESYprk0WIiIhIFgZBkuLaZBFGQSIiIlkYBEkK3YggcyAREZE0DIIkBd81TEREJB+DIElhwXUEiYiIpGMQJDk4a5iIiEg6BkGSgq+YIyIiko9BkKTgK+aIiIjkYxAkKTpeMdfOIEhERCQNgyBJ0TFZhEtKExERycMgSFLw1jAREZF8DIIkRcetYeZAIiIieRgESQ6+Yo6IiEg6BkEzk5OTA19fX/j4+OCDDz6Q1o+ORwQ5WYSIiEgelewOUN/RaDRISUlBQUEBHBwcEBISgqlTp8LJyanP+3LtFXNEREQkC0cEzUhxcTGGDRsGDw8P2NvbY9KkScjNzZXSFwveGiYiIpJOehBcvnw5RowYAXt7e7i4uGDKlCk4duxYt/ssXboUCoVC7+Pq6mpQr6amBjNnzoSzszP69euH4cOHo6SkxOh2W1pakJycDLVaDVtbW0RGRuLgwYO31JfbtW/fPsTFxcHd3R0KhQKffPJJp/Xef/99eHt7w8bGBqGhofjiiy9022pra+Hh4aH72dPTEzU1NT3eV2MoFDevQ0RERL1LehAsLCxEUlISDhw4gLy8PGg0GsTExODixYvd7jds2DDU1dXpPuXl5Xrbz507h6ioKFhaWmLnzp2oqKjAihUrMGDAAKPbnT17NvLy8rB+/XqUl5cjJiYG48ePNwhPN+vL9YqKitDW1mZQ/u2336K+vr7L/S5evIigoCCsXLmyyzpZWVlITk5GamoqysrKMHr0aMTGxqK6uhpA56NvCkmJTDdrmAOCRERE8og7TENDgwAgCgsLu6yTlpYmgoKCuj3O4sWLxahRo2653dbWVqFUKkVOTo5evaCgIJGammpSXzpotVoRFBQkHnvsMaHRaHTlx44dE66uruKtt94y6jgARHZ2tkF5eHi4SExM1CsbOnSoWLJkiRBCiKKiIjFlyhTdtnnz5omNGzd221ZTU5MAIJqamozqm7E+KftBqBfniOlrvurR4xIREZHx39/SRwRv1NTUBAA3ncBQWVkJd3d3eHt7Y9q0aaiqqtLbvm3bNoSFheHxxx+Hi4sLgoODsWbNGqPb1Wg00Gq1sLGx0atna2uL/fv3m9SXDhYWFtixYwfKysowa9YstLe34+TJkxg3bhzi4+Px8ssvd3vO3bly5QpKSkoQExOjVx4TE4Mvv/wSABAeHo4jR46gpqYGLS0t2LFjByZMmNDp8VatWgV/f3+MGDHilvtkjPb2Xj08ERERdeOOCoJCCKSkpGDUqFEICAjost7IkSORmZmJ3NxcrFmzBvX19YiMjMTZs2d1daqqqpCeng4fHx/k5uYiMTER8+bNQ2ZmplHt2tvbIyIiAm+88QZqa2uh1WqxYcMGfP3116irqzOpL9dzd3dHfn4+ioqKMH36dIwbNw7R0dFYvXr1rV42AEBjYyO0Wi0GDRqkVz5o0CDdLWeVSoUVK1Zg7NixCA4OxqJFi+Ds7Nzp8ZKSklBRUWHwTGRPuTZrmPeGiYiIZLmjlo+ZM2cODh8+bDDidqPY2FjdnwMDAxEREYEhQ4bg448/RkpKCgCgvb0dYWFhWLZsGQAgODgYR48eRXp6OmbNmmVUu+vXr8ezzz4LDw8PKJVKhISEYPr06SgtLTWpLzfy8vJCZmYmxowZg8GDB2Pt2rU99qzejccRQuiVxcfHIz4+vkfauh0WfMUcERGRdHfMiODcuXOxbds2FBQUwNPT06R97ezsEBgYiMrKSl2Zm5sb/P399er5+fnpJk4Y0+6QIUNQWFiICxcu4PTp0yguLkZbWxu8vb1N6suNzpw5g+effx5xcXFobW3FggULTDndTg0cOBBKpdJgwklDQ4PBKOGdgK+YIyIikk96EBRCYM6cOdi6dSvy8/O7DVlduXz5Mr755hu4ubnpyqKiogyWgzl+/DjUarXJ7drZ2cHNzQ3nzp1Dbm4uJk+ebFJfrtfY2Ijo6Gj4+fnp2t6yZQsWLlxoyikbsLKyQmhoKPLy8vTK8/LyEBkZeVvH7g26QUomQSIiImmk3xpOSkrCpk2b8Omnn8Le3l43ouXo6AhbW1usXLkS2dnZ+Pzzz3X7LFy4EHFxcfDy8kJDQwPefPNNNDc3IyEhQVdnwYIFiIyMxLJly/DEE0+guLgYGRkZyMjIMKpdAMjNzYUQAr6+vjhx4gQWLVoEX19fPPPMMyb1pUN7ezsmTpwItVqNrKwsqFQq+Pn5Yc+ePRg7diw8PDy6HB28cOECTpw4ofv51KlTOHToEJycnODl5QUASElJwdNPP42wsDBEREQgIyMD1dXVSExMvKW/m950LQcyCRIREUnT6/OXbwJXx4QMPuvWrRNCXF2eRa1W6+3z5JNPCjc3N2FpaSnc3d3F1KlTxdGjRw2OvX37dhEQECCsra3F0KFDRUZGhtHtCiFEVlaWGDx4sLCyshKurq4iKSlJnD9//pb60mH37t3i0qVLBuVlZWWiurq6y/0KCgo67W9CQoJevVWrVgm1Wi2srKxESEhIt8vwGKO3lo/ZWV4r1ItzxNT3i3r0uERERGT897dCCD6uT11rbm6Go6Mjmpqa4ODg0GPHzT1aj/9eX4IQrwHY+mJUjx2XiIiIjP/+lv6MIJknPiJIREQkH4MgSaFbR5BJkIiISBoGQZKCI4JERETyMQiSFNeWj2EUJCIikoVBkKToCILtzIFERETSMAiSFHzXMBERkXwMgiQF7wwTERHJxyBIUnDWMBERkXwMgiQFZw0TERHJxyBIUnRMFuGLbYiIiORhECQpLHhrmIiISDoGQZLi2q1hJkEiIiJZGARJDt2tYbndICIiMmcMgiSFAh3rCBIREZEsDIIkBSeLEBERyccgSFJwQWkiIiL5GARJCgsL3homIiKSjUGQpLg2IsgoSEREJAuDIEmhe0ZQbjeIiIjMGoMgScIFpYmIiGRjECQpro0IMgkSERHJwiBIUnQ8I9jeLrUbREREZs3kILhr1y7s379f9/OqVaswfPhwTJ8+HefOnevRztHdq+Ndw0RERCSPyUFw0aJFaG5uBgCUl5fjpZdewqRJk1BVVYWUlJQe7yDdnbigNBERkXwqU3c4deoU/P39AQD/+Mc/8Mgjj2DZsmUoLS3FpEmTeryDdHfiK+aIiIjkM3lE0MrKCq2trQCAPXv2ICYmBgDg5OSkGykkuplrI4Jy+0FERGTOTB4RHDVqFFJSUhAVFYXi4mJkZWUBAI4fPw5PT88e7yDd3ThrmIiISB6TRwRXrlwJlUqFv//970hPT4eHhwcAYOfOnZg4cWKPd5DuTh2TRdqZA4mIiKQxeUTQy8sLOTk5BuV//vOfe6RDZB54a5iIiEg+o4Jgc3MzHBwcdH/uTkc9ou5cWz2GSZCIiEgWo4LgPffcg7q6Ori4uGDAgAFQdLIGnBACCoUCWq22xztJdx8FXzFHREQknVFBMD8/H05OTro/dxYEiUxx7RVzREREJItRQXDMmDG6Pz/00EO91RcyIx3/leCC0kRERPKYPGv4d7/7Xae3f5uamvDUU0/1SKfo7qfgrGEiIiLpTA6CmZmZiIqKwsmTJ3Vle/fuRWBgIL777rue7BvdxfiKOSIiIvlMDoKHDx/Gfffdh+HDh2PNmjVYtGgRYmJi8Nvf/hb79+/vjT7SXUh3a1hqL4iIiMybyUHQ0dERmzdvxrx58/Df//3fePfdd7Fz5068/vrrUCqVvdFH6kE5OTnw9fWFj48PPvjgA2n9UHC2CBERkXQmB0EA+Otf/4o///nPeOqppzB48GDMmzcP//rXv3q6b9TDNBoNUlJSkJ+fj9LSUrz11lv48ccfpfSFI4JERETymRwEY2Nj8dprryEzMxMbN25EWVkZfvWrX+HBBx/E22+/3Rt9pB5SXFyMYcOGwcPDA/b29pg0aRJyc3Ol9IXPCBIREclnchDUaDQ4fPgwHnvsMQCAra0t0tPT8fe///2WXjO3fPlyjBgxAvb29nBxccGUKVNw7NixbvdZunQpFAqF3sfV1dWgXk1NDWbOnAlnZ2f069cPw4cPR0lJidHttrS0IDk5GWq1Gra2toiMjMTBgwdvqS+3a9++fYiLi4O7uzsUCgU++eSTTuu9//778Pb2ho2NDUJDQ/HFF1/ottXW1ureDQ0Anp6eqKmp6fG+GoPvGiYiIpLP5CCYl5cHd3d3g/KHH34Y5eXlJnegsLAQSUlJOHDgAPLy8qDRaBATE4OLFy92u9+wYcNQV1en+9zY9rlz5xAVFQVLS0vs3LkTFRUVWLFiBQYMGGB0u7Nnz0ZeXh7Wr1+P8vJyxMTEYPz48Qbh6WZ9uV5RURHa2toMyr/99lvU19d3ud/FixcRFBSElStXdlknKysLycnJSE1NRVlZGUaPHo3Y2FhUV1cD6Hz0Tfbi4II3h4mIiOQRd5iGhgYBQBQWFnZZJy0tTQQFBXV7nMWLF4tRo0bdcrutra1CqVSKnJwcvXpBQUEiNTXVpL500Gq1IigoSDz22GNCo9Hoyo8dOyZcXV3FW2+9ZdRxAIjs7GyD8vDwcJGYmKhXNnToULFkyRIhhBBFRUViypQpum3z5s0TGzdu7LatpqYmAUA0NTUZ1Tdjnf7xolAvzhG/TN3Ro8clIiIi47+/TR4R1Gq1+NOf/oTw8HC4urrCyclJ73O7mpqaAOCmx6qsrIS7uzu8vb0xbdo0VFVV6W3ftm0bwsLC8Pjjj8PFxQXBwcFYs2aN0e1qNBpotVrY2Njo1bO1tTVYJudmfelgYWGBHTt2oKysDLNmzUJ7eztOnjyJcePGIT4+Hi+//HK359ydK1euoKSkBDExMXrlMTEx+PLLLwEA4eHhOHLkCGpqatDS0oIdO3ZgwoQJnR5v1apV8Pf3x4gRI265T93pGInkeCAREZE8JgfB1157De+88w6eeOIJNDU1ISUlBVOnToWFhQWWLl16W50RQiAlJQWjRo1CQEBAl/VGjhyJzMxM5ObmYs2aNaivr0dkZCTOnj2rq1NVVYX09HT4+PggNzcXiYmJmDdvHjIzM41q197eHhEREXjjjTdQW1sLrVaLDRs24Ouvv0ZdXZ1Jfbmeu7s78vPzUVRUhOnTp2PcuHGIjo7G6tWrb/WyAQAaGxuh1WoxaNAgvfJBgwbpbjmrVCqsWLECY8eORXBwMBYtWgRnZ+dOj5eUlISKigqDZyJ7iu6GNJMgERGRPKYONQ4ePFh3u7R///7ixIkTQggh3n33XfHUU0+ZPHR5vRdffFGo1Wpx+vRpk/a7cOGCGDRokFixYoWuzNLSUkREROjVmzt3rnjwwQeNbvfEiRPiV7/6lQAglEqlGDFihJgxY4bw8/MzqS+dKSwsFADE4MGDRVtbmzGnqYNObg3X1NQIAOLLL7/UK3/zzTeFr6+vSce/Xm/dGq47f0moF+eIIa981qPHJSIiol68NVxfX4/AwEAAQP/+/XW3VB955BF89tlntxxI586di23btqGgoACenp4m7WtnZ4fAwEBUVlbqytzc3ODv769Xz8/PTzdxwph2hwwZgsLCQly4cAGnT59GcXEx2tra4O3tbVJfbnTmzBk8//zziIuLQ2trKxYsWGDK6XZq4MCBUCqVBhNOGhoaDEYJ7wRcT5qIiEg+k4Ogp6en7tbo/fffj927dwMADh48CGtra5M7IITAnDlzsHXrVuTn53cbsrpy+fJlfPPNN3Bzc9OVRUVFGSwHc/z4cajVapPbtbOzg5ubG86dO4fc3FxMnjzZpL5cr7GxEdHR0fDz89O1vWXLFixcuNCUUzZgZWWF0NBQ5OXl6ZXn5eUhMjLyto7dG3QLSnMdQSIiInlMHWpcvHix+MMf/iCEEOL//u//hEqlEvfff7+wsrISixcvNnno8oUXXhCOjo5i7969oq6uTvdpbW0VQgjx17/+VYwbN05vn5deekns3btXVFVViQMHDohHHnlE2Nvbi++++05Xp7i4WKhUKvGHP/xBVFZWio0bN4p+/fqJDRs2GNWuEELs2rVL7Ny5U1RVVYndu3eLoKAgER4eLq5cuWJSXzpotVoRGhoqJk2aJC5fvqwrP3z4sHB2dhbvvPNOl9eppaVFlJWVibKyMgFAvPPOO6KsrEx8//33ujqbN28WlpaWYu3ataKiokIkJycLOzu7TvtirN66NXym+eqt4fuW5Ny8MhEREZnE2O/v214+5quvvhIrVqwQn3766S3tj6t3Bw0+69atE0JcXZ5FrVbr7fPkk08KNzc3YWlpKdzd3cXUqVPF0aNHDY69fft2ERAQIKytrcXQoUNFRkaG0e0KIURWVpYYPHiwsLKyEq6uriIpKUmcP3/+lvrSYffu3eLSpUsG5WVlZaK6urrL/QoKCjrtb0JCgl69VatWCbVaLaysrERISEi3y/AYo7eCYEPzT0K9OEeoFzMIEhER9TRjv78VQvDeHHWtubkZjo6OaGpqgoODQ48dt/HCZYS9uQcAcGr5JOkLWxMREd1NjP3+NvkZwes5ODh0uWYeUXcsrgt+/K8IERGRHEYHwR9++MGgjIOJdKuuH//jbxEREZEcRgfBgIAArF+/vjf7Qmbk+jvB/A8FERGRHEYHwWXLliEpKQmPPvqo7q0ZM2fO7NHnxsh8KK4bE2QMJCIiksPoIPjiiy/iX//6F86dO4dhw4Zh27ZtSE9Px8CBA3uzf3S30hsRlNcNIiIic6YypbK3tzfy8/OxcuVKPProo/Dz84NKpX+I0tLSHu0g3Z0srg+CHBMkIiKSwqQgCADff/89/vGPf8DJyQmTJ082CIJExlBw1jAREZF0JqW4NWvW4KWXXsL48eNx5MgR/OIXv+itftFdTm/WMIMgERGRFEYHwYkTJ6K4uBgrV67ErFmzerNPZAYUvDVMREQkndFBUKvV4vDhw/D09OzN/pCZ0Js1zBxIREQkhdFBMC8vrzf7QWZGf0SQiIiIZLitV8wR3SouKE1ERCQfgyBJcf2t4XbmQCIiIikYBEkKBV82TEREJB2DIEnRft0wYPF3Z6HlsCAREVGfYxCkPrfrSB3G/M9e3c//lVmCUW/lY9eROnmdIiIiMkMMgtSndh2pwwsbSlHf/JNeeX3TT3hhQynDIBERUR9iEKQ+o20XeG17RaePBHaUvba9greJiYiI+giDIPWZ4lM/oq7ppy63CwB1TT+h+NSPfdcpIiIiM8YgSH2moaXrEHgr9YiIiOj2MAhSn3Gxt+nRekRERHR7GASpz4R7O8HN0QaKLrYrALg52iDc26kvu0VERGS2GASpzygtFEiL8wcAgzDY8XNanD+UFl1FRSIiIupJDILUpyYGuCF9ZghcHfVv/7o62iB9ZggmBrhJ6hkREZH5UcnuAJmfiQFu+LW/KwLTdqG1rR0rngjClOEeHAkkIiLqYxwRJCmUFgpYWyoBAA94ODIEEhERScAgSNJ0hD+t4ALSREREMjAIkjQWiv8EQb5JhIiISAoGQZKmY0SwvV1yR4iIiMwUgyBJoxsR5K1hIiIiKRgESRqL//z2tTMIEhERScEgSNIoFR23hhkEiYiIZGAQJGksLDhZhIiISCYGQZJGyWcEiYiIpGIQJGk4a5iIiEguBkGShrOGiYiI5GIQJGk4a5iIiEguBkEzk5OTA19fX/j4+OCDDz6Q2hfOGiYiIpJLJbsD1Hc0Gg1SUlJQUFAABwcHhISEYOrUqXBycpLSH84aJiIikosjgmakuLgYw4YNg4eHB+zt7TFp0iTk5uZK649uRJC3homIiKSQHgSXL1+OESNGwN7eHi4uLpgyZQqOHTvW7T5Lly6FQqHQ+7i6uhrUq6mpwcyZM+Hs7Ix+/fph+PDhKCkpMbrdlpYWJCcnQ61Ww9bWFpGRkTh48GC356JQKJCcnGz6hbiJffv2IS4uDu7u7lAoFPjkk086rff+++/D29sbNjY2CA0NxRdffKHbVltbCw8PD93Pnp6eqKmp6fG+GuvaiKC0LhAREZk16UGwsLAQSUlJOHDgAPLy8qDRaBATE4OLFy92u9+wYcNQV1en+5SXl+ttP3fuHKKiomBpaYmdO3eioqICK1aswIABA4xud/bs2cjLy8P69etRXl6OmJgYjB8/vtPwdPDgQWRkZOCBBx7ott9FRUVoa2szKP/2229RX1/f5X4XL15EUFAQVq5c2WWdrKwsJCcnIzU1FWVlZRg9ejRiY2NRXV0NABCdjLwp/jMqJwPXESQiIpJM3GEaGhoEAFFYWNhlnbS0NBEUFNTtcRYvXixGjRp1y+22trYKpVIpcnJy9OoFBQWJ1NRUvbKWlhbh4+Mj8vLyxJgxY8T8+fM7bUOr1YqgoCDx2GOPCY1Goys/duyYcHV1FW+99ZZRfQUgsrOzDcrDw8NFYmKiXtnQoUPFkiVLhBBCFBUViSlTpui2zZs3T2zcuLHbtpqamgQA0dTUZFTfTDF9zVdCvThHZJf+0OPHJiIiMmfGfn9LHxG8UVNTEwDcdAJDZWUl3N3d4e3tjWnTpqGqqkpv+7Zt2xAWFobHH38cLi4uCA4Oxpo1a4xuV6PRQKvVwsbGRq+era0t9u/fr1eWlJSEhx9+GOPHj++2zxYWFtixYwfKysowa9YstLe34+TJkxg3bhzi4+Px8ssvd7t/d65cuYKSkhLExMTolcfExODLL78EAISHh+PIkSOoqalBS0sLduzYgQkTJnR6vFWrVsHf3x8jRoy45T7djAWfESQiIpLqjgqCQgikpKRg1KhRCAgI6LLeyJEjkZmZidzcXKxZswb19fWIjIzE2bNndXWqqqqQnp4OHx8f5ObmIjExEfPmzUNmZqZR7drb2yMiIgJvvPEGamtrodVqsWHDBnz99deoq6vT7bt582aUlpZi+fLlRp2ju7s78vPzUVRUhOnTp2PcuHGIjo7G6tWrjb1MnWpsbIRWq8WgQYP0ygcNGqS75axSqbBixQqMHTsWwcHBWLRoEZydnTs9XlJSEioqKrp9JvJ2KTlrmIiISKo7avmYOXPm4PDhwwYjbjeKjY3V/TkwMBAREREYMmQIPv74Y6SkpAAA2tvbERYWhmXLlgEAgoODcfToUaSnp2PWrFlGtbt+/Xo8++yz8PDwgFKpREhICKZPn47S0lIAwOnTpzF//nzs3r3bYOSwO15eXsjMzMSYMWMwePBgrF27tsee1bvxOEIIvbL4+HjEx8f3SFu3i7OGiYiI5LpjRgTnzp2Lbdu2oaCgAJ6enibta2dnh8DAQFRWVurK3Nzc4O/vr1fPz89PN3HCmHaHDBmCwsJCXLhwAadPn0ZxcTHa2trg7e0NACgpKUFDQwNCQ0OhUqmgUqlQWFiI9957DyqVClqtttP+njlzBs8//zzi4uLQ2tqKBQsWmHS+nRk4cCCUSqXBhJOGhgaDUcI7BWcNExERySU9CAohMGfOHGzduhX5+fm6kGWKy5cv45tvvoGbm5uuLCoqymA5mOPHj0OtVpvcrp2dHdzc3HDu3Dnk5uZi8uTJAIDo6GiUl5fj0KFDuk9YWBhmzJiBQ4cOQalUGhyrsbER0dHR8PPz07W9ZcsWLFy40OTzvp6VlRVCQ0ORl5enV56Xl4fIyMjbOnZv4axhIiIiuaTfGk5KSsKmTZvw6aefwt7eXjei5ejoCFtbW6xcuRLZ2dn4/PPPdfssXLgQcXFx8PLyQkNDA9588000NzcjISFBV2fBggWIjIzEsmXL8MQTT6C4uBgZGRnIyMgwql0AyM3NhRACvr6+OHHiBBYtWgRfX18888wzAK4+R3jjs4x2dnZwdnbu9BnH9vZ2TJw4EWq1GllZWVCpVPDz88OePXswduxYeHh4dDk6eOHCBZw4cUL386lTp3Do0CE4OTnBy8sLAJCSkoKnn34aYWFhiIiIQEZGBqqrq5GYmGjaX0of0b1rmM8IEhERydHr85dvAkCnn3Xr1gkhri4Vo1ar9fZ58sknhZubm7C0tBTu7u5i6tSp4ujRowbH3r59uwgICBDW1tZi6NChIiMjw+h2hRAiKytLDB48WFhZWQlXV1eRlJQkzp8/3+35dLd8jBBC7N69W1y6dMmgvKysTFRXV3e5X0FBQaf9TUhI0Ku3atUqoVarhZWVlQgJCel2GR5j9ObyMUkbS4R6cY74cH9Vjx+biIjInBn7/a0QgvflqGvNzc1wdHREU1MTHBwcevTY8zeX4dNDtfh/D/th9ujBPXpsIiIic2bs97f0ZwTJfHHWMBERkVwMgiQNZw0TERHJxSBI0vwnB3JEkIiISBIGQZKGbxYhIiKSi0GQpOl41zCDIBERkRwMgiRNx4ggJ64TERHJwSBI0ljwzSJERERSMQiSNErOGiYiIpKKQZCk4axhIiIiuRgESRoLzhomIiKSikGQpFFy1jAREZFUDIIkDWcNExERycUgSNJw1jAREZFcDIIkDWcNExERycUgSNLoZg3zGUEiIiIpGARJGt2sYd4aJiIikoJBkKTpmDXMEUEiIiI5GARJmo5nBLmgNBERkRwMgiTNtVnDkjtCRERkphgESRrdiCBvDRMREUnBIEjSdMwa5ptFiIiI5GAQJGk4a5iIiEguBkGShrOGiYiI5GIQJGk4IkhERCQXgyBJoxsRZA4kIiKSgkGQpOGsYSIiIrkYBEkaBWcNExERScUgSNIo+YwgERGRVAyCJA1nDRMREcnFIEjScNYwERGRXAyCJA1nDRMREcnFIEjScNYwERGRXAyCJA1nDRMREcnFIEjS6EYE+YwgERGRFAyCJE3HM4IcESQiIpKDQZCk4axhIiIiuRgEzUxOTg58fX3h4+ODDz74QGpfOFmEiIhILpXsDlDf0Wg0SElJQUFBARwcHBASEoKpU6fCyclJSn8suHwMERGRVBwRNCPFxcUYNmwYPDw8YG9vj0mTJiE3N1dafyw4a5iIiEgq6UFw+fLlGDFiBOzt7eHi4oIpU6bg2LFj3e6zdOlSKBQKvY+rq6tBvZqaGsycORPOzs7o168fhg8fjpKSEqPbbWlpQXJyMtRqNWxtbREZGYmDBw92ey4KhQLJycmmX4ib2LdvH+Li4uDu7g6FQoFPPvmk03rvv/8+vL29YWNjg9DQUHzxxRe6bbW1tfDw8ND97OnpiZqamh7vq7E4a5iIiEgu6UGwsLAQSUlJOHDgAPLy8qDRaBATE4OLFy92u9+wYcNQV1en+5SXl+ttP3fuHKKiomBpaYmdO3eioqICK1aswIABA4xud/bs2cjLy8P69etRXl6OmJgYjB8/vtPwdPDgQWRkZOCBBx7ott9FRUVoa2szKP/2229RX1/f5X4XL15EUFAQVq5c2WWdrKwsJCcnIzU1FWVlZRg9ejRiY2NRXV0NABCdBC5Fx2J+ElgYMWtY2y5QVNmIP+V+iz/lHkPRiUaOIBIREfUUcYdpaGgQAERhYWGXddLS0kRQUFC3x1m8eLEYNWrULbfb2toqlEqlyMnJ0asXFBQkUlNT9cpaWlqEj4+PyMvLE2PGjBHz58/vtA2tViuCgoLEY489JjQaja782LFjwtXVVbz11ltG9RWAyM7ONigPDw8XiYmJemVDhw4VS5YsEUIIUVRUJKZMmaLbNm/ePLFx48Zu22pqahIARFNTk1F9M0VFbZNQL84RoW/s7nT7zvJaEfRarlAvztH7BL2WK3aW1/Z4f4iIiO4Wxn5/Sx8RvFFTUxMA3HQCQ2VlJdzd3eHt7Y1p06ahqqpKb/u2bdsQFhaGxx9/HC4uLggODsaaNWuMblej0UCr1cLGxkavnq2tLfbv369XlpSUhIcffhjjx4/vts8WFhbYsWMHysrKMGvWLLS3t+PkyZMYN24c4uPj8fLLL3e7f3euXLmCkpISxMTE6JXHxMTgyy+/BACEh4fjyJEjqKmpQUtLC3bs2IEJEyZ0erxVq1bB398fI0aMuOU+3UzHreHORvh2HalD4oZSnG81HD0939qGxA2l2HWkrtf6RkREZA7uqCAohEBKSgpGjRqFgICALuuNHDkSmZmZyM3NxZo1a1BfX4/IyEicPXtWV6eqqgrp6enw8fFBbm4uEhMTMW/ePGRmZhrVrr29PSIiIvDGG2+gtrYWWq0WGzZswNdff426umsBZPPmzSgtLcXy5cuNOkd3d3fk5+ejqKgI06dPx7hx4xAdHY3Vq1cbe5k61djYCK1Wi0GDBumVDxo0SHfLWaVSYcWKFRg7diyCg4OxaNEiODs7d3q8pKQkVFRUdPtM5O3qatawtl1g6bajN93/te0VvE1MRER0G+6o5WPmzJmDw4cPG4y43Sg2Nlb358DAQERERGDIkCH4+OOPkZKSAgBob29HWFgYli1bBgAIDg7G0aNHkZ6ejlmzZhnV7vr16/Hss8/Cw8MDSqUSISEhmD59OkpLSwEAp0+fxvz587F7926DkcPueHl5ITMzE2PGjMHgwYOxdu3aHntW78bjCCH0yuLj4xEfH98jbd0u8Z9nFi+3afHVybMI93aC0kKB4lM/or758k33r2v6CcWnfkTEkM7DLBEREXXvjhkRnDt3LrZt24aCggJ4enqatK+dnR0CAwNRWVmpK3Nzc4O/v79ePT8/P93ECWPaHTJkCAoLC3HhwgWcPn0axcXFaGtrg7e3NwCgpKQEDQ0NCA0NhUqlgkqlQmFhId577z2oVCpotdpO+3vmzBk8//zziIuLQ2trKxYsWGDS+XZm4MCBUCqVBhNOGhoaDEYJ7wS7jtRh+pqvAQA/adrx1JoDGPVWPnYdqUNDy09GH8eUukRERKRPehAUQmDOnDnYunUr8vPzdSHLFJcvX8Y333wDNzc3XVlUVJTBcjDHjx+HWq02uV07Ozu4ubnh3LlzyM3NxeTJkwEA0dHRKC8vx6FDh3SfsLAwzJgxA4cOHYJSqTQ4VmNjI6Kjo+Hn56dre8uWLVi4cKHJ5309KysrhIaGIi8vT688Ly8PkZGRt3XsnrbrSB1e2FCKf1/QH/Wrb/oJL2woxXeN3c8Yv953ja093T0iIiKzIf3WcFJSEjZt2oRPP/0U9vb2uhEtR0dH2NraYuXKlcjOzsbnn3+u22fhwoWIi4uDl5cXGhoa8Oabb6K5uRkJCQm6OgsWLEBkZCSWLVuGJ554AsXFxcjIyEBGRoZR7QJAbm4uhBDw9fXFiRMnsGjRIvj6+uKZZ54BcPU5whufZbSzs4Ozs3Onzzi2t7dj4sSJUKvVyMrKgkqlgp+fH/bs2YOxY8fCw8Ojy9HBCxcu4MSJE7qfT506hUOHDsHJyQleXl4AgJSUFDz99NMICwtDREQEMjIyUF1djcTERNP+UnqRtl3gte0V6OzJPgFAAeBvxdUYZG+NMy03vz28+WA15oy7XzfxhIiIiIwnPQimp6cDAB566CG98nXr1uG3v/0tGhsbcfLkSb1tP/zwA5566ik0NjbiF7/4BR588EEcOHBAN9oHACNGjEB2djZeeeUVvP766/D29sZf/vIXzJgxw6h2gasziV955RX88MMPcHJywqOPPoo//OEPsLS0vKVztbCwwPLlyzF69GhYWVnpygMDA7Fnz54uJ24AwD//+U+MHTtW93PHs5AJCQn46KOPAABPPvkkzp49i9dffx11dXUICAjAjh079K6LbMWnfkRdU9e3cwWA+ubLWDD+l/jznuM3PR6fEyQiIrp1CiH4WgfqWnNzMxwdHdHU1AQHB4fbPt6nh2owf/Ohm9Z7d9pwHD59HmuLvjOq7uThHjetR0REZC6M/f6W/owgmRcXe+NmV7vY22C8v+FrA2/nmERERKRP+q1hMi/h3k5wc7RBfdNPnT4nqADg6miDcO+rC3v3t1bhwmVNp8e6sS4RERGZhiOC1KeUFgqkxV1d1ufG6R0dP6fF+UNpoYDSQoGRXYS8G+sSERGR6RgEqc9NDHBD+swQuDrq39J1dbRB+swQTAy4tgzQPXZXJ9XYW6tuWpeIiIhMw1vDJMXEADf82t8VDyzNxcUrWvzp8SD8JthDb3RP2y50awo+FuaBLQdP4+KVdiT+ajB83RzgaGsFbbvgiCAREdEtYhAkaZQWCthZq3DxihZ+bvZ6gW7XkTq8tr1Ct9TMuqLvdcPXq/dV6eq5OdogLc6fI4NERES3gLeGSSpry6u/gpc17bqyjjeP3LjeYDsMdbyNZNeRut7sJhER0V2JQZCkslZdfQ3f5barMa+7N490pqPea9sroG3nkphERESmYBAkqaxVHSOCWgA3f/NIZwSuvWGEiIiIjMcgSFJdC4JXRwQbWkwLgde7nX2JiIjMEYMgSaW7NfyfIHg7bwnhG0aIiIhMwyBIUukmi7RdvTXc8eaRW1kQJv/bMz3YMyIiorsfgyBJdeOt4evfPGKqtftP4Yqms7nFRERE1BkGQZLqxlvDwLU3jwzsb2XSsdoFsP6r73qye0RERHc1LihNUtn859bwpTYNvjp5Fg0tP8HF3ga/9neFk501nvjfr0w63vc/tvZGN4mIiO5KDIIkVceI4Oq9Vbhw+biu3M3RBo+HeZp8vNbLmh7rGxER0d2Ot4ZJqtrzlwAAF24IcPVNP+G9z08AACyVxk8d+UdpDd8yQkREZCQGQZJG2y5w4NTZTrdd/46QIb+wM2kWMd8yQkREZBzeGiZpik/9iIuXtTetZ29jifSZIXhte8VN3zrS8ZaRmWu+gkKhwE8aLWxUSgzsbw3FDWlSCIHGC1e6rWNsvZ6qc6ce625v7+fcd16rO7e9n3Pfea365lgWFgp43GOLyCED8eBgZygtbmXxtNvDIEjSmPImkIkBbvi1vyv+nHccKwtO3LT+V6fO3U7XiIiI+syqgpMY0M8Sf5waiIkBbn3aNm8NkzTGvglkgO3VZWSUFgpE3T+wN7tEREQkxfnWNiRuKO3z59wZBEmacG8nONpa3rTefb/op7ePmyNfJUdERHenvn7OnUGQpFFaKPCbYPdOt13/lIS99bWweDtvHiEiIrrT1TX9hOJTP/ZZewyCJFWol1On5YMcrOE7qD+Aq88SXv+/o4kBbnh32vC+6B4REVGfM+UZ+tvFIEjS7DpShyVbDxuUW6ss8JOmHcfOXAAAbDhQjVFv5euem9h1pA5/3Pltn/aViIiorxj7DH1P4KxhkmLXkTokbijtdNtlTbveu4eBqwtMv7ChFM//yhsZ+06BqwQSEdHdyM3RBuHend8t6w0MgtTntO0CS7cdNWmfjuC35guGQCIiunulxfn36XqCvDVMfa741I+ob758S/vyhSFERHQ3uqefJVbPDOnzdQQ5Ikh9ri8fglU72cJjgO0dsfr8z2m1e3Nr7+fcd16rO7e9n3Pfea365lh8swiZpb58CPaPjwYhYohzn7VHRET0c8Jbw9Tnwr2d4OpgfUv7mvKfpb5+4JaIiOjnhkGQ+pzSQoGl8cNM2kfxn89/jfaGsVmwrx+4JSIi+rlhECQpJga4YfXMEAzoZ/iKuX5WSoNyV0cbpM8MwSuT/JE+M6Tb18zJeuCWiIjo50YhhOA8TOpSc3MzHB0d0dTUBAcHhx4/vrZd4MDJs/iqqhGrCk5CANi/eCzu6WeFYWm5AIAPE8IwxtdFb3RP2y6uzj5uuoTGC5dx/lIbFFAgYoiztAduiYiI7hTGfn9zsghJpbRQIMpnIKJ8BuLDou/QekULbbuA5rp1Ykb5/MIg2CktFJwEQkREdJt4a5juGFaqq7+OVzTtuKzRAgAUCsBSydE9IiKi3sAgaGZycnLg6+sLHx8ffPDBB7K7o8dK+Z8gqG3H5barr5izVllA0dmiTURERHTbeGvYjGg0GqSkpKCgoAAODg4ICQnB1KlT4eR0Zyyxoj8i2BEElTK7REREdFfjiKAZKS4uxrBhw+Dh4QF7e3tMmjQJubm5srul09mtYWsVf0WJiIh6i/Rv2eXLl2PEiBGwt7eHi4sLpkyZgmPHjnW7z9KlS6FQKPQ+rq6uBvVqamowc+ZMODs7o1+/fhg+fDhKSkqMbrelpQXJyclQq9WwtbVFZGQkDh482O25KBQKJCcnm34hbmLfvn2Ii4uDu7s7FAoFPvnkk07rvf/++/D29oaNjQ1CQ0PxxRdf6LbV1tbCw8ND97Onpydqamp6vK+3Su/WcMeIoKX0X1EiIqK7lvRv2cLCQiQlJeHAgQPIy8uDRqNBTEwMLl682O1+w4YNQ11dne5TXl6ut/3cuXOIioqCpaUldu7ciYqKCqxYsQIDBgwwut3Zs2cjLy8P69evR3l5OWJiYjB+/PhOw9PBgweRkZGBBx54oNt+FxUVoa2tzaD822+/RX19fZf7Xbx4EUFBQVi5cmWXdbKyspCcnIzU1FSUlZVh9OjRiI2NRXV1NYCr7zm80Z30/J319SOCbbw1TERE1OvEHaahoUEAEIWFhV3WSUtLE0FBQd0eZ/HixWLUqFG33G5ra6tQKpUiJydHr15QUJBITU3VK2tpaRE+Pj4iLy9PjBkzRsyfP7/TNrRarQgKChKPPfaY0Gg0uvJjx44JV1dX8dZbbxnVVwAiOzvboDw8PFwkJibqlQ0dOlQsWbJECCFEUVGRmDJlim7bvHnzxMaNG7ttq6mpSQAQTU1NRvXtdjyWXiTUi3PEjsO1ouDbM0K9OEdMendfr7dLRER0tzH2+1v6iOCNmpqaAOCmExgqKyvh7u4Ob29vTJs2DVVVVXrbt23bhrCwMDz++ONwcXFBcHAw1qxZY3S7Go0GWq0WNjb6b7CwtbXF/v379cqSkpLw8MMPY/z48d322cLCAjt27EBZWRlmzZqF9vZ2nDx5EuPGjUN8fDxefvnlbvfvzpUrV1BSUoKYmBi98piYGHz55ZcAgPDwcBw5cgQ1NTVoaWnBjh07MGHChE6Pt2rVKvj7+2PEiBG33CdTWXZya9jGkiOCREREveWOCoJCCKSkpGDUqFEICAjost7IkSORmZmJ3NxcrFmzBvX19YiMjMTZs2d1daqqqpCeng4fHx/k5uYiMTER8+bNQ2ZmplHt2tvbIyIiAm+88QZqa2uh1WqxYcMGfP3116irq9Ptu3nzZpSWlmL58uVGnaO7uzvy8/NRVFSE6dOnY9y4cYiOjsbq1auNvUydamxshFarxaBBg/TKBw0apLvlrFKpsGLFCowdOxbBwcFYtGgRnJ07X5Q5KSkJFRUV3T4T2dM6Jotc1ps1fEf9ihIREd1V7qjlY+bMmYPDhw8bjLjdKDY2VvfnwMBAREREYMiQIfj444+RkpICAGhvb0dYWBiWLVsGAAgODsbRo0eRnp6OWbNmGdXu+vXr8eyzz8LDwwNKpRIhISGYPn06SktLAQCnT5/G/PnzsXv3boORw+54eXkhMzMTY8aMweDBg7F27doee1bvxuMIIfTK4uPjER8f3yNt9bSOySJt2nZ09JhBkIiIqPfcMd+yc+fOxbZt21BQUABPT0+T9rWzs0NgYCAqKyt1ZW5ubvD399er5+fnp5s4YUy7Q4YMQWFhIS5cuIDTp0+juLgYbW1t8Pb2BgCUlJSgoaEBoaGhUKlUUKlUKCwsxHvvvQeVSgWtVttpf8+cOYPnn38ecXFxaG1txYIFC0w6384MHDgQSqXSYMJJQ0ODwSjhnYrrCBIREfUt6UFQCIE5c+Zg69atyM/P14UsU1y+fBnffPMN3NzcdGVRUVEGy8EcP34carXa5Hbt7Ozg5uaGc+fOITc3F5MnTwYAREdHo7y8HIcOHdJ9wsLCMGPGDBw6dAhKpWGIaWxsRHR0NPz8/HRtb9myBQsXLjT5vK9nZWWF0NBQ5OXl6ZXn5eUhMjLyto7dVzoNglw+hoiIqNdIvzWclJSETZs24dNPP4W9vb1uRMvR0RG2trZYuXIlsrOz8fnnn+v2WbhwIeLi4uDl5YWGhga8+eabaG5uRkJCgq7OggULEBkZiWXLluGJJ55AcXExMjIykJGRYVS7AJCbmwshBHx9fXHixAksWrQIvr6+eOaZZwBcfY7wxmcZ7ezs4Ozs3Okzju3t7Zg4cSLUajWysrKgUqng5+eHPXv2YOzYsfDw8OhydPDChQs4ceKE7udTp07h0KFDcHJygpeXFwAgJSUFTz/9NMLCwhAREYGMjAxUV1cjMTHRtL8USTreKfyv0+fhYGsJgLeGiYiIelWvz1++CQCdftatWyeEuLpUjFqt1tvnySefFG5ubsLS0lK4u7uLqVOniqNHjxoce/v27SIgIEBYW1uLoUOHioyMDKPbFUKIrKwsMXjwYGFlZSVcXV1FUlKSOH/+fLfn093yMUIIsXv3bnHp0iWD8rKyMlFdXd3lfgUFBZ32NyEhQa/eqlWrhFqtFlZWViIkJKTbZXiM0VfLx+wsrxV+v9sp1Itz9D6z1n7dq+0SERHdjYz9/lYI0ckqw0T/0dzcDEdHRzQ1NcHBwaFX2th1pA4vbChFV7+Iq2eGYGKAWxdbiYiI6EbGfn/zvhtJpW0XeG17RZchEABe214BbTv/v0JERNTTGARJquJTP6Ku6adu69Q1/YTiUz/2UY+IiIjMB4MgSdXQ0n0INLUeERERGY9BkKRysTduIe7vGlt7uSdERETmh0GQpAr3dsKAfpY3rffnPcex60jdTesRERGR8RgESTpjJ65z0ggREVHPYhAkqYpP/YimSxqj6nLSCBERUc9iECSpTJ0EwkkjREREPYdBkKQydrLIrdYnIiKirjEIklTh3k5wdbA2qq6bow3CvZ16uUdERETmg0GQpFJaKLA0fphRddPi/KG0UPRyj4iIiMwHgyBJNzHADatnhnS5jMw9/Sz5vmEiIqJeoJLdASLgahj8tb8rDpw8i6KT/0bt+Z/gcY8tIocMxIODnTkSSERE1AsYBOmOobRQIMpnIKJ8BsruChERkVngrWEiIiIiM8UgSERERGSmGASJiIiIzBSDIBEREZGZYhAkIiIiMlMMgkRERERmikGQiIiIyEwxCBIRERGZKQZBIiIiIjPFN4tQt4QQAIDm5mbJPSEiIiJjdXxvd3yPd4VBkLrV0tICALj33nsl94SIiIhM1dLSAkdHxy63K8TNoiKZtfb2dtTW1sLe3h4KhaLHjtvc3Ix7770Xp0+fhoODQ48dlwzxWvcNXue+wevcd3it+0ZvXWchBFpaWuDu7g4Li66fBOSIIHXLwsICnp6evXZ8BwcH/gPTR3it+wavc9/gde47vNZ9ozeuc3cjgR04WYSIiIjITDEIEhEREZkpBkGSwtraGmlpabC2tpbdlbser3Xf4HXuG7zOfYfXum/Ivs6cLEJERERkpjgiSERERGSmGASJiIiIzBSDIBEREZGZYhAkIiIiMlMMgiTF+++/D29vb9jY2CA0NBRffPGF7C79rOzbtw9xcXFwd3eHQqHAJ598orddCIGlS5fC3d0dtra2eOihh3D06FG9OpcvX8bcuXMxcOBA2NnZIT4+Hj/88EMfnsWdb/ny5RgxYgTs7e3h4uKCKVOm4NixY3p1eK1vX3p6Oh544AHdgroRERHYuXOnbjuvce9Yvnw5FAoFkpOTdWW81j1j6dKlUCgUeh9XV1fd9jvpOjMIUp/LyspCcnIyUlNTUVZWhtGjRyM2NhbV1dWyu/azcfHiRQQFBWHlypWdbn/77bfxzjvvYOXKlTh48CBcXV3x61//WvfuaABITk5GdnY2Nm/ejP379+PChQt45JFHoNVq++o07niFhYVISkrCgQMHkJeXB41Gg5iYGFy8eFFXh9f69nl6euKPf/wj/vnPf+Kf//wnxo0bh8mTJ+u+GHmNe97BgweRkZGBBx54QK+c17rnDBs2DHV1dbpPeXm5btsddZ0FUR8LDw8XiYmJemVDhw4VS5YskdSjnzcAIjs7W/dze3u7cHV1FX/84x91ZT/99JNwdHQUq1evFkIIcf78eWFpaSk2b96sq1NTUyMsLCzErl27+qzvPzcNDQ0CgCgsLBRC8Fr3pnvuuUd88MEHvMa9oKWlRfj4+Ii8vDwxZswYMX/+fCEEf597UlpamggKCup02512nTkiSH3qypUrKCkpQUxMjF55TEwMvvzyS0m9urucOnUK9fX1etfY2toaY8aM0V3jkpIStLW16dVxd3dHQEAA/x660dTUBABwcnICwGvdG7RaLTZv3oyLFy8iIiKC17gXJCUl4eGHH8b48eP1ynmte1ZlZSXc3d3h7e2NadOmoaqqCsCdd51VPXo0optobGyEVqvFoEGD9MoHDRqE+vp6Sb26u3Rcx86u8ffff6+rY2VlhXvuucegDv8eOieEQEpKCkaNGoWAgAAAvNY9qby8HBEREfjpp5/Qv39/ZGdnw9/fX/elx2vcMzZv3ozS0lIcPHjQYBt/n3vOyJEjkZmZiV/+8pc4c+YM3nzzTURGRuLo0aN33HVmECQpFAqF3s9CCIMyuj23co3599C1OXPm4PDhw9i/f7/BNl7r2+fr64tDhw7h/Pnz+Mc//oGEhAQUFhbqtvMa377Tp09j/vz52L17N2xsbLqsx2t9+2JjY3V/DgwMREREBIYMGYKPP/4YDz74IIA75zrz1jD1qYEDB0KpVBr8j6ahocHgf0d0azpmpnV3jV1dXXHlyhWcO3euyzp0zdy5c7Ft2zYUFBTA09NTV85r3XOsrKxw//33IywsDMuXL0dQUBDeffddXuMeVFJSgoaGBoSGhkKlUkGlUqGwsBDvvfceVCqV7lrxWvc8Ozs7BAYGorKy8o77nWYQpD5lZWWF0NBQ5OXl6ZXn5eUhMjJSUq/uLt7e3nB1ddW7xleuXEFhYaHuGoeGhsLS0lKvTl1dHY4cOcK/h+sIITBnzhxs3boV+fn58Pb21tvOa917hBC4fPkyr3EPio6ORnl5OQ4dOqT7hIWFYcaMGTh06BAGDx7Ma91LLl++jG+++QZubm533u90j049ITLC5s2bhaWlpVi7dq2oqKgQycnJws7OTnz33Xeyu/az0dLSIsrKykRZWZkAIN555x1RVlYmvv/+eyGEEH/84x+Fo6Oj2Lp1qygvLxdPPfWUcHNzE83NzbpjJCYmCk9PT7Fnzx5RWloqxo0bJ4KCgoRGo5F1WnecF154QTg6Ooq9e/eKuro63ae1tVVXh9f69r3yyiti37594tSpU+Lw4cPi1VdfFRYWFmL37t1CCF7j3nT9rGEheK17yksvvST27t0rqqqqxIEDB8Qjjzwi7O3tdd9zd9J1ZhAkKVatWiXUarWwsrISISEhuuU4yDgFBQUCgMEnISFBCHF1eYK0tDTh6uoqrK2txa9+9StRXl6ud4xLly6JOXPmCCcnJ2FrayseeeQRUV1dLeFs7lydXWMAYt26dbo6vNa379lnn9X9e/CLX/xCREdH60KgELzGvenGIMhr3TOefPJJ4ebmJiwtLYW7u7uYOnWqOHr0qG77nXSdFUII0bNjjERERET0c8BnBImIiIjMFIMgERERkZliECQiIiIyUwyCRERERGaKQZCIiIjITDEIEhEREZkpBkEiIiIiM8UgSERE3dq7dy8UCgXOnz8vuytE1MMYBImIfia0Wi0iIyPx6KOP6pU3NTXh3nvvxf/7f/+vV9qNjIxEXV0dHB0de+X4RCQP3yxCRPQzUllZieHDhyMjIwMzZswAAMyaNQv/+te/cPDgQVhZWUnuIRH9nHBEkIjoZ8THxwfLly/H3LlzUVtbi08//RSbN2/Gxx9/3GUIXLx4MX75y1+iX79+GDx4MH73u9+hra0NACCEwPjx4zFx4kR0jAucP38eXl5eSE1NBWB4a/j7779HXFwc7rnnHtjZ2WHYsGHYsWNH7588EfU4lewOEBGRaebOnYvs7GzMmjUL5eXl+P3vf4/hw4d3Wd/e3h4fffQR3N3dUV5ejv/6r/+Cvb09Xn75ZSgUCnz88ccIDAzEe++9h/nz5yMxMRGDBg3C0qVLOz1eUlISrly5gn379sHOzg4VFRXo379/75wsEfUq3homIvoZ+vbbb+Hn54fAwECUlpZCpTL+//X/8z//g6ysLPzzn//Ulf3f//0fnn76aaSkpODdd99FWVkZfvnLXwK4OiI4duxYnDt3DgMGDMADDzyARx99FGlpaT1+XkTUt3hrmIjoZ+jDDz9Ev379cOrUKfzwww8AgMTERPTv31/36fD3v/8do0aNgqurK/r374/f/e53qK6u1jve448/jqlTp2L58uVYsWKFLgR2Zt68eXjzzTcRFRWFtLQ0HD58uHdOkoh6HYMgEdHPzFdffYU///nP+PTTTxEREYHnnnsOQgi8/vrrOHTokO4DAAcOHMC0adMQGxuLnJwclJWVITU1FVeuXNE7ZmtrK0pKSqBUKlFZWdlt+7Nnz0ZVVRWefvpplJeXIywsDH/9619763SJqBcxCBIR/YxcunQJCQkJ+O///m+MHz8eH3zwAQ4ePIj//d//hYuLC+6//37dBwCKioqgVquRmpqKsLAw+Pj44Pvvvzc47ksvvQQLCwvs3LkT7733HvLz87vtx7333ovExERs3boVL730EtasWdMr50tEvYtBkIjoZ2TJkiVob2/HW2+9BQDw8vLCihUrsGjRInz33XcG9e+//35UV1dj8+bNOHnyJN577z1kZ2fr1fnss8/w4YcfYuPGjfj1r3+NJUuWICEhAefOneu0D8nJycjNzcWpU6dQWlqK/Px8+Pn59fi5ElHv42QRIqKficLCQkRHR2Pv3r0YNWqU3rYJEyZAo9Fgz549UCgUettefvllfPjhh7h8+TIefvhhPPjgg1i6dCnOnz+Pf//73wgMDMT8+fPxyiuvAAA0Gg2ioqJw3333ISsry2CyyNy5c7Fz50788MMPcHBwwMSJE/HnP/8Zzs7OfXYtiKhnMAgSERERmSneGiYiIiIyUwyCRERERGaKQZCIiIjITDEIEhEREZkpBkEiIiIiM8UgSERERGSmGASJiIiIzBSDIBEREZGZYhAkIiIiMlMMgkRERERmikGQiIiIyEwxCBIRERGZqf8P5CqB/cy8hcYAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learning_rates = [0.000001]\n",
    " \n",
    "\n",
    "for idx,lr in enumerate (learning_rates):   \n",
    "      \n",
    "      print('')\n",
    "      print('-------------------------------------Learning Rate',lr,'-----------------------------------------')\n",
    "      lsr_tensor_SGD_2 = copy.deepcopy(factor_core_iterate_SGD[-1])\n",
    "      learning_rate = lr\n",
    "      epochs = 500\n",
    "      batch_size = 650\n",
    "\n",
    "      momentum = 0\n",
    "      nesterov = False\n",
    "      decay_factor = 0\n",
    "      hypers = {'max_iter': 1, 'threshold': 1e-4, 'ranks': tuple(tensor_mode_ranks), 'separation_rank': separation_rank,'learning_rate':learning_rate,'epochs':epochs,'batch_size': batch_size, 'momentum':momentum, 'nesterov': nesterov, 'decay_factor': decay_factor}\n",
    "\n",
    "      normalized_estimation_error_SGD_2, test_nmse_loss_SGD_2, test_R2_loss_SGD_2, test_correlation_SGD_2, objective_function_values_SGD_2,gradient_values_SGD_2,iterate_differences_SGD_2,epoch_level_gradients_SGD_2,epoch_level_function_2,tensor_iteration_SGD_2,factor_core_iterate_SGD_2 = train_test_sgd(X_train, Y_train, X_test, Y_test, lambda1, hypers, Y_train_mean,lsr_tensor_SGD_2,B_tensored,intercept= False, Initializer = lsr_tensor_SGD_2.core_tensor)\n",
    "\n",
    "    \n",
    "      #Get current time and store in variable\n",
    "      formatted_time = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "      max_iter = hypers['max_iter']\n",
    "      \n",
    "      if platform.system() ==  'Windows':\n",
    "        pkl_file = rf\"D:\\Tensor Based ML for Neuro Imaging\\INSPIRE_CAHBHIR\\Python Scripts\\LSR-Tensor-Ridge-Regression_All_Data\\Platform_For_Experimenmts\\With_New_Dataset\\Core_Tensor_Isolated_Experiments\\SGD_level2_learning_rate_{learning_rate}_batch_size_{batch_size}_decay_{decay_factor}_intercept5_ExecutionTime{formatted_time}_n_train_{n_train}_n_test_{n_test}_tensor_dimension_{tensor_dimensions}_tensor_mode_ranks_{tensor_mode_ranks}_separation_rank_{separation_rank}_max_iter_{max_iter}.pkl\" \n",
    "      elif platform.system() == 'Darwin': \n",
    "        pkl_file = f\"/Users/lakrama/Neuro Project Codes/LSR-Tensor-Ridge-Regression_All_Data/Platforms for Experments/Data/Experiments_Isolating_Sub_Problems/Final lr search/After Indentifinig Batch Size Issue/Gradient Descent/SGD_learning_rate_{learning_rate}_batch_size_{batch_size}_decay_{decay_factor}_intercept5_,ExecutionTime{formatted_time}, n_train_{n_train},n_test_{n_test}, tensor_dimensions:{tensor_dimensions}, tensor_mode_= ranks:{tensor_mode_ranks}, separation_rank:{separation_rank}, max_iter={max_iter}.pkl\"\n",
    "      \n",
    "      #with open(pkl_file, \"wb\") as file:\n",
    "       #dill.dump((lsr_tensor_SGD_2,lambda1, normalized_estimation_error_SGD_2, test_nmse_loss_SGD_2, test_R2_loss_SGD_2, test_correlation_SGD_2, objective_function_values_SGD_2,gradient_values_SGD_2,iterate_differences_SGD_2,epoch_level_gradients_SGD_2,epoch_level_function_2,tensor_iteration_SGD_2,factor_core_iterate_SGD_2), file)\n",
    "\n",
    "\n",
    "      print(\"Error Report on Testing _ With best Lambda\")\n",
    "      print(\"SGD_Alpha chosen for model: \", lambda1)\n",
    "      print(\"SGD_Test Normalized Estimation Error: \", normalized_estimation_error_SGD_2)\n",
    "      print(\"SGD_Test NMSE Loss: \", test_nmse_loss_SGD_2)\n",
    "      print(\"SGD_Test R2 Loss: \", test_R2_loss_SGD_2)\n",
    "      print(\"SGD_Test Correlation: \", test_correlation_SGD_2)\n",
    "      print(\"Objective Function Values\", objective_function_values_SGD_2[0,1,2])\n",
    "\n",
    "      # Looking at the  variation of function values within a BCD iteration\n",
    "\n",
    "      import matplotlib.pyplot as plt\n",
    "\n",
    "      # Create a line plot\n",
    "      plt.plot(epoch_level_gradients_SGD_2[0,1,2,:], marker='o')\n",
    "\n",
    "      # Add title and labels\n",
    "      plt.title('Gradient Value')\n",
    "      plt.xlabel('X-axis')\n",
    "      plt.yscale('log')\n",
    "      plt.ylabel('Y-axis')\n",
    "\n",
    "      plt.show()\n",
    "\n",
    "      print(f'final gradient: {epoch_level_gradients_SGD_2[0,1,2,-1]}')\n",
    "      # fucnion value\n",
    "\n",
    "      # Create a line plot\n",
    "      plt.plot(epoch_level_function_2[0,1,2,:], marker='o')\n",
    "\n",
    "      # Add title and labels\n",
    "      plt.title('Function Value')\n",
    "      plt.xlabel('X-axis')\n",
    "      plt.yscale('log') \n",
    "      plt.ylabel('Y-axis')\n",
    "\n",
    "      plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.34503602  0.48104816 -0.26876711  0.30533446]\n",
      " [-0.14360852 -0.12148134 -0.14583721 -0.15483982]\n",
      " [-0.04785435  0.29882323 -0.05810391  0.18266948]\n",
      " [ 0.01659465 -0.04146101 -0.52261706  0.0006165 ]]\n",
      "Norm: 0.9999999999999999\n",
      "----------------\n",
      "[[-0.34500456  0.4810287  -0.26874205  0.3053034 ]\n",
      " [-0.14359626 -0.12143176 -0.14576909 -0.15488222]\n",
      " [-0.04789685  0.29880574 -0.05810547  0.18265766]\n",
      " [ 0.01658146 -0.04145989 -0.52263534  0.00063773]]\n",
      "Norm: 0.9999564\n",
      "[[[7.25543036e+05 6.13337698e+05 9.49653161e+00]\n",
      "  [2.89877520e+05 5.46632335e+05 9.49653161e+00]]]\n",
      "Least Squares Term 0.25115467802780556\n",
      "Ridge Term 2.4998593350170495\n",
      "Least Squares Term Gradient 5.009171626757617\n",
      "Ridge Term_Gradient:4.99985933303833\n",
      "Gradient Norm:0.02125062689360581\n",
      "[[[[ 4.74502732e-03]\n",
      "   [ 3.08778630e-03]\n",
      "   [-2.88358155e-03]\n",
      "   [-1.72867276e-03]\n",
      "   [-1.68984504e-03]\n",
      "   [-1.06908997e-03]\n",
      "   [-1.67059492e-02]\n",
      "   [ 7.55397550e-04]\n",
      "   [-3.32827633e-03]\n",
      "   [ 7.72380453e-04]\n",
      "   [-3.36917269e-03]\n",
      "   [ 3.05216723e-03]\n",
      "   [-7.30805737e-03]\n",
      "   [-5.91436728e-03]\n",
      "   [ 1.83388067e-03]\n",
      "   [-6.97554872e-04]]\n",
      "\n",
      "  [[-1.72978952e+00]\n",
      "   [-7.21090463e-01]\n",
      "   [-2.36600311e-01]\n",
      "   [ 8.46482629e-02]\n",
      "   [ 2.40686005e+00]\n",
      "   [-6.06113830e-01]\n",
      "   [ 1.51076313e+00]\n",
      "   [-2.08064018e-01]\n",
      "   [-1.34039466e+00]\n",
      "   [-7.29625249e-01]\n",
      "   [-2.87145118e-01]\n",
      "   [-2.61627453e+00]\n",
      "   [ 1.53385346e+00]\n",
      "   [-7.68511020e-01]\n",
      "   [ 9.11471700e-01]\n",
      "   [ 3.88910746e-03]]\n",
      "\n",
      "  [[-1.72504449e+00]\n",
      "   [-7.18002677e-01]\n",
      "   [-2.39483893e-01]\n",
      "   [ 8.29195902e-02]\n",
      "   [ 2.40517020e+00]\n",
      "   [-6.07182920e-01]\n",
      "   [ 1.49405718e+00]\n",
      "   [-2.07308620e-01]\n",
      "   [-1.34372294e+00]\n",
      "   [-7.28852868e-01]\n",
      "   [-2.90514290e-01]\n",
      "   [-2.61322236e+00]\n",
      "   [ 1.52654541e+00]\n",
      "   [-7.74425387e-01]\n",
      "   [ 9.13305581e-01]\n",
      "   [ 3.19155259e-03]]]]\n"
     ]
    }
   ],
   "source": [
    "print(G1)\n",
    "print('Norm:', np.linalg.norm(G1,'fro'))\n",
    "print('----------------')\n",
    "print(lsr_tensor_SGD.core_tensor)\n",
    "print('Norm:', np.linalg.norm(lsr_tensor_SGD.core_tensor,'fro'))\n",
    "\n",
    "print(gradient_values_SGD)\n",
    "\n",
    "#let's look at each component of LSR\n",
    "\n",
    "X_tilde,y_tilde = lsr_tensor_SGD_2.bcd_core_update_x_y(X_train,Y_train)\n",
    "\n",
    "g = np.reshape(lsr_tensor_SGD_2.core_tensor, (-1, 1), order = 'F') #Flatten Core Matrix Column Wise\n",
    "Omega = X_tilde\n",
    "\n",
    "print(f'Least Squares Term {np.linalg.norm(y_tilde.reshape(-1,1) - Omega @ g)}')\n",
    "print(f'Ridge Term {lambda1*(np.linalg.norm(lsr_tensor_SGD_2.core_tensor,'fro'))**2}')\n",
    "print(f'Least Squares Term Gradient {np.linalg.norm((-2 * Omega.T) @ (y_tilde.reshape(-1,1) - Omega @ g))}')\n",
    "print(f'Ridge Term_Gradient:{np.linalg.norm(2 * lambda1 * g)}')\n",
    "print(f'Gradient Norm:{np.linalg.norm((-2 * Omega.T) @ (y_tilde.reshape(-1,1) - Omega @ g) + (2 * lambda1 * g))}')\n",
    "gradient_value = (-2 * Omega.T) @ (y_tilde.reshape(-1,1) - Omega @ g) + (2 * lambda1 * g)\n",
    "\n",
    "A = np.array([[gradient_value,(2 * Omega.T) @ (y_tilde.reshape(-1,1)- Omega @ g),(2 * lambda1 * g)]])\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjoAAAHFCAYAAAD7ZFORAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAABSdklEQVR4nO3deVhTV+I+8DdsYSlGEYGgiEjdEMWKe92wdUGlX3W0q4pL609cWmudWsexaJeh1o7TDbHaqu1Yq+1UrVYHpe7WfaGKuLZYbA1FUYMbWzi/P5ikhiQQQpIbkvfzPHnanHu5OfcSktdzzyITQggQEREROSE3qStAREREZCsMOkREROS0GHSIiIjIaTHoEBERkdNi0CEiIiKnxaBDRERETotBh4iIiJwWgw4RERE5LQYdIiIicloMOhIaPnw4fHx8cOvWLZP7PPfcc/D09MQff/xR69e7fPkyZDIZVq1aVeOfvXr1KubPn4/MzEyDbfPnz4dMJqt1/WyppKQEkydPhlKphLu7Ozp06FDtz2zevBkJCQkIDg6Gl5cXAgIC8Nhjj+HLL79EaWmp7StdDe11r+7Rt29fqatqNaWlpfjkk0/QuXNnBAQEwNfXF+Hh4fi///s/bNiwwaJj9u3bF9HR0RbXqW/fvnrX28fHBzExMXj//fdRXl5u8XErW7VqFWQyGY4dO1btvuPGjUOzZs2s9tq1pf3sMfbo1KmTpHU7cOAA5s+fb/RzuG/fvnb/+/nggw8gk8mQnp5ucp/ly5dDJpNh/fr1Zh9XinNxFB5SV8CVTZw4ERs3bsSaNWswZcoUg+1qtRobNmzA0KFDERwcXOvXUyqVOHjwICIjI2v8s1evXsWCBQvQrFkzg5Dw/PPPY9CgQbWuny2lpaXhk08+wUcffYTY2Fg89NBDJvcVQmDChAlYtWoVBg8ejMWLFyMsLAxqtRq7du3ClClTcP36dbz00kt2PANDla+7SqXCiBEjMH36dDz77LO68nr16klRPZsYM2YM1q9fjxkzZmDBggWQy+X45ZdfkJ6ejm3btmH48OGS1Kt58+b48ssvAQD5+flYunQpXn75ZahUKixcuFCSOjmiyu9NAFX+LdrDgQMHsGDBAowbNw7169fX27ZkyRK712f06NGYPXs2VqxYYfJzdeXKlWjUqBESEhLsXLs6SpBkysrKRGhoqIiNjTW6PS0tTQAQmzdvrvXrFBUV1eoYR48eFQDEypUra3UcqTz//PPCx8fHrH0XLlwoAIgFCxYY3a5SqcS+fftqXafy8nJx7969Wh9HKycnRwAQixYtstox7a2qa/LLL78IAOL11183ul2j0Vj0mn369BFt27a16GdN/XxJSYlo3ry58PX1FSUlJUZ/rqa//5UrVwoA4ujRo9Xum5iYKMLDw80+tq058ntz0aJFAoDIycmRuio6Tz75pPDy8hLXr1832Hb27FkBQLzyyis1OmafPn1Enz59rFTDuoW3riTk7u6OxMREHD9+HKdPnzbYvnLlSiiVSsTHx+PatWuYMmUKoqKi8NBDDyEoKAj9+vXDvn379H5G20T87rvv4q233kJERATkcjl27dpl9NbVpUuXMH78eLRo0QK+vr5o3LgxEhIS9Oqze/dudO7cGQAwfvx4XZPz/PnzARi/dVVeXo53330XrVu3hlwuR1BQEMaOHYvffvtNbz/tbYOjR4+iV69e8PX1RfPmzfHOO++Y1exfVFSEOXPmICIiAl5eXmjcuDGmTp2q1wwtk8nw6aef4v79+7q6m7p9V1paioULF6J169aYN2+e0X1CQkLQs2dP3fMbN25gypQpaNy4Mby8vNC8eXPMnTsXxcXFej8nk8kwbdo0LF26FG3atIFcLsfnn38OALh48SKeffZZBAUFQS6Xo02bNkhNTa32/M1x7NgxPPHEEwgICIC3tzceeeQRfP3113r7aG+L7Nq1C0lJSQgMDETDhg0xYsQIXL16VW/fnTt3om/fvmjYsCF8fHzQtGlT/OUvf8G9e/esek0qKygoAFDRMmmMm9ufH2fa87l8+bLePrt374ZMJsPu3bsNfn7fvn3o1q0bfHx80LhxY8ybNw8ajcboa1XH09MTsbGxuHfvHq5duwag6nPdv38/HnvsMfj7+8PX1xc9evTAli1bjB775s2bGD9+PAICAuDn54eEhAT88ssv1dZJCIElS5agQ4cO8PHxQYMGDTBy5EiDn9X+TR48eBA9evSAj48PmjVrhpUrVwIAtmzZgo4dO8LX1xft2rWr8hZLTZi6tVL5Npz2c+y9997D4sWLERERgYceegjdu3fHoUOHDH7+8OHDSEhIQMOGDeHt7Y3IyEjMmDEDQMVn11//+lcAQEREhO7zQfv+MFanmr63//3vf6NNmzbw9fVFTEwMvv/++2qvxcSJE1FSUoI1a9YYbNP+HiZMmAAAWLBgAbp27YqAgADUq1cPHTt2xGeffQZRzXrdpv4WTHVxMOdzxGFJnbRc3cWLF4VMJhMzZszQKz9z5owAIF577TUhhBDnzp0TSUlJYu3atWL37t3i+++/FxMnThRubm5i165dup/T/supcePGIi4uTvznP/8R27dvFzk5ObptD7bK7NmzR7zyyiviP//5j9izZ4/YsGGDGDZsmPDx8RHnzp0TQgihVqt1/5r8+9//Lg4ePCgOHjworly5IoQQIjk5WVR+K02aNEkAENOmTRPp6eli6dKlolGjRiIsLExcu3ZNt1+fPn1Ew4YNRYsWLcTSpUtFRkaGmDJligAgPv/88yqvXXl5uRg4cKDw8PAQ8+bNE9u3bxfvvfee8PPzE4888oiuFevgwYNi8ODBwsfHR1f3/Px8o8c8cOCAACBmz55d5Wtr3b9/X7Rv3174+fmJ9957T2zfvl3MmzdPeHh4iMGDB+vtq/29tG/fXqxZs0bs3LlTZGVliTNnzgiFQiHatWsnvvjiC7F9+3bxyiuvCDc3NzF//nyz6iGE8X8179y5U3h5eYlevXqJdevWifT0dDFu3DiD94H299u8eXMxffp0sW3bNvHpp5+KBg0aiLi4OL3X8Pb2Fv379xcbN24Uu3fvFl9++aUYM2aMuHnzptWuiTF37twR9evXFyEhIeKTTz6p8l/g2vOpvM+uXbsEAL2/Ge17MDQ0VHz44Ydi27Zt4sUXXxQAxNSpU6u+6MJ0i1DHjh2Fh4eHrtXG1Lnu3r1beHp6itjYWLFu3TqxceNGMWDAACGTycTatWsNziksLExMmDBB/Pe//xXLli0TQUFBIiwsTHf9hTDeovPCCy8IT09P8corr4j09HSxZs0a0bp1axEcHCzy8vIMrkerVq3EZ599JrZt2yaGDh2qa+Vs166d+Oqrr8TWrVtFt27dhFwuF7///nuV10j73ly4cKEoLS3Ve5SXl+te11iLQ+Vz0R6rWbNmYtCgQWLjxo1i48aNol27dqJBgwbi1q1bun3T09OFp6enaN++vVi1apXYuXOnWLFihXj66aeFEEJcuXJFTJ8+XQAQ69ev130+qNVqo3Wq6Xu7WbNmokuXLuLrr78WW7duFX379hUeHh7i559/rvJ6aTQaER4eLjp06KBXXlZWJpRKpejWrZuubNy4ceKzzz4TGRkZIiMjQ7z55pvCx8fHoEW68rkY+1t48Po++Plg7ueIo2LQcQB9+vQRgYGBek3cr7zyigAgLly4YPRnysrKRGlpqXjsscfE8OHDdeXaN2lkZKRBk7mxN7Cx45aUlIgWLVqIl19+WVde1a2rykFH27Q6ZcoUvf0OHz4sAIi//e1veucOQBw+fFhv36ioKDFw4ECT9RSi4kMMgHj33Xf1ytetWycAiGXLlunKEhMThZ+fX5XHE0KItWvXCgBi6dKl1e4rhBBLly4VAMTXX3+tV669/bV9+3ZdGQChUCjEjRs39PYdOHCgaNKkie7DVWvatGnC29vbYH9TjAWd1q1bi0ceeUSUlpbq7Tt06FChVCp1t3u0X6KVf2fvvvuuACBUKpUQQoj//Oc/AoDIzMw0WQ9rXBNTtmzZIgIDAwUAAUA0bNhQjBo1SmzatElvv5oGHQDiu+++09v3hRdeEG5ubuLXX3+tsk7aoKP94r569ap47bXXBAAxatSoas+1W7duIigoSNy+fVtXVlZWJqKjo0WTJk10QUB7Tg/+vQshxI8//igAiLfeektXVjkcHDx4UAAQ//znP/V+9sqVK8LHx0e8+uqrBtfj2LFjurKCggLh7u4ufHx89EJNZmamACA+/PDDKq+R9r1p7JGRkaF73ZoEnXbt2omysjJd+ZEjRwQA8dVXX+nKIiMjRWRkpLh//77JulV166pynWr63g4ODhaFhYW6sry8POHm5iZSUlJM1kdL+7l64sQJXdnmzZsFALF8+XKjP6PRaERpaal44403RMOGDXXvHWPnUpOgY+7niKPirSsHMHHiRFy/fh2bNm0CAJSVlWH16tXo1asXWrRoodtv6dKl6NixI7y9veHh4QFPT0/s2LEDZ8+eNTjmE088AU9Pz2pfu6ysDP/4xz8QFRUFLy8veHh4wMvLCxcvXjR6XHPs2rULQEWT84O6dOmCNm3aYMeOHXrlISEh6NKli15Z+/bt8euvv1b5Ojt37jT6OqNGjYKfn5/B69jCzp074efnh5EjR+qVa+tUuQ79+vVDgwYNdM+LioqwY8cODB8+HL6+vigrK9M9Bg8ejKKiIqPN8ea4dOkSzp07h+eeew4ADI6tUqlw/vx5vZ954okn9J63b98eAHS/iw4dOsDLywuTJk3C559/bvSWSW2vSVUGDx6M3NxcbNiwAbNmzULbtm2xceNGPPHEE5g2bZpZxzDG39/f4NyfffZZlJeXY+/evdX+/JkzZ+Dp6QlPT0+Ehobin//8J5577jksX75cb7/K53r37l0cPnwYI0eO1OuU6+7ujjFjxuC3334z+B1pf59aPXr0QHh4uO7vzpjvv/8eMpkMo0eP1nsfhISEICYmxuD2hVKpRGxsrO55QEAAgoKC0KFDB4SGhurK27RpAwDV/q1qvfTSSzh69Kjeo2vXrmb9bGVDhgyBu7u77nnl9+qFCxfw888/Y+LEifD29rboNSqr6Xs7Li4O/v7+uufBwcEICgoy63qNHz8ebm5uWLFiha5s5cqV8PPzw1NPPaVXp8cffxwKhQLu7u7w9PTE66+/joKCAuTn51tymnos+RxxNAw6DmDkyJFQKBS6e69bt27FH3/8gYkTJ+r2Wbx4MZKSktC1a1d8++23OHToEI4ePYpBgwbh/v37Bsc01Y+hspkzZ2LevHkYNmwYNm/ejMOHD+Po0aOIiYkxelxzVNWXIjQ0VLddq2HDhgb7yeXyal+/oKAAHh4eaNSokV65TCZDSEiIweuYo2nTpgCAnJwcs/YvKChASEiIQR+loKAgeHh4GNSh8jUpKChAWVkZPvroI90XpfYxePBgAMD169drfB4AdFMSzJo1y+DY2lF+lY9d+Xchl8sBQPe7iIyMxA8//ICgoCBMnToVkZGRiIyMxAcffGC1a1IdHx8fDBs2DIsWLcKePXtw6dIlREVFITU1FWfOnKnRsbSMjWoMCQkBALPeR5GRkTh69CiOHTuGrKws3Lp1C6tXr4ZCodDbr/K53rx5E0IIk38rxl5fW6/KZVXV848//oAQAsHBwQbvhUOHDhm8DwICAgyOoZ1ioXIZUBHYzdGkSRN06tRJ7/FgEKiJ6t6r2r5RTZo0sej4xtT0vW3pZxsAhIeH47HHHsOaNWtQXFyM69ev4/vvv8eoUaN01+zIkSMYMGAAgIoh5z/++COOHj2KuXPnAoDFn+EPsuRzxNFweLkD8PHxwTPPPIPly5dDpVJhxYoV8Pf3x6hRo3T7rF69Gn379kVaWprez96+fdvoMc2d12b16tUYO3Ys/vGPf+iVX79+3WCopbm0f9wqlcrgQ+bq1asIDAy06LjGXqesrAzXrl3TCztCCOTl5ek6UNdEp06dEBAQgO+++w4pKSnVXseGDRvi8OHDEELo7Zufn4+ysjKDc618vAYNGuj+9T516lSjrxEREVHj8wCge+05c+ZgxIgRRvdp1apVjY/bq1cv9OrVCxqNBseOHcNHH32EGTNmIDg4GE8//XStr0lNNW3aFJMmTcKMGTNw5swZtG3bVvcv+ModRE19IBubpyovLw+A8S+ryry9vc2aD8bY79/NzQ0qlcpgX20n8MrXS1uvymUPP/ywydcNDAyETCbDvn37dIHgQcbK7M3b2xtqtdqg3NIvUe1nQuUBELVR0/d2bU2cOBEZGRn47rvvcPXqVZSUlOj9A3jt2rXw9PTE999/r9dqtXHjxmqPbe7fiK0+R+yJLToOYuLEidBoNFi0aBG2bt2Kp59+Gr6+vrrtMpnM4MPo1KlTOHjwYK1e19hxt2zZgt9//12vrPK/lqrSr18/ABUh6kFHjx7F2bNn8dhjj9Wmyjra41R+nW+//RZ379616HU8PT0xe/ZsnDt3Dm+++abRffLz8/Hjjz/q6nDnzh2DD5YvvvhCr46m+Pr6Ii4uDidPnkT79u0N/rXbqVMns75ojWnVqhVatGiBn376yehxa/OvaaDi9krXrl11o8NOnDgBoPbXxJTbt2/jzp07Rrdpb7NqW0G0o3ROnTqlt5/29rCxY1fetmbNGri5uaF3794W1dccfn5+6Nq1K9avX6/3t1VeXo7Vq1ejSZMmaNmypd7PaOfr0Tpw4AB+/fXXKieDGzp0KIQQ+P33342+D9q1a2fV87JEs2bNcOHCBb0v3oKCAhw4cMCi47Vs2RKRkZFYsWKFwZf5g2ry2War97Ypw4YNQ8OGDbFixQqsXLkSLVu21BvxKZPJ4OHhoXcL7/79+/j3v/9d7bHN/Rux9eeIPbBFx0F06tQJ7du3x/vvvw8hhF5qByo+qN58800kJyejT58+OH/+PN544w1ERESgrKzM4tcdOnQoVq1ahdatW6N9+/Y4fvw4Fi1aZNASExkZCR8fH3z55Zdo06YNHnroIYSGhurdr9dq1aoVJk2ahI8++ghubm6Ij4/H5cuXMW/ePISFheHll1+2uL4P6t+/PwYOHIjZs2ejsLAQjz76KE6dOoXk5GQ88sgjGDNmjEXH/etf/4qzZ88iOTkZR44cwbPPPqubMHDv3r1YtmwZFixYgEcffRRjx45FamoqEhMTcfnyZbRr1w779+/HP/7xDwwePBiPP/54ta/3wQcfoGfPnujVqxeSkpLQrFkz3L59G5cuXcLmzZt1fZEs8cknnyA+Ph4DBw7EuHHj0LhxY9y4cQNnz57FiRMn8M0339ToeEuXLsXOnTsxZMgQNG3aFEVFRbo+BNpztcY1Meb8+fMYOHAgnn76afTp0wdKpRI3b97Eli1bsGzZMvTt2xc9evQAAHTu3BmtWrXCrFmzUFZWhgYNGmDDhg3Yv3+/0WM3bNgQSUlJyM3NRcuWLbF161YsX74cSUlJutuZtpKSkoL+/fsjLi4Os2bNgpeXF5YsWYKsrCx89dVXBq1Ax44dw/PPP49Ro0bhypUrmDt3Lho3bmx00lGtRx99FJMmTcL48eNx7Ngx9O7dG35+flCpVNi/fz/atWuHpKQkm55ndcaMGYNPPvkEo0ePxgsvvICCggK8++67tZrwMjU1FQkJCejWrRtefvllNG3aFLm5udi2bZsuMGpD3gcffIDExER4enqiVatWRr+8bfXeNkUul+O5557DRx99BCEE3nnnHb3tQ4YMweLFi/Hss89i0qRJKCgowHvvvWdWC11ISAgef/xxpKSkoEGDBggPD8eOHTuMzrZs7c8Ru5OsGzQZ+OCDDwQAERUVZbCtuLhYzJo1SzRu3Fh4e3uLjh07io0bN5ockWBsYi5jvelv3rwpJk6cKIKCgoSvr6/o2bOn2Ldvn9EREF999ZVo3bq18PT0FABEcnKyEML48HKNRiMWLlwoWrZsKTw9PUVgYKAYPXq0bki6lqmhueZOeHb//n0xe/ZsER4eLjw9PYVSqRRJSUl6Q221xzNn1NWDvvvuOzFkyBDRqFEj4eHhoRtqvXTpUlFcXKzbr6CgQEyePFkolUrh4eEhwsPDxZw5cwwmaUQVw5VzcnLEhAkTROPGjYWnp6do1KiR6NGjh95ImuqY+t3/9NNP4sknnxRBQUHC09NThISEiH79+umNLDM1GV3lkRkHDx4Uw4cPF+Hh4UIul4uGDRuKPn36GIx6ssY1qezmzZvirbfeEv369RONGzcWXl5ews/PT3To0EG89dZbBpPvXbhwQQwYMEDUq1dPNGrUSEyfPl1s2bLF6Kirtm3bit27d4tOnToJuVwulEql+Nvf/mYwysQYcyccrOpc9+3bJ/r16yf8/PyEj4+P6Natm8FEodrf0fbt28WYMWNE/fr1hY+Pjxg8eLC4ePGi3r6m/n5WrFghunbtqnudyMhIMXbsWL0RVqbOJzw8XAwZMqRG56Vl7oSBn3/+uWjTpo3w9vYWUVFRYt26dTX6jHvwc0nr4MGDIj4+XigUCiGXy0VkZKTeiFIhhJgzZ44IDQ0Vbm5ueu8PY5+DtX1vh4eHi8TExCqvw4N++uknAUC4u7uLq1evGmxfsWKFaNWqlZDL5aJ58+YiJSVFfPbZZwYjyYydi0qlEiNHjhQBAQFCoVCI0aNHi2PHjhkdYWvO54ijkglRzaxCRERERHUU++gQERGR02LQISIiIqfFoENEREROi0GHiIiInBaDDhERETktBh0iIiJyWi4/YWB5eTmuXr0Kf3//Wk9FT0RERPYhhMDt27cRGhoKNzfT7TYuH3SuXr2KsLAwqatBREREFrhy5UqVi7e6fNDRTvN95cqVWk01TkRERPZTWFiIsLCwatfacvmgo71dVa9ePQYdIiKiOqa6bifsjExEREROi0GHiIiInBaDDhERETktBh0iIiJyWgw6RERE5LQYdIiIiMhpMegQERGR02LQISIiIqflskEnNTUVUVFR6Ny5s9RVISIiIhuRCSGE1JWQUmFhIRQKBdRqtdVmRtaUCxzJuYH820UI8vdGl4gAuLtxwVAiIiJrMff72+WXgLC29CwVFmzOhkpdpCtTKryRnBCFQdFKCWtGRETkelz21pUtpGepkLT6hF7IAYA8dRGSVp9AepZKopoRERG5JgYdK9GUCyzYnA1j9wG1ZQs2Z0NT7tJ3ComIiOyKQcdKjuTcMGjJeZAAoFIX4UjODftVioiIyMUx6FhJ/m3TIceS/YiIiKj2GHSsJMjf26r7ERERUe0x6FhJl4gAKBXeMDWIXIaK0VddIgLsWS0iIiKXxqBjJe5uMiQnRAGAQdjRPk9OiOJ8OkRERHbEoGNFg6KVSBvdESEK/dtTIQpvpI3uyHl0iIiI7IxBx8oGRSuxf3Y/dGtecYtqbLdw7J/djyGHiIhIAgw6NuDuJkOTBr4AgJD63rxdRUREJBEGHRvx9qy4tMWl5RLXhIiIyHUx6NiIt4c7AKCoTCNxTYiIiFyXUwQdDw8PdOjQAR06dMDzzz8vdXUAAN6eFUGHLTpERETScYrVy+vXr4/MzEypq6FH7lGRIYtK2aJDREQkFado0XFE2hYdBh0iIiLpSB509u7di4SEBISGhkImk2Hjxo0G+yxZsgQRERHw9vZGbGws9u3bp7e9sLAQsbGx6NmzJ/bs2WOnmldN2xm5iLeuiIiIJCN50Ll79y5iYmLw8ccfG92+bt06zJgxA3PnzsXJkyfRq1cvxMfHIzc3V7fP5cuXcfz4cSxduhRjx45FYWGhvapvklzbR4edkYmIiCQjedCJj4/HW2+9hREjRhjdvnjxYkycOBHPP/882rRpg/fffx9hYWFIS0vT7RMaGgoAiI6ORlRUFC5cuGDy9YqLi1FYWKj3sIU/b12xRYeIiEgqkgedqpSUlOD48eMYMGCAXvmAAQNw4MABAMDNmzdRXFwMAPjtt9+QnZ2N5s2bmzxmSkoKFAqF7hEWFmaTunv9b5LAq+r7OPhzATTlwiavQ0RERKY5dNC5fv06NBoNgoOD9cqDg4ORl5cHADh79iw6deqEmJgYDB06FB988AECAkyvED5nzhyo1Wrd48qVK1avd3qWCnM2ZAEAfi24h2eWH0LPhTuRnqWy+msRERGRaXVieLlMpr+EghBCV9ajRw+cPn3a7GPJ5XLI5XKr1u9B6VkqJK0+gcrtN3nqIiStPsHFPYmIiOzIoVt0AgMD4e7urmu90crPzzdo5amp1NRUREVFoXPnzrU6zoM05QILNmcbhBwAurIFm7N5G4uIiMhOHDroeHl5ITY2FhkZGXrlGRkZ6NGjR62OPXXqVGRnZ+Po0aO1Os6DjuTcgEpdZHK7AKBSF+FIzg2rvSYRERGZJvmtqzt37uDSpUu65zk5OcjMzERAQACaNm2KmTNnYsyYMejUqRO6d++OZcuWITc3F5MnT5aw1sbl3zYdcizZj4iIiGpH8qBz7NgxxMXF6Z7PnDkTAJCYmIhVq1bhqaeeQkFBAd544w2oVCpER0dj69atCA8Pl6rKJgX5e1t1PyIiIqodmRDCJTuMpKamIjU1FRqNBhcuXIBarUa9evVqdUxNuUDPhTuRpy4y2k9HBiBE4Y39s/vB3U1mZA8iIiIyR2FhIRQKRbXf3w7dR8eWbNFHx91NhuSEKAAVoeZB2ufJCVEMOURERHbiskHHVgZFK5E2uiOCFfq3p0IU3hxaTkREZGeS99FxRoOilegfFYLWf/8vSssFPnymA4a0C2VLDhERkZ25bIuOLebReZC7mwy+8oocGaVUMOQQERFJwGWDji366FTm7VlxeYtKuYI5ERGRFFw26NiD3KNiBfPiMgYdIiIiKTDo2NCfLTrlEteEiIjINbls0LF1Hx0A8PasaNHhrSsiIiJpuGzQsUsfHd2tK7boEBERScFlg449yNkZmYiISFIMOjaiKRe4V1IRcLKvFkJT7pIrbRAREUmKQccG0rNU6LlwJ47/ehMA8On+HPRcuBPpWSqJa0ZERORaGHSsLD1LhaTVJ6BSF+mV56mLkLT6BMMOERGRHbls0LHFqCtNucCCzdlGVy7Xli3YnM3bWERERHbiskHHFqOujuTcMGjJeZAAoFIX4UjODau9JhEREZnmskHHFvJvmw45luxHREREtcOgY0VB/t5W3Y+IiIhqh0HHirpEBECp8IapdcplAJQKb3SJCLBntYiIiFwWg44VubvJkJwQBQAGYUf7PDkhCu5upqIQERERWRODjpUNilYibXRHhCj0b0+FKLyRNrojBkUrJaoZERGR63HZoGPLRT0HRSuxf3Y/TO/3MACgjdIf+2f3Y8ghIiKyM5cNOrZe1NPdTYZ2jRUAKlYx5+0qIiIi+3PZoGMP3p4Vq5cXlXL1ciIiIikw6NiQ3KPi8hZz9XIiIiJJMOjY0J8tOgw6REREUmDQsSFd0CnjrSsiIiIpMOjYkLdnxeVliw4REZE0GHRsSNuiU8wWHSIiIkm4bNCx5Tw6Wp5uFZdXUy6w/+I1aMqFzV6LiIiIDMmEEC797VtYWAiFQgG1Wo169epZ7bjpWSrM33QGeYXFujKlwhvJCVGcOJCIiKiWzP3+dtkWHVtKz1IhafUJvZADAHnqIiStPoH0LJVENSMiInItDDpWpikXWLA5G8aaybRlCzZn8zYWERGRHTDoWNmRnBtQqYtMbhcAVOoiHMm5Yb9KERERuSgGHSvLv2065FiyHxEREVmOQcfKgvy9rbofERERWY5Bx8q6RARAqfCGqbXKZagYfdUlIsCe1SIiInJJDDpW5u4mQ3JCFAAYhB3t8+SEKLi7mYpCREREZC0MOjYwKFqJtNEdEaLQvz0VovBG2uiOnEeHiIjITjykroCzGhStRP+oEDz5yUEc//Umnu8ZgTmD27Alh4iIyI7YomND7m4yNGngA6CiNYchh4iIyL4YdGzM538Le3IFcyIiIvtz2aBjj0U9AcDHqyLo3Cth0CEiIrI3lw06U6dORXZ2No4ePWrT19G26Nxniw4REZHduWzQsRff/7Xo3GeLDhERkd0x6NiYl0fFJb6YfxsHfy7gYp5ERER2xKBjQ+lZKny882cAwPFfb+GZ5YfQc+FOpGepJK4ZERGRa2DQsZH0LBWSVp9AYVGpXnmeughJq08w7BAREdkBg44NaMoFFmzOhrGbVNqyBZuzeRuLiIjIxhh0bOBIzg2o1EUmtwsAKnURjuTcsF+liIiIXBCDjg3k3zYdcizZj4iIiCzDoGMDQf7e1e9Ug/2IiIjIMgw6NtAlIgBKhTdMrWwlA6BUeKNLRIA9q0VERORyGHRswN1NhuSEKKPbtOEnOSGKi3wSERHZGIOOjQyKViJtdEcE+8v1ykMU3kgb3RGDopUS1YyIiMh1eEhdAWc2KFqJXi0aoW3yNgDAisRO6NMqiC05REREdsIWHRvz9XKH7H+5JrqxgiGHiIjIjpwm6Ny7dw/h4eGYNWuW1FXRI5PJ4MsVzImIiCThNEHn7bffRteuXaWuhlE+/1vB/B5XMCciIrIrpwg6Fy9exLlz5zB48GCpq2KUNuiwRYeIiMi+JA86e/fuRUJCAkJDQyGTybBx40aDfZYsWYKIiAh4e3sjNjYW+/bt09s+a9YspKSk2KnGNaMpFxCiYk2r45dvcn0rIiIiO5I86Ny9excxMTH4+OOPjW5ft24dZsyYgblz5+LkyZPo1asX4uPjkZubCwD47rvv0LJlS7Rs2dKe1TZLepYKPRfuxG83K5Z6eHvrWfRcuJMrlxMREdmJTGibGxyATCbDhg0bMGzYMF1Z165d0bFjR6SlpenK2rRpg2HDhiElJQVz5szB6tWr4e7ujjt37qC0tBSvvPIKXn/9daOvUVxcjOLiYt3zwsJChIWFQa1Wo169elY7l/QsFZJWnzBYwVw75opz6RAREVmusLAQCoWi2u9vyVt0qlJSUoLjx49jwIABeuUDBgzAgQMHAAApKSm4cuUKLl++jPfeew8vvPCCyZCj3V+hUOgeYWFhVq+3plxgweZsg5ADQFe2YHM2b2MRERHZmEMHnevXr0Oj0SA4OFivPDg4GHl5eRYdc86cOVCr1brHlStXrFFVPUdybkClNr0yuQCgUhfhSM4Nq782ERER/alOzIwsk+lPsieEMCgDgHHjxlV7LLlcDrlcXu1+tZF/23TIsWQ/IiIisoxDt+gEBgbC3d3doPUmPz/foJWnplJTUxEVFYXOnTvX6jjGBPl7W3U/IiIisoxDBx0vLy/ExsYiIyNDrzwjIwM9evSo1bGnTp2K7OxsHD16tFbHMaZLRACUCm+YWuxBBkCp8EaXiACrvzYRERH9SfJbV3fu3MGlS5d0z3NycpCZmYmAgAA0bdoUM2fOxJgxY9CpUyd0794dy5YtQ25uLiZPnixhravm7iZDckIUklafgAzQ65SsDT/JCVFc94qIiMjGJB9evnv3bsTFxRmUJyYmYtWqVQAqJgx89913oVKpEB0djX/961/o3bu3VV7f3OFplkjPUmHB5my9jslKhTeSE6I4tJyIiKgWzP3+ljzoSCU1NRWpqanQaDS4cOGCTYIOUDHU/N30c/hk7y/oEKbAt0mPsiWHiIiolpxiHh1bsmUfnQe5u8nQvkl9AICXhztDDhERkR25bNCxJ1+5dvXyMolrQkRE5FoYdOzAz6uiz/fdYq5eTkREZE8uG3RsOY9OZd6eFZe54E4xDv5cwKUfiIiI7MRlOyNr2XLUFVAx8mrexjO4dufPhUQ58oqIiKh22BnZAWhXMH8w5ABAnroISatPID1LJVHNiIiIXAODjo1wBXMiIiLpuWzQsXUfHa5gTkREJD2XDTq2nkeHK5gTERFJz2WDjq1xBXMiIiLpMejYCFcwJyIikh6Djo1oVzAHYBB2uII5ERGRfTDo2NCgaCXSRndEiEL/9lSIwhtpoztyHh0iIiIbc9mgY6+ZkQdFK7F/dj90CFMAAP5f7+bYP7sfQw4REZEduGzQsdfq5UDFbazGDXwBVPTL4e0qIiIi+3DZoGNvfl4VK5jfLeHCnkRERPbCoGMn3p4VQScz9yYX9iQiIrITD6kr4ArSs1RYf+I3AEDG2XxknM3nwp5ERER2wBYdG9Mu7HmnWP+WFRf2JCIisj0GHRviwp5ERETSctmgY4/h5VzYk4iISFouG3TsMbycC3sSERFJy2WDjj1wYU8iIiJpMejYEBf2JCIikhaDjg09uLBnZVzYk4iIyPYYdGxMu7BnkL9cr5wLexIREdkeJwy0g0HRSvR8uBGi528DAKwc1xm9WzZiSw4REZGNsUXHTvzk7rpg00ZZjyGHiIjIDlw26NhjHp0HlQvA26Picu+/dI2TBBIREdmBTAjh0t+4hYWFUCgUUKvVqFevnk1eIz1LhQWbs/UmD+RaV0RERJYz9/vbZVt07EW71lXlGZK51hUREZHtMejYENe6IiIikhaDjg1xrSsiIiJpMejYENe6IiIikhaDjg1xrSsiIiJpMejYENe6IiIikhaDjg09uNZV5bDDta6IiIhsj0HHxrRrXYUo9G9Pca0rIiIi2+NaV3YwKFqJ/lEh+Of281iy+2e0a1wPG6f2ZEsOERGRjbFFx07c3WSIaVIfAFB4vwxHcm5w/hwiIiIbY9Cxk/QsFeZsOA0A+PXGPTyz/BB6LtzJmZGJiIhsyGWDjj0X9dQuA3HjboleOZeBICIisi0u6mnjRT015QI9F+40OUOyDBUdk/fP7sc+O0RERGbiop4OgstAEBERSYdBx8a4DAQREZF0GHRsjMtAEBERSceioLN3716UlZUZlJeVlWHv3r21rpQz4TIQRERE0rEo6MTFxeHGDcM+JWq1GnFxcbWulDPhMhBERETSsSjoCCEgkxl+MRcUFMDPz6/WlXI2XAaCiIhIGjVaAmLEiBEAAJlMhnHjxkEul+u2aTQanDp1Cj169LBuDZ2EdhmIkWk/4uQVNeJaNcLzvZqjW/OGUleNiIjIadUo6CgUCgAVLTr+/v7w8fHRbfPy8kK3bt3wwgsvWLeGTiQjOw/n8m4DAHadv4Zd569BqfBGckIUW3WIiIhsoEZBZ+XKlQCAZs2aYdasWbxNVQPa2ZErz86onR2Zt7CIiIisz6I+OsnJyQw5NaApF1iwOdsg5ADQlS3YnM1FPomIiKzMoqDzxx9/YMyYMQgNDYWHhwfc3d31HqSPsyMTERFJo0a3rrTGjRuH3NxczJs3D0ql0ugILPoTZ0cmIiKShkVBZ//+/di3bx86dOhg5eo4J86OTEREJA2Lbl2FhYXBURY9v337Njp37owOHTqgXbt2WL58udRVMsDZkYmIiKRhUdB5//338dprr+Hy5ctWrk7N+fr6Ys+ePcjMzMThw4eRkpKCgoICqaul58HZkSvj7MhERES2Y9Gtq6eeegr37t1DZGQkfH194enpqbfd2PIQtuLu7g5fX18AQFFRETQajcO0Nj1IOzvy69+dQf7tYl15COfRISIishmLgs77779vtQrs3bsXixYtwvHjx6FSqbBhwwYMGzZMb58lS5Zg0aJFUKlUaNu2Ld5//3306tVLt/3WrVvo06cPLl68iEWLFiEwMNBq9bOmQdFKdGveEB3eyAAAvDqwFZ7v1RxeHlxEnoiIyBYsCjqJiYlWq8Ddu3cRExOD8ePH4y9/+YvB9nXr1mHGjBlYsmQJHn30UXzyySeIj49HdnY2mjZtCgCoX78+fvrpJ/zxxx8YMWIERo4cieDgYKvV0VrSs1SYvzlb9/zdbefx70O/skWHiIjIRmTCwvs8P//8M1auXImff/4ZH3zwAYKCgpCeno6wsDC0bdvWssrIZAYtOl27dkXHjh2RlpamK2vTpg2GDRuGlJQUg2MkJSWhX79+GDVqlNHXKC4uRnHxn7eOCgsLERYWBrVajXr16llUb3OYmhlZ2yuHMyMTERGZr7CwEAqFotrvb4vumezZswft2rXD4cOHsX79ety5cwcAcOrUKSQnJ1tWYyNKSkpw/PhxDBgwQK98wIABOHDgAICKyQsLCwsBVJz03r170apVK5PHTElJgUKh0D3CwsKsVl9TODMyERGRNCwKOq+99hreeustZGRkwMvLS1ceFxeHgwcPWq1y169fh0ajMbgNFRwcjLy8PADAb7/9ht69eyMmJgY9e/bEtGnT0L59e5PHnDNnDtRqte5x5coVq9XXFM6MTEREJA2L+uicPn0aa9asMShv1KiRTYZ2V555WQihK4uNjUVmZqbZx5LL5ZDL5dasXrU4MzIREZE0LGrRqV+/PlQqlUH5yZMn0bhx41pXSiswMBDu7u661hut/Pz8Wnc2Tk1NRVRUFDp37lyr45iDMyMTERFJw6Kg8+yzz2L27NnIy8uDTCZDeXk5fvzxR8yaNQtjx461WuW8vLwQGxuLjIwMvfKMjAz06NGjVseeOnUqsrOzcfTo0VodxxycGZmIiEgaFgWdt99+G02bNkXjxo1x584dREVFoXfv3ujRowf+/ve/1+hYd+7cQWZmpu72U05ODjIzM5GbmwsAmDlzJj799FOsWLECZ8+excsvv4zc3FxMnjzZkqpL4sGZkSuHHc6MTEREZDsWDy8HKoaYnzx5EuXl5XjkkUfQokWLGh9j9+7diIuLMyhPTEzEqlWrAFRMGPjuu+9CpVIhOjoa//rXv9C7d29Lqw2g4tZVamoqNBoNLly4YPPh5UDFEPMFm7P1OiYH+Hnirf+LxuD2oTZ9bSIiImdi7vDyWgUdZ2DuhbKWradUePXbU7hTXKYrU3IZCCIiohox9/vb7FFXM2fOxJtvvgk/Pz/MnDmzyn0XL15sfk1dSHqWClPXGE4amKcuQtLqE5w0kIiIyMrMDjonT55EaWmp7v9NqTwUnCpUN2mgDBWTBvaPCmFfHSIiIisxO+js2rXL6P+TeWoyaWD3yIb2qxgREZETc9lls+05jw7ASQOJiIikYHaLzogRI8w+6Pr16y2qjD1NnToVU6dO1XVmsjVOGkhERGR/ZrfoPLgQZr169bBjxw4cO3ZMt/348ePYsWOHXUJDXcRJA4mIiOzP7BadlStX6v5/9uzZePLJJ7F06VK4u7sDADQaDaZMmWKXIdp1kXbSwKTVJyADDDolC3DSQCIiImuzqI/OihUrMGvWLF3IAQB3d3fMnDkTK1assFrlbMnefXQAYFC0EmmjO0Lh62mwrb6RMiIiIqodi4JOWVkZzp49a1B+9uxZlJeX17pS9mDPta4qU98rNVqWtPoE0rMMF0slIiIiy5h96+pB48ePx4QJE3Dp0iV069YNAHDo0CG88847GD9+vFUr6Ew4lw4REZF9WRR03nvvPYSEhOBf//oXVKqKFgilUolXX30Vr7zyilUr6Ew4lw4REZF9WRR03Nzc8Oqrr+LVV19FYWEhALATshk4lw4REZF9WRR0HsSAYz7OpUNERGRfFged//znP/j666+Rm5uLkpISvW0nTpyodcVsLTU1FampqdBoNHZ7Te1cOnnqIqP9dGQAQjiXDhERkdVYNOrqww8/xPjx4xEUFISTJ0+iS5cuaNiwIX755RfEx8dbu442IcWoK+1cOsZoux5zLh0iIiLrsSjoLFmyBMuWLcPHH38MLy8vvPrqq8jIyMCLL74ItVpt7To6Fe1cOoF+XnrlwfXkSBvdEYOilRLVjIiIyPlYFHRyc3PRo0cPAICPjw9u374NABgzZgy++uor69XOibm7V261YSsOERGRtVkUdEJCQlBQUAAACA8Px6FDhwAAOTk5EMJY7xPSSs9SIWn1CfxRWKxX/kdhEScMJCIisjKLgk6/fv2wefNmAMDEiRPx8ssvo3///njqqacwfPhwq1bQmVQ3YSBQMWGgppxhkYiIyBosGnW1bNky3VIPkydPRkBAAPbv34+EhARMnjzZqhV0JpwwkIiIyL5qHHTKysrw9ttvY8KECQgLCwMAPPnkk3jyySetXjlbkmJ4OScMJCIisq8a37ry8PDAokWL7BoQbEGK4eWcMJCIiMi+LOqj8/jjj2P37t1Wrorz004YaGp8lQyAkhMGEhERWY1FfXTi4+MxZ84cZGVlITY2Fn5+fnrbn3jiCatUztloJwxMWn0CMsCgU7IAJwwkIiKyJpmwYDy4m5vphiCZTFanbmsVFhZCoVBArVbbbd2u9CwVXlt/GrfuleqV1/f1xDsj2nHSQCIiomqY+/1t0a2r8vJyk4+6FHKkpK4UcrRlnEuHiIjIemp06+r+/fvYsWMHhg4dCgCYM2cOiov/nPjOw8MDb7zxBry92ZnWlOrm0pGhYi6d/lEhvIVFRERUSzUKOl988QW+//57XdD5+OOP0bZtW/j4+AAAzp07h5CQEMycOdP6NXUSnEuHiIjIfmp06+rLL7/EhAkT9MrWrFmDXbt2YdeuXVi0aBG++eYbq1bQVlJTUxEVFYXOnTvb9XU5lw4REZH91CjoXLhwAS1bttQ99/b21uuY3KVLF2RnZ1uvdjYkxTw6AOfSISIisqca3bpSq9Xw8PjzR65du6a3vby8XK/PDhnSzqWTpy4y2k8H4Fw6RERE1lKjFp0mTZogKyvL5PZTp06hSZMmta6UM9POpVOVJ2KU7IhMRERkBTUKOoMHD8brr7+OoiLD/iP379/HggULMGTIEKtVzlkNilZiUu8Ik9uX7c3hEHMiIiIrqNGEgX/88Qc6dOgALy8vTJs2DS1btoRMJsO5c+fw8ccfo6ysDCdPnkRwcLAt62xVUkwYqCkX6Llwp8nRVzIAIQpv7J/djy07RERERpj7/V2jPjrBwcE4cOAAkpKS8Nprr0GbkWQyGfr3748lS5bUqZAjFQ4xJyIiso8ar3UVERGB9PR03LhxA5cuXQIAPPzwwwgIYOdZc3GIORERkX1YtKgnAAQEBKBLly7WrIvL4BBzIiIi+7BorSuqHe0Qc1O9b2TgEHMiIiJrYNCRgDlDzJMTotgRmYiIqJYYdCSiHWJeOcu4yYBJvSMwKFopTcWIiIicCIOORNKzVFi2NwfllQb3C8F5dIiIiKzFZYOOVIt6AhXz6CzYnG10CQht2YLN2dBUTkFERERUIy4bdKRa1BOo2Tw6REREZDmXDTpS4jw6RERE9sGgIwHOo0NERGQfDDoSqG4eHYDz6BAREVkDg44EzJlH54kYJefRISIiqiUGHYlo59ExhUPMiYiIao9BRyKacoFNP1UdZDjEnIiIqHYYdCTCIeZERES2x6AjEQ4xJyIisj0GHYlwiDkREZHtMehIpLoh5jJwiDkREVFtMehIRDvE3FRXYwEgOSGKQ8yJiIhqgUGHiIiInBaDjkS0K5ibIgOHlxMREdVWnQ86V65cQd++fREVFYX27dvjm2++kbpKZuHwciIiItvzkLoCteXh4YH3338fHTp0QH5+Pjp27IjBgwfDz89P6qpVicPLiYiIbK/OBx2lUgmlUgkACAoKQkBAAG7cuOHwQYfDy4mIiGxP8ltXe/fuRUJCAkJDQyGTybBx40aDfZYsWYKIiAh4e3sjNjYW+/btM3qsY8eOoby8HGFhYTaude1xBXMiIiLbkzzo3L17FzExMfj444+Nbl+3bh1mzJiBuXPn4uTJk+jVqxfi4+ORm5urt19BQQHGjh2LZcuW2aPatcYVzImIiGxPJoRwmGE9MpkMGzZswLBhw3RlXbt2RceOHZGWlqYra9OmDYYNG4aUlBQAQHFxMfr3748XXngBY8aMqfI1iouLUVxcrHteWFiIsLAwqNVq1KtXz7onZIaUrdn4ZG+O0W0yAGmjO2JQtNK+lSIiInJwhYWFUCgU1X5/S96iU5WSkhIcP34cAwYM0CsfMGAADhw4AAAQQmDcuHHo169ftSEHAFJSUqBQKHQPKW9zcQVzIiIi23LooHP9+nVoNBoEBwfrlQcHByMvLw8A8OOPP2LdunXYuHEjOnTogA4dOuD06dMmjzlnzhyo1Wrd48qVKzY9h6pwiDkREZFt1YlRVzKZfj8VIYSurGfPnigvLzf7WHK5HHK53Kr1sxSHmBMREdmWQ7foBAYGwt3dXdd6o5Wfn2/QylNTqampiIqKQufOnWt1nNrgEHMiIiLbcuig4+XlhdjYWGRkZOiVZ2RkoEePHrU69tSpU5GdnY2jR4/W6ji1wSHmREREtiX5ras7d+7g0qVLuuc5OTnIzMxEQEAAmjZtipkzZ2LMmDHo1KkTunfvjmXLliE3NxeTJ0+WsNbWoR1inrT6hMl9OMSciIjIcpIHnWPHjiEuLk73fObMmQCAxMRErFq1Ck899RQKCgrwxhtvQKVSITo6Glu3bkV4eHitXjc1NRWpqanQaDS1Ok5tDYpWYlLvCJNDzJftzcEjTRtwiDkREZEFHGoeHSmYOw7fVjTlAj0X7jQ5+koGIEThjf2z+7Flh4iI6H+cYh4dV8Ah5kRERLbDoCMxDjEnIiKyHZcNOo4wvBzgEHMiIiJbctmg4wjDy4Hqh5jLwCHmRERElnLZoOMotEPMTfUIFwCSE6LYEZmIiMgCDDpERETktBh0JKYpF1iwOdvkdhm4gjkREZGlXDboOEpnZA4vJyIish2XDTqO0hmZw8uJiIhsx2WDjqPg8HIiIiLbYdCRmDkrmNf39eTwciIiIgsw6EisuuHlAHDrXikysvPsViciIiJnwaDjAPpHhaC+r6fJ7Rx5RUREZBmXDTqOMuoKqBh5deteqcntHHlFRERkGZcNOo4y6grgyCsiIiJbcdmg40g48oqIiMg2GHQcgDkjr7iwJxERUc0x6DgA7cirqjwRo+TCnkRERDXEoOMgBkUrMal3hMnty/bmID1LZccaERER1X0uG3QcadQVULG456afqg4yHGJORERUMy4bdBxp1BXAxT2JiIhswWWDjqPhEHMiIiLrY9BxEBxiTkREZH0MOg5CO8S8KhxiTkREVDMMOg7C3U2GJ2KUVe7DIeZEREQ1w6DjIMwZdbXpJxVHXREREdUAg46DqG7UFcBRV0RERDXFoOMgOOqKiIjI+lw26DjahIEcdUVERGR9Lht0HG3CQHMW9qzv68lRV0RERDXgskHH0WgX9qyqq/Gte6XIyM6zW52IiIjqOgYdB9I/KgT1fT1NbpeB610RERHVBIOOAzmScwO37pWa3M71roiIiGqGQceBcOQVERGRdTHoOBCOvCIiIrIuBh0HYs7IK653RUREZD4GHQeiHXlVFa53RUREZD4GHQczKFqJSb0jTG5ftjcH6VlVr4lFREREFRh0HIw5i3tyiDkREZF5GHQcTHWLe3KIORERkfkYdBwMh5gTERFZj8sGHUdb1FOLQ8yJiIisx2WDjqMt6qnVJSKgymUgAC7uSUREZC6XDTp1GQeXExERmYdBx8FUt94VANy8V8rOyERERGZg0HEw7IxMRERkPQw6DoadkYmIiKyHQcfBmLPeFTsjExERmYdBx8Fo17uqat7jW/dKkZGdZ7c6ERER1VUMOg6of1RIlUPMZeAyEEREROZg0HFA1Y284jIQRERE5mHQcUAceUVERGQdDDoOiCOviIiIrINBxwFpR15V5+bdEjvUhoiIqO5i0HFA7m4yzBvSptr93tzCDslERERVYdBxUA385NXuww7JREREVXOKoDN8+HA0aNAAI0eOlLoqVsMOyURERLXnFEHnxRdfxBdffCF1NayKHZKJiIhqzymCTlxcHPz9/aWuhlWZ0yFZqfDmUhBERERVkDzo7N27FwkJCQgNDYVMJsPGjRsN9lmyZAkiIiLg7e2N2NhY7Nu3z/4VtTN3NxmeiFFWuc8TMUq4u1W1KhYREZFrkzzo3L17FzExMfj444+Nbl+3bh1mzJiBuXPn4uTJk+jVqxfi4+ORm5tr55ral6ZcYNNPqir32fSTiqOuiIiIquAhdQXi4+MRHx9vcvvixYsxceJEPP/88wCA999/H9u2bUNaWhpSUlJq/HrFxcUoLi7WPS8sLKx5pe3gSM4NqNRVdzTWjrrqHtnQTrUiIiKqWyRv0alKSUkJjh8/jgEDBuiVDxgwAAcOHLDomCkpKVAoFLpHWFiYNapqdRx1RUREVHsOHXSuX78OjUaD4OBgvfLg4GDk5eXpng8cOBCjRo3C1q1b0aRJExw9etTkMefMmQO1Wq17XLlyxWb1rw1zR1Ndvn7PxjUhIiKquyS/dWUOmUy/w60QQq9s27ZtZh9LLpdDLq9+Mj6pdYkIQEg9OfIKi6vcb+3RXEzr9zA7JRMRERnh0C06gYGBcHd312u9AYD8/HyDVp6aSk1NRVRUFDp37lyr49iKu5sMz3RpWu1+nB2ZiIjINIcOOl5eXoiNjUVGRoZeeUZGBnr06FGrY0+dOhXZ2dlV3uaSWrNAP7P2Yz8dIiIi4yS/dXXnzh1cunRJ9zwnJweZmZkICAhA06ZNMXPmTIwZMwadOnVC9+7dsWzZMuTm5mLy5MkS1to+ODsyERFR7UgedI4dO4a4uDjd85kzZwIAEhMTsWrVKjz11FMoKCjAG2+8AZVKhejoaGzduhXh4eG1et3U1FSkpqZCo9HU6ji21CUiAPV9PXHrXqnJfer7enJ2ZCIiIhNkQgiXnnGusLAQCoUCarUa9erVk7o6ejTlArFvZVQZdBr4euLY3/uzMzIREbkUc7+/HbqPjqs7knOjypADADfvlbIzMhERkQkMOg6MkwYSERHVjssGHUcfXg5w0kAiIqLactmgUxeGl2snDazO2qO5XNyTiIjICJcNOnUBJw0kIiKqHQYdB8dJA4mIiCzHoOPg2E+HiIjIci4bdOpCZ2SA/XSIiIhqw2WDTl3ojAywnw4REVFtuGzQqUvYT4eIiMgyDDp1QKBf9beuarIfERGRq2DQqQvMXcaKy10RERHpYdCpA67fKTZrvx1n/7BxTYiIiOoWlw06dWXUFWD+EPPvMq9y5BUREdEDXDbo1JVRV0DFEPMAP89q9yu4W8KRV0RERA9w2aBTl7i7yTC8Q2Oz9s3IzrNxbYiIiOoOBp064vGoELP24+0rIiKiPzHo1BG8fUVERFRzDDp1hLubDP8XE2rWvnnq+zauDRERUd3gskGnLo260mrSwNes/W7cLbFxTYiIiOoGlw06dWnUlVbAQ+bNfLz/5+s2rgkREVHd4LJBpy4KqWfefDq7zl3D1lMqG9eGiIjI8THo1CHmdkgGgHnfZXH0FRERuTwGnTqkJvPpcPQVERERg06dY+58OgCwbO/PNqwJERGR42PQqWO6RATA39vdrH13nb+G7zOv2rhGREREjotBp45xd5NhZMcmZu8/be1JbDrxuw1rRERE5LgYdOqgAW2VNdr/xa8z8cgb2/Huf8/hx0vX2UmZiIhchofUFZBKamoqUlNTodFopK5KjWlHX924W2r2z9y8V4ole37Gkj0V/XYaK7zg6+UBAQG5hwe83GUo0Qij/y0uK0epprzKfc3Zh8dzrOM5wznweDwej+dYr1l5X5mbDG1CFBgZ2wQ9Hg6Eu5vMVl+NJsmEEC79z/vCwkIoFAqo1WrUq1dP6uqYbeupq5iy5qTU1SAiIjKLn5c7/vlkDAZF1+yuhCnmfn/z1lUdNbh9KIa0C5a6GkRERGa5W6LB5NUnkJ5l3wltGXTqsA+fiYW3B3+FRERUdyzYnG3XvqL8lqzD3N1kWPxkjNTVICIiMptKXWTXCW0ZdOq4we1D8UKvZlJXg4iIyGz5t4vs9loMOk5g7pC2eKFXhNTVICIiMkuQv3mLVFsDg46TmDskCkue7QhPCYbuERERmUup8EaXiAC7vR6DjhMZ3F6Jc2/F48W4h+HO3ywRETmg5IQou86nw3l06ug8OtXRlAscuHgdH+68gBNXbkFTLnWNiIjIlfnJ3fHPUfafR8dlZ0Z2du5uMvRq1Qi9WjWCplzg0M8F2HcpH6euqHG/tAzFpdLPmMnjcdZUHo/H4/Gc+2/cEWZGZtBxAe5uMjzaIhCPtgiUuipERER25bI9OVJTUxEVFYXOnTtLXRUiIiKyEfbRcdI+OkRERM6Ma10RERGRy2PQISIiIqfFoENEREROi0GHiIiInBaDDhERETktBh0iIiJyWgw6RERE5LRcfmZk7TRChYWFEteEiIiIzKX93q5uOkCXDzq3b98GAISFhUlcEyIiIqqp27dvQ6FQmNzu8jMjl5eX4+rVq/D394dMZr3FxgoLCxEWFoYrV6647IzLrn4NXP38AV4DVz9/gNfA1c8fsN01EELg9u3bCA0NhZub6Z44Lt+i4+bmhiZNmtjs+PXq1XPZN7eWq18DVz9/gNfA1c8f4DVw9fMHbHMNqmrJ0WJnZCIiInJaDDpERETktBh0bEQulyM5ORlyuVzqqkjG1a+Bq58/wGvg6ucP8Bq4+vkD0l8Dl++MTERERM6LLTpERETktBh0iIiIyGkx6BAREZHTYtAhIiIip8WgYyNLlixBREQEvL29ERsbi3379kldJavYu3cvEhISEBoaCplMho0bN+ptF0Jg/vz5CA0NhY+PD/r27YszZ87o7VNcXIzp06cjMDAQfn5+eOKJJ/Dbb7/Z8Swsl5KSgs6dO8Pf3x9BQUEYNmwYzp8/r7ePM1+DtLQ0tG/fXjfxV/fu3fHf//5Xt92Zz92UlJQUyGQyzJgxQ1fmzNdh/vz5kMlkeo+QkBDddmc+9wf9/vvvGD16NBo2bAhfX1906NABx48f12139uvQrFkzg/eBTCbD1KlTATjY+QuyurVr1wpPT0+xfPlykZ2dLV566SXh5+cnfv31V6mrVmtbt24Vc+fOFd9++60AIDZs2KC3/Z133hH+/v7i22+/FadPnxZPPfWUUCqVorCwULfP5MmTRePGjUVGRoY4ceKEiIuLEzExMaKsrMzOZ1NzAwcOFCtXrhRZWVkiMzNTDBkyRDRt2lTcuXNHt48zX4NNmzaJLVu2iPPnz4vz58+Lv/3tb8LT01NkZWUJIZz73I05cuSIaNasmWjfvr146aWXdOXOfB2Sk5NF27ZthUql0j3y8/N125353LVu3LghwsPDxbhx48Thw4dFTk6O+OGHH8SlS5d0+zj7dcjPz9d7D2RkZAgAYteuXUIIxzp/Bh0b6NKli5g8ebJeWevWrcVrr70mUY1so3LQKS8vFyEhIeKdd97RlRUVFQmFQiGWLl0qhBDi1q1bwtPTU6xdu1a3z++//y7c3NxEenq63epuLfn5+QKA2LNnjxDCNa9BgwYNxKeffupy53779m3RokULkZGRIfr06aMLOs5+HZKTk0VMTIzRbc5+7lqzZ88WPXv2NLndVa7Dg1566SURGRkpysvLHe78eevKykpKSnD8+HEMGDBAr3zAgAE4cOCARLWyj5ycHOTl5emdu1wuR58+fXTnfvz4cZSWlurtExoaiujo6Dp5fdRqNQAgICAAgGtdA41Gg7Vr1+Lu3bvo3r27S507AEydOhVDhgzB448/rlfuCtfh4sWLCA0NRUREBJ5++mn88ssvAFzj3AFg06ZN6NSpE0aNGoWgoCA88sgjWL58uW67q1wHrZKSEqxevRoTJkyATCZzuPNn0LGy69evQ6PRIDg4WK88ODgYeXl5EtXKPrTnV9W55+XlwcvLCw0aNDC5T10hhMDMmTPRs2dPREdHA3CNa3D69Gk89NBDkMvlmDx5MjZs2ICoqCiXOHettWvX4sSJE0hJSTHY5uzXoWvXrvjiiy+wbds2LF++HHl5eejRowcKCgqc/ty1fvnlF6SlpaFFixbYtm0bJk+ejBdffBFffPEFAOd/D1S2ceNG3Lp1C+PGjQPgeOfv8quX24pMJtN7LoQwKHNWlpx7Xbw+06ZNw6lTp7B//36Dbc58DVq1aoXMzEzcunUL3377LRITE7Fnzx7ddmc+dwC4cuUKXnrpJWzfvh3e3t4m93PW6xAfH6/7/3bt2qF79+6IjIzE559/jm7dugFw3nPXKi8vR6dOnfCPf/wDAPDII4/gzJkzSEtLw9ixY3X7Oft10Prss88QHx+P0NBQvXJHOX+26FhZYGAg3N3dDRJpfn6+Qbp1NtqRF1Wde0hICEpKSnDz5k2T+9QF06dPx6ZNm7Br1y40adJEV+4K18DLywsPP/wwOnXqhJSUFMTExOCDDz5wiXMHKprc8/PzERsbCw8PD3h4eGDPnj348MMP4eHhoTsPZ78OWn5+fmjXrh0uXrzoMu8BpVKJqKgovbI2bdogNzcXgGt8Dmj9+uuv+OGHH/D888/ryhzt/Bl0rMzLywuxsbHIyMjQK8/IyECPHj0kqpV9REREICQkRO/cS0pKsGfPHt25x8bGwtPTU28flUqFrKysOnF9hBCYNm0a1q9fj507dyIiIkJvuytcg8qEECguLnaZc3/sscdw+vRpZGZm6h6dOnXCc889h8zMTDRv3twlroNWcXExzp49C6VS6TLvgUcffdRgWokLFy4gPDwcgGt9DqxcuRJBQUEYMmSIrszhzt+qXZtJCPHn8PLPPvtMZGdnixkzZgg/Pz9x+fJlqatWa7dv3xYnT54UJ0+eFADE4sWLxcmTJ3VD59955x2hUCjE+vXrxenTp8UzzzxjdEhhkyZNxA8//CBOnDgh+vXrV2eGVCYlJQmFQiF2796tN7Ty3r17un2c+RrMmTNH7N27V+Tk5IhTp06Jv/3tb8LNzU1s375dCOHc516VB0ddCeHc1+GVV14Ru3fvFr/88os4dOiQGDp0qPD399d9vjnzuWsdOXJEeHh4iLfffltcvHhRfPnll8LX11esXr1at48rXAeNRiOaNm0qZs+ebbDNkc6fQcdGUlNTRXh4uPDy8hIdO3bUDT+u63bt2iUAGDwSExOFEBXDKpOTk0VISIiQy+Wid+/e4vTp03rHuH//vpg2bZoICAgQPj4+YujQoSI3N1eCs6k5Y+cOQKxcuVK3jzNfgwkTJuje140aNRKPPfaYLuQI4dznXpXKQceZr4N2PhRPT08RGhoqRowYIc6cOaPb7szn/qDNmzeL6OhoIZfLRevWrcWyZcv0trvCddi2bZsAIM6fP2+wzZHOXyaEENZtIyIiIiJyDOyjQ0RERE6LQYeIiIicFoMOEREROS0GHSIiInJaDDpERETktBh0iIiIyGkx6BAREZHTYtAhIqpEJpNh48aNUleDiKyAQYeIHMq4ceMgk8kMHoMGDZK6akRUB3lIXQEiosoGDRqElStX6pXJ5XKJakNEdRlbdIjI4cjlcoSEhOg9GjRoAKDitlJaWhri4+Ph4+ODiIgIfPPNN3o/f/r0afTr1w8+Pj5o2LAhJk2ahDt37ujts2LFCrRt2xZyuRxKpRLTpk3T2379+nUMHz4cvr6+aNGiBTZt2mTbkyYim2DQIaI6Z968efjLX/6Cn376CaNHj8YzzzyDs2fPAgDu3buHQYMGoUGDBjh69Ci++eYb/PDDD3pBJi0tDVOnTsWkSZNw+vRpbNq0CQ8//LDeayxYsABPPvkkTp06hcGDB+O5557DjRs37HqeRGQFVl8mlIioFhITE4W7u7vw8/PTe7zxxhtCiIoV5CdPnqz3M127dhVJSUlCCCGWLVsmGjRoIO7cuaPbvmXLFuHm5iby8vKEEEKEhoaKuXPnmqwDAPH3v/9d9/zOnTtCJpOJ//73v1Y7TyKyD/bRISKHExcXh7S0NL2ygIAA3f93795db1v37t2RmZkJADh79ixiYmLg5+en2/7oo4+ivLwc58+fh0wmw9WrV/HYY49VWYf27dvr/t/Pzw/+/v7Iz8+39JSISCIMOkTkcPz8/AxuJVVHJpMBAIQQuv83to+Pj49Zx/P09DT42fLy8hrViYikxz46RFTnHDp0yOB569atAQBRUVHIzMzE3bt3ddt//PFHuLm5oWXLlvD390ezZs2wY8cOu9aZiKTBFh0icjjFxcXIy8vTK/Pw8EBgYCAA4JtvvkGnTp3Qs2dPfPnllzhy5Ag+++wzAMBzzz2H5ORkJCYmYv78+bh27RqmT5+OMWPGIDg4GAAwf/58TJ48GUFBQYiPj8ft27fx448/Yvr06fY9USKyOQYdInI46enpUCqVemWtWrXCuXPnAFSMiFq7di2mTJmCkJAQfPnll4iKigIA+Pr6Ytu2bXjppZfQuXNn+Pr64i9/+QsWL16sO1ZiYiKKiorwr3/9C7NmzUJgYCBGjhxpvxMkIruRCSGE1JUgIjKXTCbDhg0bMGzYMKmrQkR1APvoEBERkdNi0CEiIiKnxT46RFSn8G47EdUEW3SIiIjIaTHoEBERkdNi0CEiIiKnxaBDRERETotBh4iIiJwWgw4RERE5LQYdIiIicloMOkREROS0GHSIiIjIaf1/dFCZCjEu8uMAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#gradient of core tensor\n",
    "gradient_set_1 = epoch_level_gradients_SGD[0,1,-1]\n",
    "gradient_set_2 = epoch_level_gradients_SGD_2[0,1,-1]\n",
    "\n",
    "#concatanating the gradients \n",
    "concatanated_gradients = np.concatenate((gradient_set_1,gradient_set_2))\n",
    "\n",
    "# Plotting the concatenated array\n",
    "plt.plot(concatanated_gradients, marker = 'o')\n",
    "plt.title('Variation of Core Tensor Sub Problem Gradients')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Gradient')\n",
    "plt.yscale('log')\n",
    "plt.show()\n",
    "\n",
    "#gradient of core tensor\n",
    "function_set_1 = epoch_level_function[0,1,-1]\n",
    "function_set_2 = epoch_level_function_2[0,1,-1]\n",
    "\n",
    "#concatanating the gradients \n",
    "concatanated_gradients = np.concatenate((function_set_1,function_set_2))\n",
    "\n",
    "# Plotting the concatenated array\n",
    "plt.plot(concatanated_gradients, marker = 'o')\n",
    "plt.title('Variation of Core Tensor Sub Problem Function Value')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Gradient')\n",
    "plt.yscale('log')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to change the batch size of the core tensor sub problem from 64 - batch_size"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchtensor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
