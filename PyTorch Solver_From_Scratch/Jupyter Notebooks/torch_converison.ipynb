{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Low Separation Rank Tensor Decomposition\n",
    "class LSR_tensor_dot():\n",
    "    # Constructor\n",
    "    def __init__(self, shape, ranks, separation_rank, dtype = torch.float32, intercept = False ,initialize = True, device = torch.device(\"cpu\")):\n",
    "        super(LSR_tensor_dot, self).__init__()\n",
    "        self.shape = shape\n",
    "        self.ranks = ranks\n",
    "        self.separation_rank = separation_rank\n",
    "        self.dtype = dtype\n",
    "        self.order = len(shape)\n",
    "        self.init_params(initialize)\n",
    "        self.init_params(intercept)\n",
    "\n",
    "    # Initialize Parameters\n",
    "    def init_params(self, intercept = False ,initialize = True):\n",
    "        # Initialize core tensor as independent standard gaussians\n",
    "        if not initialize:\n",
    "            #self.core_tensor = np.zeros(shape = self.ranks)\n",
    "            self.core_tensor = torch.nn.parameter.Parameter(torch.zeros(self.ranks, device = self.device))\n",
    "        else:\n",
    "            #self.core_tensor = np.random.normal(size = self.ranks)\n",
    "            self.core_tensor = torch.nn.parameter.Parameter(torch.normal(mean=0,std=1,size=self.ranks,device=self.device))\n",
    "            \n",
    "        # Set up Factor Matrices\n",
    "        \n",
    "        #self.factor_matrices = []\n",
    "        self.factor_matrices = torch.nn.ModuleList()\n",
    "        \n",
    "        \n",
    "        # Initialize all factor matrices\n",
    "        for s in range(self.separation_rank):\n",
    "            factors_s = torch.nn.ParameterList()\n",
    "            for k in range(self.order):\n",
    "                if not initialize:\n",
    "                    #factor_matrix_B = np.eye(self.shape[k])[:, self.ranks[k]]\n",
    "                    factor_matrix_B = torch.eye(self.shape[k])[:,self.rank[k]]\n",
    "                    factors_s.append(factor_matrix_B)\n",
    "                else:\n",
    "                    #factor_matrix_A = np.random.normal(0,1,size= (self.shape[k], self.ranks[k]))\n",
    "                    factor_matrix_A = torch.normal(mean=0,std=1,size=[self.order[k],self.ranks[k]],dtype=self.dtype, device = self.device)\n",
    "                    factors_s.append(factor_matrix_A)\n",
    "\n",
    "            self.factor_matrices.append(factors_s)\n",
    "\n",
    "        if intercept:\n",
    "          ('intercept is initialized')\n",
    "          self.b = np. random.normal(0,1)\n",
    "        else:\n",
    "          (print('intercept is not initialized'))\n",
    "          self.b = 0\n",
    "\n",
    "    # Expand core tensor and factor matrices to full tensor, optionally excluding\n",
    "    # a given term from the separation rank decomposition\n",
    "    \n",
    "    def expand_to_tensor(self, skip_term = None):\n",
    "        full_lsr_tensor = torch.zeros(shape = self.shape)\n",
    "\n",
    "        #Calculate Expanded Tensor\n",
    "        for s, term_s_factors in enumerate(self.factor_matrices):\n",
    "            if s == skip_term: continue\n",
    "            expanded_tucker_term = term_s_factors[0] @ self.core_tensor @ term_s_factors[1].T\n",
    "            full_lsr_tensor += expanded_tucker_term\n",
    "\n",
    "        #Column Wise Flatten full_lsr_tensor\n",
    "        #full_lsr_tensor = full_lsr_tensor.flatten(order = 'F')\n",
    "        full_lsr_tensor = full_lsr_tensor.T.fatten() # the transposing make the column major flattening possible\n",
    "        return full_lsr_tensor\n",
    "\n",
    "    # Absorb all factor matrices and core tensor into the input tensor except for matrix s, k\n",
    "    # Used during a factor matrix update step of block coordiante descent\n",
    "\n",
    "    def bcd_factor_update_x_y(self, s, k, x, y):\n",
    "        # Convert inputs to PyTorch tensors if they are not already\n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "        y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "        # Take x and swap axes 1 and 2 so that vectorization occurs \"COLUMN WISE\"\n",
    "        x_transpose = x.transpose(1, 2)\n",
    "        x_transpose_vectorized = x_transpose.reshape(x_transpose.shape[0], -1)\n",
    "\n",
    "        # If we are unfolding along mode 1, use x. Else, if we are unfolding along mode, use x_transpose\n",
    "        x_partial_unfold = x if k == 0 else x_transpose\n",
    "\n",
    "        # If k = 0 (skip first factor matrix), we have 2nd factor matrix. If k = 1 (skip second factor matrix), we have first factor matrix\n",
    "        kronecker_term = self.factor_matrices[s][1] if k == 0 else self.factor_matrices[s][0]\n",
    "\n",
    "        # If k = 0, G^T. Else if k = 1, put G\n",
    "        core_tensor_term = self.core_tensor.T if k == 0 else self.core_tensor\n",
    "\n",
    "        omega = x_partial_unfold @ kronecker_term @ core_tensor_term\n",
    "        omega = omega.transpose(1, 2)\n",
    "        omega = omega.reshape(omega.shape[0], -1)\n",
    "\n",
    "        X_tilde = omega\n",
    "        y_tilde = y\n",
    "\n",
    "        if self.separation_rank != 1:\n",
    "            gamma = x_transpose_vectorized @ self.expand_to_tensor(skip_term=s)\n",
    "            y_tilde = y - gamma\n",
    "\n",
    "        return X_tilde, y_tilde\n",
    "\n",
    "    # Absorb all factor matrices the input tensor (not the core tensor)\n",
    "    # Used during a core tensor update step of block coordiante descent\n",
    "    \n",
    "    def bcd_core_update_x_y(self, x, y):\n",
    "        #Take x and swap axes 1 and 2 so that vectorization occurs \"COLUMN WISE\"\n",
    "        x_transpose = np.transpose(x, (0, 2, 1))\n",
    "        x_transpose_vectorized = np.reshape(x_transpose, newshape = (x_transpose.shape[0], -1))\n",
    "\n",
    "        #Calculate y_tilde\n",
    "        y_tilde = y\n",
    "\n",
    "        #Calculate Kronecker Factor Sum\n",
    "        kron_factor_sum = 0\n",
    "        for term_s_factors in self.factor_matrices:\n",
    "            kron_factor_sum += np.kron(term_s_factors[1], term_s_factors[0])\n",
    "\n",
    "        #Return Core Update\n",
    "        return (kron_factor_sum.T @ x_transpose_vectorized.T).T, y_tilde\n",
    "\n",
    "\n",
    "    #Retrieve factor matrix\n",
    "    def get_factor_matrix(self, s, k):\n",
    "      return self.factor_matrices[s][k]\n",
    "\n",
    "    #Update factor matrix\n",
    "    def update_factor_matrix(self, s, k, updated_factor_matrix: np.ndarray):\n",
    "      self.factor_matrices[s][k] = updated_factor_matrix\n",
    "\n",
    "    def update_intercept(self,updated_b):\n",
    "      self.b = updated_b\n",
    "\n",
    "    #Retrieve Core Matrix\n",
    "    def get_core_matrix(self):\n",
    "      return self.core_tensor\n",
    "\n",
    "    #Update core matrix\n",
    "    def update_core_matrix(self, updated_core_matrix: np.ndarray):\n",
    "      self.core_tensor = updated_core_matrix\n",
    "\n",
    "    #Retrive intercept\n",
    "    def get_intercept(self):\n",
    "      return self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 2, 4)\n",
      "[[[0.75315588 0.47029232 0.99990486 0.24514298]\n",
      "  [0.98507055 0.12298366 0.58938297 0.1345212 ]]\n",
      "\n",
      " [[0.61641096 0.21658602 0.63565466 0.61150186]\n",
      "  [0.78475318 0.4756201  0.83391043 0.49277239]]\n",
      "\n",
      " [[0.12865372 0.61475575 0.48189199 0.83594962]\n",
      "  [0.1065275  0.41471524 0.69178931 0.77946007]]]\n",
      "torch.Size([3, 2, 4])\n",
      "torch.Size([3, 4, 2])\n"
     ]
    }
   ],
   "source": [
    "array_random = np.random.random((3, 2, 4))\n",
    "print(array_random.shape)\n",
    "print(array_random)\n",
    "a = torch.tensor(array_random,dtype=torch.float32)\n",
    "print(a.shape)\n",
    "print(a.transpose(1,2).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchtensor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
