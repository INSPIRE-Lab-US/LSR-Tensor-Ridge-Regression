{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Needed Libraries \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "###system \n",
    "import sys\n",
    "sys.path.append('/Users/lakrama/Neuro Project Codes/LSR-Tensor-Ridge-Regression/Platforms for Experments/CodeFiles')\n",
    "\n",
    "###loading and saving data\n",
    "import pickle\n",
    "import dill\n",
    "import datetime\n",
    "\n",
    "###bases\n",
    "import numpy as np\n",
    "\n",
    "import copy\n",
    "\n",
    "#preprocessing \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading all the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 32, 32)\n",
      "(5000,)\n",
      "(1000, 32, 32)\n",
      "(1000,)\n",
      "(32, 32)\n",
      "(4, 4)\n",
      "(6000,)\n",
      "factor matrix 1_1:(32, 4)\n",
      "factor matrix 1_1:(32, 4)\n",
      "factor matrix 1_1:(32, 4)\n",
      "factor matrix 1_1:(32, 4)\n"
     ]
    }
   ],
   "source": [
    "file= open(\"/Users/lakrama/Neuro Project Codes/Datasets/Data_Sets/Synthetic Data/Uncentered X/Bounded_Var_Time:2024-04-15 19:35:40, intercept:5,n_train:5000, n_test:1000, tensor_dimensions:[32 32], tensor_mode_ranks:[4 4], separation_rank:2.pkl\", 'rb')\n",
    "data = pickle.load(file)\n",
    "file.close()\n",
    "\n",
    "X_train_Full = data[0]\n",
    "print(data[0].shape)\n",
    "\n",
    "Y_train_Full = data[1]\n",
    "print(data[1].shape)\n",
    "\n",
    "X_test_Full = data[2]\n",
    "print(data[2].shape)\n",
    "\n",
    "Y_test_Full = data[3]\n",
    "print(data[3].shape)\n",
    "\n",
    "B_tensored = data[4]\n",
    "print(data[4].shape)\n",
    "\n",
    "G1 = data[5]\n",
    "print(data[5].shape)\n",
    "\n",
    "all_factormatrices = data[6]\n",
    "#print(data[6].shape)\n",
    "\n",
    "Y_train_nonoise = data[7]\n",
    "print(data[7].shape)\n",
    "\n",
    "#all the factor matrices \n",
    "B_1_1 = all_factormatrices[0][0]\n",
    "print(f'factor matrix 1_1:{B_1_1.shape}')\n",
    "B_1_2 = all_factormatrices[0][1]\n",
    "print(f'factor matrix 1_1:{B_1_2.shape}')\n",
    "B_2_1 = all_factormatrices[1][0]\n",
    "print(f'factor matrix 1_1:{B_2_1.shape}')\n",
    "B_2_2 = all_factormatrices[0][0]\n",
    "print(f'factor matrix 1_1:{B_2_2.shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All Needed Scripts "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LSR Tensor Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Low Separation Rank Tensor Decomposition\n",
    "class LSR_tensor_dot():\n",
    "    # Constructor\n",
    "    def __init__(self, shape, ranks, separation_rank, dtype = np.float32, intercept = False ,initialize = True):\n",
    "        super(LSR_tensor_dot, self).__init__()\n",
    "        self.shape = shape\n",
    "        self.ranks = ranks\n",
    "        self.separation_rank = separation_rank\n",
    "        self.dtype = dtype\n",
    "        self.order = len(shape)\n",
    "        self.init_params(initialize)\n",
    "        self.init_params(intercept)\n",
    "\n",
    "    # Initialize Parameters\n",
    "    def init_params(self, intercept = False ,initialize = True):\n",
    "        # Initialize core tensor as independent standard gaussians\n",
    "        if not initialize:\n",
    "            self.core_tensor = np.zeros(shape = self.ranks)\n",
    "        else:\n",
    "            self.core_tensor = np.random.normal(size = self.ranks)\n",
    "\n",
    "        # Set up Factor Matrices\n",
    "        self.factor_matrices = []\n",
    "\n",
    "        # Initialize all factor matrices\n",
    "        for s in range(self.separation_rank):\n",
    "            factors_s = []\n",
    "            for k in range(self.order):\n",
    "                if not initialize:\n",
    "                    factor_matrix_B = np.eye(self.shape[k])[:, self.ranks[k]]\n",
    "                    factors_s.append(factor_matrix_B)\n",
    "                else:\n",
    "                    factor_matrix_A = np.random.normal(0,1,size= (self.shape[k], self.ranks[k]))\n",
    "                    factors_s.append(factor_matrix_A)\n",
    "\n",
    "            self.factor_matrices.append(factors_s)\n",
    "\n",
    "        if intercept:\n",
    "          ('intercept is initialized')\n",
    "          self.b = np. random.normal(0,1)\n",
    "        else:\n",
    "          (print('intercept is not initialized'))\n",
    "          self.b = 0\n",
    "\n",
    "    # Expand core tensor and factor matrices to full tensor, optionally excluding\n",
    "    # a given term from the separation rank decomposition\n",
    "    def expand_to_tensor(self, skip_term = None):\n",
    "        full_lsr_tensor = np.zeros(shape = self.shape)\n",
    "\n",
    "        #Calculate Expanded Tensor\n",
    "        for s, term_s_factors in enumerate(self.factor_matrices):\n",
    "            if s == skip_term: continue\n",
    "            expanded_tucker_term = term_s_factors[0] @ self.core_tensor @ term_s_factors[1].T\n",
    "            full_lsr_tensor += expanded_tucker_term\n",
    "\n",
    "        #Column Wise Flatten full_lsr_tensor\n",
    "        full_lsr_tensor = full_lsr_tensor.flatten(order = 'F')\n",
    "        return full_lsr_tensor\n",
    "\n",
    "    # Absorb all factor matrices and core tensor into the input tensor except for matrix s, k\n",
    "    # Used during a factor matrix update step of block coordiante descent\n",
    "    def bcd_factor_update_x_y(self, s, k, x, y):\n",
    "        #Take x and swap axes 1 and 2 so that vectorization occurs \"COLUMN WISE\"\n",
    "        x_transpose = np.transpose(x, (0, 2, 1))\n",
    "        x_transpose_vectorized = np.reshape(x_transpose, newshape = (x_transpose.shape[0], -1))\n",
    "\n",
    "        #if we are unfolding along mode 1, use x. Else, if we are unfolding along mode, use x_transpose\n",
    "        x_partial_unfold = x if k == 0 else x_transpose\n",
    "\n",
    "        #If k = 0(skip first factor matrix), we have 2nd factor matrix. If k= 1(skip second factor matrix), we have first factor matrix\n",
    "        kronecker_term = self.factor_matrices[s][1] if k == 0 else self.factor_matrices[s][0]\n",
    "\n",
    "        #if k = 0, G^T. Else if k = 1, put G\n",
    "        core_tensor_term = self.core_tensor.T if k == 0 else self.core_tensor\n",
    "\n",
    "        omega = x_partial_unfold @ kronecker_term @ core_tensor_term\n",
    "        omega = np.transpose(omega, (0, 2, 1))\n",
    "        omega = np.reshape(omega, newshape = (omega.shape[0], -1))\n",
    "\n",
    "        X_tilde = omega\n",
    "        y_tilde = y\n",
    "\n",
    "        if self.separation_rank == 1:\n",
    "            pass\n",
    "        else:\n",
    "            gamma = np.dot(x_transpose_vectorized,self.expand_to_tensor(skip_term = s))\n",
    "            #gamma = gamma.reshape(-1,1)\n",
    "            y_tilde = y - gamma\n",
    "\n",
    "        return X_tilde, y_tilde\n",
    "\n",
    "    # Absorb all factor matrices the input tensor (not the core tensor)\n",
    "    # Used during a core tensor update step of block coordiante descent\n",
    "    def bcd_core_update_x_y(self, x, y):\n",
    "        #Take x and swap axes 1 and 2 so that vectorization occurs \"COLUMN WISE\"\n",
    "        x_transpose = np.transpose(x, (0, 2, 1))\n",
    "        x_transpose_vectorized = np.reshape(x_transpose, newshape = (x_transpose.shape[0], -1))\n",
    "\n",
    "        #Calculate y_tilde\n",
    "        y_tilde = y\n",
    "\n",
    "        #Calculate Kronecker Factor Sum\n",
    "        kron_factor_sum = 0\n",
    "        for term_s_factors in self.factor_matrices:\n",
    "            kron_factor_sum += np.kron(term_s_factors[1], term_s_factors[0])\n",
    "\n",
    "        #Return Core Update\n",
    "        return (kron_factor_sum.T @ x_transpose_vectorized.T).T, y_tilde\n",
    "\n",
    "\n",
    "    #Retrieve factor matrix\n",
    "    def get_factor_matrix(self, s, k):\n",
    "      return self.factor_matrices[s][k]\n",
    "\n",
    "    #Update factor matrix\n",
    "    def update_factor_matrix(self, s, k, updated_factor_matrix: np.ndarray):\n",
    "      self.factor_matrices[s][k] = updated_factor_matrix\n",
    "\n",
    "    def update_intercept(self,updated_b):\n",
    "      self.b = updated_b\n",
    "\n",
    "    #Retrieve Core Matrix\n",
    "    def get_core_matrix(self):\n",
    "      return self.core_tensor\n",
    "\n",
    "    #Update core matrix\n",
    "    def update_core_matrix(self, updated_core_matrix: np.ndarray):\n",
    "      self.core_tensor = updated_core_matrix\n",
    "\n",
    "    #Retrive intercept\n",
    "    def get_intercept(self):\n",
    "      return self.b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BCD Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lsr_ten: LSR Tensor\n",
    "#training_data: X\n",
    "#training_labels: Y\n",
    "#hypers: hyperparameters\n",
    "def lsr_bcd_regression(lsr_ten, training_data: np.ndarray, training_labels: np.ndarray, hypers: dict,intercept = False):\n",
    "    #Get LSR Tensor Information and other hyperparameters\n",
    "    shape, ranks, sep_rank, order = lsr_ten.shape, lsr_ten.ranks, lsr_ten.separation_rank, lsr_ten.order\n",
    "    lambda1 = hypers[\"weight_decay\"]\n",
    "    max_iter = hypers[\"max_iter\"]\n",
    "    threshold = hypers[\"threshold\"]\n",
    "    lr        = hypers[\"learning_rate\"]\n",
    "    epochs    = hypers[\"epochs\"]\n",
    "    batch_size = hypers[\"batch_size\"]\n",
    "    decay_factor = hypers[\"decay_factor\"]\n",
    "    b_intercept = intercept\n",
    "\n",
    "    #Create models for each factor matrix and core matrix\n",
    "    #factor_matrix_models = [[Ridge(alpha = lambda1, solver = 'svd', fit_intercept = intercept) for k in range(len(ranks))] for s in range(sep_rank)]\n",
    "    #core_tensor_model = Ridge(alpha = lambda1, solver = 'svd', fit_intercept = intercept)\n",
    "\n",
    "    #Store objective function values\n",
    "    objective_function_values = np.ones(shape = (max_iter, sep_rank, len(ranks) + 1)) * np.inf\n",
    "\n",
    "    X, y = training_data, training_labels\n",
    "    if intercept: b_start = lsr_ten.get_intercept()\n",
    "    expanded_lsr_start  = lsr_ten.expand_to_tensor()\n",
    "    expanded_lsr_start  = np.reshape(expanded_lsr_start, X[0].shape, order = 'F')\n",
    "    objective_function_value_star = objective_function_tensor_sep(y, X, expanded_lsr_start,lsr_ten, lambda1, b if intercept else None)\n",
    "    print('Objective Function Value:',objective_function_value_star)\n",
    "\n",
    "    #Normalized Estimation Error\n",
    "    iterations_normalized_estimation_error = np.zeros(shape = (max_iter,))\n",
    "    \n",
    "    #Gradient Values\n",
    "    gradient_values = np.ones(shape = (max_iter, sep_rank, len(ranks) + 1)) * np.inf\n",
    "\n",
    "    #Epoch Level Function Values \n",
    "    epoch_level_function_values = np.ones(shape = (max_iter,sep_rank,len(ranks)+1,epochs))*np.inf\n",
    "\n",
    "    #Epoch Level Gradients\n",
    "    epoch_gradient_values = np.ones(shape = (max_iter,sep_rank,len(ranks)+1,epochs))*np.inf\n",
    "\n",
    "    #saving the tensor\n",
    "    tensor_iteration = []\n",
    "    #saving iterate-wise data\n",
    "    factor_core_iterates = []\n",
    "\n",
    "    #iterate differences \n",
    "    iterate_difference = np.ones(shape = (max_iter, sep_rank, len(ranks) + 1)) * np.inf\n",
    "\n",
    "    #Run at most max_iter iterations of Block Coordinate Descent\n",
    "    for iteration in range(max_iter):\n",
    "        #print('')\n",
    "        print('--------------------------------------------------------------BCD iteration',iteration,'--------------------------------------------------------------')\n",
    "        factor_residuals = np.zeros(shape = (sep_rank, len(ranks)))\n",
    "        core_residual = 0\n",
    "\n",
    "        #Store updates to factor matrices and core tensor\n",
    "        updated_factor_matrices = np.empty((sep_rank, len(ranks)), dtype=object)\n",
    "        updated_core_tensor = None\n",
    "\n",
    "        #Iterate over the Factor Matrices.\n",
    "        for s in range(sep_rank):\n",
    "            for k in range(len(ranks)):\n",
    "                \n",
    "                #Absorb Factor Matrices into X aside from (s, k) to get X_tilde\n",
    "                print('---------------------------------------------Sep',s,'Factor',k,'-------------------------------------------------')\n",
    "                X, y = training_data, training_labels\n",
    "                X_tilde, y_tilde = lsr_ten.bcd_factor_update_x_y(s, k, X, y) #y tilde should now be y-b-<Q,X>\n",
    "                \n",
    "\n",
    "                #Solve the sub-problem pertaining to the factor tensor\n",
    "                hypers = {'lambda': lambda1, 'lr': lr, 'epochs': epochs, 'batch_size': batch_size, 'bias': b_intercept, 'decay_factor': decay_factor}\n",
    "                #weights, bias, loss_values, gap_to_optimality, nmse_values, corr_values, R2_values,sub_problem_gradient = SGD(X_tilde, y_tilde.reshape(-1,1), cost_function_code = 1, hypers = hypers , optimizer_code = 0, p_star = 0)\n",
    "                \n",
    "                sub_problem_gradient = 0\n",
    "                loss_values = 0\n",
    "                bias = 0\n",
    "\n",
    "\n",
    "                #printing the subproblem gradients\n",
    "                #print(f\"Final gradient of the subproblem {s,k} : {sub_problem_gradient[-1]}\")\n",
    "                epoch_gradient_values[iteration,s,k,:] = sub_problem_gradient\n",
    "                epoch_level_function_values[iteration,s,k,:] = loss_values\n",
    "\n",
    "                #Retrieve Original and Updated Factor Matrices\n",
    "                #Bk = lsr_ten.get_factor_matrix(s, k)\n",
    "                #Bk1 = weights\n",
    "                if intercept: b = bias\n",
    "\n",
    "                #Shape Bk1 as needed\n",
    "                #Bk1 = np.reshape(Bk1, (shape[k], ranks[k]), order = 'F') #if there is an error check here\n",
    "\n",
    "                #fixing the factor matrices to be the true value \n",
    "                Bk  = all_factormatrices[s][k]\n",
    "                Bk1 = all_factormatrices[s][k]\n",
    "\n",
    "\n",
    "                #Update Residuals and store updated factor matrix\n",
    "                factor_residuals[s][k] = np.linalg.norm(Bk1 - Bk)\n",
    "                updated_factor_matrices[s, k] = Bk1\n",
    "\n",
    "                iterate_difference[iteration,s,k] = factor_residuals[s][k]\n",
    "\n",
    "\n",
    "                #Update Factor Matrix\n",
    "                lsr_ten.update_factor_matrix(s, k, updated_factor_matrices[s, k])\n",
    "\n",
    "                #update the intercept\n",
    "                if intercept: lsr_ten.update_intercept(b)\n",
    "\n",
    "                #Calculate Objective Function Value\n",
    "                expanded_lsr = lsr_ten.expand_to_tensor()\n",
    "                expanded_lsr = np.reshape(expanded_lsr, X[0].shape, order = 'F')\n",
    "                objective_function_value = objective_function_tensor_sep(y, X, expanded_lsr,lsr_ten,lambda1)\n",
    "                objective_function_values[iteration, s, k] = objective_function_value\n",
    "\n",
    "                #Print Objective Function Values\n",
    "                print(f\"Iteration: {iteration}, Separation Rank: {s}, Factor Matrix: {k}, Objective Function Value: {objective_function_value}\")\n",
    "                \n",
    "                #Calculate Gradient Values\n",
    "                bk = np.reshape(Bk, (-1, 1), order = 'F') #Flatten Factor Matrix Column Wise\n",
    "                Omega = X_tilde\n",
    "                z = bias\n",
    "                gradient_value = (-2 * Omega.T) @ (y_tilde.reshape(-1,1) - Omega @ bk  - z) + (2 * lambda1 * bk)\n",
    "                \n",
    "                #Store Gradient Values\n",
    "                gradient_values[iteration, s, k] = np.linalg.norm(gradient_value, ord = 'fro')\n",
    "\n",
    "\n",
    "        #Absorb necessary matrices into X, aside from core tensor, to get X_tilde\n",
    "        print('---------------------------------------------Core-------------------------------------------------')\n",
    "        X, y = training_data, training_labels\n",
    "        X_tilde, y_tilde = lsr_ten.bcd_core_update_x_y(X, y)\n",
    "\n",
    "        #Solve the sub-problem pertaining to the core tensor\n",
    "        hypers = {'lambda': lambda1, 'lr': lr, 'epochs': epochs, 'batch_size': 64, 'bias': intercept, 'decay_factor':decay_factor}\n",
    "        weights, bias, loss_values, gap_to_optimality, nmse_values, corr_values, R2_values,sub_problem_gradient,loss_values = SGD(X_tilde, y_tilde.reshape(-1,1), cost_function_code = 1, hypers = hypers, optimizer_code = 0, p_star = 0)\n",
    "\n",
    "        print(f\"Final gradient of the subproblem Core : {sub_problem_gradient[-1]}\")\n",
    "        epoch_gradient_values[iteration,:,len(ranks),:] = sub_problem_gradient\n",
    "        epoch_level_function_values[iteration,:,len(ranks),:] = loss_values\n",
    "\n",
    "        #Get Original and Updated Core Tensor\n",
    "        Gk = lsr_ten.get_core_matrix()\n",
    "        Gk1 = np.reshape(weights, ranks, order = 'F')\n",
    "        b = bias\n",
    "\n",
    "        #Update Residuals and store updated Core Tensor\n",
    "        core_residual = np.linalg.norm(Gk1 - Gk)\n",
    "        updated_core_tensor = Gk1\n",
    "\n",
    "        #saving iterate differece in a list\n",
    "        iterate_difference[iteration,:,len(ranks)] = core_residual\n",
    "        \n",
    "        #Update Core Tensor\n",
    "        lsr_ten.update_core_matrix(updated_core_tensor)\n",
    "\n",
    "\n",
    "        #Update Intercept\n",
    "\n",
    "        if intercept: lsr_ten.update_intercept(b)\n",
    "\n",
    "        #Calculate Objective Function Value\n",
    "        if intercept: b = lsr_ten.get_intercept()\n",
    "        expanded_lsr = lsr_ten.expand_to_tensor()\n",
    "        expanded_lsr = np.reshape(expanded_lsr, X[0].shape, order = 'F')\n",
    "\n",
    "        #saving the lsr tensor \n",
    "        tensor_iteration.append(expanded_lsr)\n",
    "\n",
    "        objective_function_value = objective_function_tensor_sep(y, X, expanded_lsr,lsr_ten, lambda1, b if intercept else None)\n",
    "        objective_function_values[iteration, :, (len(ranks))] = objective_function_value\n",
    "        \n",
    "        #print('')\n",
    "        #Print Objective Function Value\n",
    "        # print(f\"BCD Regression, Iteration: {iteration}, Core Tensor, Objective Function Value: {objective_function_value}\")\n",
    "        \n",
    "        #Calculate Gradient Values\n",
    "        g = np.reshape(Gk, (-1, 1), order = 'F') #Flatten Core Matrix Column Wise\n",
    "        Omega = X_tilde\n",
    "        z = bias\n",
    "        gradient_value = (-2 * Omega.T) @ (y_tilde.reshape(-1,1) - Omega @ g  - z) + (2 * lambda1 * g)\n",
    "        \n",
    "        #Store Gradient Value\n",
    "        gradient_values[iteration, :, (len(ranks))] = np.linalg.norm(gradient_value, ord='fro')\n",
    "\n",
    "        #storing lsr_ten\n",
    "        factor_core_iterates.append(copy.deepcopy(lsr_ten))\n",
    "\n",
    "        #Stopping Criteria\n",
    "        diff = np.sum(factor_residuals.flatten()) + core_residual  #need to change this\n",
    "        # print('------------------------------------------------------------------------------------------')\n",
    "        # print(f\"Value of Stopping Criteria: {diff}\")\n",
    "        # print(f\"Expanded Tensor: {expanded_lsr}\")\n",
    "        # print('------------------------------------------------------------------------------------------')\n",
    "        if diff < threshold: \n",
    "            print(f'The threshold activated{diff}')\n",
    "            break\n",
    "\n",
    "\n",
    "    return lsr_ten, objective_function_values, gradient_values,iterate_difference,epoch_gradient_values,epoch_level_function_values,tensor_iteration,factor_core_iterates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Contains All Helper Functions for Optimization\n",
    "#Calculate value of objective function(vectorized case)\n",
    "def objective_function_vectorized(y: np.ndarray, X: np.ndarray, w: np.ndarray, alpha, b = None):\n",
    "    I = (X @ w).flatten()\n",
    "    y = y.flatten()\n",
    "    w = w.flatten()\n",
    "\n",
    "    if b is not None:\n",
    "      b = b.flatten()\n",
    "      function = (np.linalg.norm(y - I - b) ** 2) + (alpha * (np.linalg.norm(w) ** 2))\n",
    "    else:\n",
    "      function = (np.linalg.norm(y - I) ** 2) + (alpha * (np.linalg.norm(w) ** 2))\n",
    "    return function\n",
    "\n",
    "#Calculate value of objective function(tensor case)\n",
    "def objective_function_tensor(y: np.ndarray, X: np.ndarray, B: np.ndarray, alpha,b = None):\n",
    "    I = inner_product(X, B).flatten()\n",
    "    y = y.flatten()\n",
    "    B = B.flatten()\n",
    "    if b is not None:\n",
    "      b = b.flatten()\n",
    "      function = (np.linalg.norm(y - I -b) ** 2) + (alpha * (np.linalg.norm(B) ** 2))\n",
    "    else:\n",
    "      function = (np.linalg.norm(y - I) ** 2) + (alpha * (np.linalg.norm(B) ** 2))\n",
    "    return function\n",
    "\n",
    "#Calculate x* and p* for Objective Function(Tensor Case)\n",
    "#X_train is a Tensor of samples x m x n\n",
    "#Y_train is a normal vector of size samples x 1. It can also be a flattened array of size (samples, )\n",
    "def calculate_optimal_iterate_and_function_value(X_train: np.ndarray, Y_train: np.ndarray, lambda1):\n",
    "    X_train = X_train.reshape((X_train.shape[0], -1))\n",
    "    Y_train = Y_train.reshape((-1, 1))\n",
    "\n",
    "    #Calculate Optimal Weight Tensor and Optimal Objective Function Value\n",
    "    B_optimal = np.linalg.inv(X_train.T @ X_train + lambda1 * np.eye(X_train.shape[1])) @ X_train.T @ Y_train\n",
    "    I = X_train @ B_optimal\n",
    "    p_star = (np.linalg.norm(Y_train - I) ** 2) + (lambda1 * (np.linalg.norm(B_optimal) ** 2))\n",
    "\n",
    "    return B_optimal, p_star\n",
    "\n",
    "\n",
    "#Inner product of two tensors\n",
    "#tensor1: samples x m x n\n",
    "#tensor2: m x n\n",
    "def inner_product(tensor1: np.ndarray, tensor2: np.ndarray):\n",
    "    tensor1 = tensor1.reshape(tensor1.shape[0], -1)\n",
    "    tensor2 = tensor2.reshape(-1, 1)\n",
    "    return tensor1 @ tensor2\n",
    "\n",
    "#Calculate R2 Score\n",
    "def R2(y_true, y_pred):\n",
    "    #Flatten for insurance\n",
    "    y_true = y_true.flatten()\n",
    "    y_pred = y_pred.flatten()\n",
    "\n",
    "    #Calculate R2 Score\n",
    "    y_true_mean = np.mean(y_true)\n",
    "    tss = np.sum((y_true - y_true_mean) ** 2)\n",
    "    rss = np.sum((y_true - y_pred) ** 2)\n",
    "    r2 = 1 - (rss / tss)\n",
    "    return r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import Optimization Toolkits\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import r2_score\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Cost Function[Least Squares]\n",
    "#||(XW + b) - Y||_2^2\n",
    "class LeastSquares(nn.Module):\n",
    "    def __init__(self, input_dim, uses_bias = False):\n",
    "        super(LeastSquares, self).__init__() #Initialize class\n",
    "        self.linear = nn.Linear(input_dim, 1, uses_bias) #input_dim = dimension of each sample in X, output_dim = 1 = number of y values for each sample\n",
    "                \n",
    "    #Evaluate the Cost Function given X and y\n",
    "    def evaluate(self, X, Y, reduction = 'sum'):\n",
    "        mse_loss = nn.MSELoss(reduction = reduction)\n",
    "        return mse_loss(self.linear(X), Y.rehshape(-1,1))\n",
    "\n",
    "#Cost Function[Least Squares + L2 Regularization Term]\n",
    "#||(XW + b) - Y ||_2^2 + lambda * ||w||^2_2\n",
    "class RidgeRegression(nn.Module):\n",
    "    def __init__(self, input_dim, lmbd, uses_bias = False):\n",
    "        super(RidgeRegression, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, 1, uses_bias) #input_dim = dimension of each sample in X, output_dim = 1 = number of y values for each sample\n",
    "        self.lmbd = lmbd #Ridge Regression Lambda Value\n",
    "        \n",
    "    #Evaluate the Cost Function given X and y\n",
    "    def evaluate(self, X, Y, reduction = 'sum'):\n",
    "        mse_loss = nn.MSELoss(reduction = reduction)\n",
    "        return mse_loss(self.linear(X), Y) + self.l2_regularization()\n",
    "            \n",
    "    #Calculate value of lambda * ||w||^2_2\n",
    "    def l2_regularization(self):\n",
    "        return self.lmbd * (torch.norm(self.linear.weight) ** 2)\n",
    "    \n",
    "#Perform Exact Line Search for Ridge Regression\n",
    "#Ridge Regression: ||(XW + b) - Y ||_2^2 + lambda * ||w||^2_2\n",
    "def exact_line_search_RR(X: np.ndarray, Y: np.ndarray, lmbd, cost_function, uses_bias):\n",
    "    #Get Model Parameters\n",
    "    W = cost_function.linear.weight.data.numpy().reshape((-1, 1)) \n",
    "    b = cost_function.linear.bias.item() if uses_bias else 0\n",
    "    \n",
    "    #Search Direction\n",
    "    DeltaW = -1 * cost_function.linear.weight.grad.numpy().reshape((-1, 1))\n",
    "    Deltab = -1 * cost_function.linear.bias.grad if uses_bias else 0\n",
    "    \n",
    "    #Compute value of t\n",
    "    numerator = -((X@W + b - Y).T @ (X @ DeltaW + Deltab)) - (lmbd * (W.T @ DeltaW))\n",
    "    denominator = (np.linalg.norm(X @ DeltaW + Deltab) ** 2) + (lmbd * (np.linalg.norm(DeltaW) ** 2))\n",
    "    t = (numerator / denominator) [0, 0]\n",
    "    \n",
    "    return t    \n",
    "    \n",
    "#Optimize a Cost Function via Stochastic Gradient Descent\n",
    "#X: Shape n x d where n is the number of samples and d is the number of features\n",
    "#Y: Shape n x 1 where n is the number of samples\n",
    "#cost_function_code: 0 for Normal Least Squares, 1 for Ridge Regression\n",
    "#hypers: hyperparameters\n",
    "#optimizer_code: 0 for SGD, 1 for Adagrad, 2 for RMSProp\n",
    "#p_star: estimated optimal value\n",
    "#W_true: true weights\n",
    "def SGD(X: np.ndarray, Y: np.ndarray, cost_function_code = 1, hypers = {}, optimizer_code = 0, p_star = 0, W_true = None):\n",
    "    hypers = defaultdict(int, hypers) #Convert hypers to defaultdict\n",
    "\n",
    "    #Get necessary hyperparameters\n",
    "    uses_bias = hypers['bias'] #determine whether the bias term is needed\n",
    "    lmbd = hypers['lambda'] #Lambda for ridge regression\n",
    "    lr = hypers['lr'] #learning rate\n",
    "    epochs = hypers['epochs'] #number of epochs\n",
    "    batch_size = hypers['batch_size'] #batch size to use for SGD\n",
    "\n",
    "    #Get additional hyperparameters\n",
    "    momentum = hypers['momentum']\n",
    "    dampening = hypers['dampening']\n",
    "    nesterov = hypers['nesterov']\n",
    "    decay_factor = hypers['decay_factor']\n",
    "    \n",
    "    #Initialize Cost Function\n",
    "    if cost_function_code == 0:\n",
    "        cost_function = LeastSquares(X.shape[1], uses_bias)\n",
    "    elif cost_function_code == 1:\n",
    "        cost_function = RidgeRegression(X.shape[1], lmbd, uses_bias)\n",
    "    \n",
    "    #Convert X and Y to pytorch tensors\n",
    "    X = torch.tensor(X, dtype = torch.float32)\n",
    "    Y = torch.tensor(Y, dtype = torch.float32)\n",
    "    \n",
    "    #Splitting data into minibatches \n",
    "    dataset = TensorDataset(X,Y)\n",
    "    dataloader = DataLoader(dataset, batch_size = batch_size,  shuffle=True )\n",
    "    \n",
    "    #Initialize Optimizer\n",
    "    if optimizer_code == 0:\n",
    "        optimizer = optim.SGD(cost_function.parameters(), lr = lr, momentum = momentum, dampening = dampening, nesterov = nesterov)\n",
    "    elif optimizer_code == 1:\n",
    "        optimizer = optim.Adagrad(cost_function.parameters(), lr = lr)\n",
    "    elif optimizer_code == 2:\n",
    "        optimizer = optim.RMSprop(cost_function.parameters(), lr = lr, alpha = decay_factor, momentum = momentum)\n",
    "    \n",
    "    #Store batch loss values\n",
    "    loss_values = []\n",
    "    \n",
    "    #Store gap to optimality\n",
    "    gap_to_optimality = []\n",
    "\n",
    "    #saving gradient \n",
    "    sub_problem_gradient =  []\n",
    "    \n",
    "    #Store Metric Values\n",
    "\n",
    "    nmse_values = []\n",
    "    corr_values = []\n",
    "    R2_values = []\n",
    "\n",
    "    #Training Loop\n",
    "    for epoch in range(epochs):\n",
    "    \n",
    "        num_batches = 0\n",
    "        sum_gradient_norm = 0\n",
    "\n",
    "        for X_sample, Y_sample in dataloader:\n",
    "        \n",
    "            Y_sample = Y_sample.reshape(-1,1)\n",
    "\n",
    "            # Compute stochastic loss\n",
    "            stochastic_loss = cost_function.evaluate(X_sample, Y_sample, 'sum')\n",
    "            \n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Backward pass to compute stochastic gradient\n",
    "            stochastic_loss.backward()\n",
    "            \n",
    "            # Update parameters\n",
    "            optimizer.step()\n",
    "            \n",
    "            #storing the batch wise gradients \n",
    "            for param in cost_function.parameters():\n",
    "               #print(f\"Gradient Norm: {torch.norm(param.grad)}\")\n",
    "               stochastic_gradient  = torch.norm(param.grad)\n",
    "               sum_gradient_norm += stochastic_gradient\n",
    "            \n",
    "            num_batches += 1\n",
    "\n",
    "        #Print and Store batch loss values\n",
    "        batch_loss = cost_function.evaluate(X, Y, 'sum')\n",
    "        loss_values.append(batch_loss.item())\n",
    "        gap_to_optimality.append(batch_loss.item() - p_star)\n",
    "\n",
    "        #Zero gradients\n",
    "        #optimizer.zero_grad()\n",
    "\n",
    "        # Backward pass to compute stochastic gradient\n",
    "        #batch_loss.backward()\n",
    "\n",
    "        #for param in cost_function.parameters():\n",
    "        #       #print(f\"Gradient Norm: {torch.norm(param.grad)}\")\n",
    "        #       gradient_after_epoch = torch.norm(param.grad)\n",
    "        #       sub_problem_gradient.append(gradient_after_epoch)\n",
    "\n",
    "        epcoh_gradient = sum_gradient_norm/num_batches\n",
    "        sub_problem_gradient.append(epcoh_gradient)\n",
    "        \n",
    "        #Calculate Metrics\n",
    "        weights = cost_function.linear.weight.data.numpy().reshape((-1, 1))\n",
    "        bias = cost_function.linear.bias.item() if uses_bias else 0\n",
    "        X_numpy = X.numpy()\n",
    "        Y_predicted = X_numpy @ weights + bias\n",
    "        Y_numpy = Y.numpy()\n",
    "        \n",
    "\n",
    "        nmse = np.sum(np.square((Y_predicted - Y_numpy))) / np.sum(np.square(Y_numpy))\n",
    "        correlation = np.corrcoef(Y_predicted.flatten(), Y_numpy.flatten())[0, 1]\n",
    "        R2_score = r2_score(Y_numpy, Y_predicted)\n",
    "        \n",
    "        nmse_values.append(nmse)\n",
    "        corr_values.append(correlation)\n",
    "        R2_values.append(R2_score)\n",
    "        \n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {batch_loss:.4f}, Gap to Optimality: {gap_to_optimality[-1]:.4f}, NMSE: {nmse}, Correlation: {correlation}, R2: {R2_score}')\n",
    "\n",
    "    # Create a figure with two horizontal subplots\n",
    "    #fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "    # Plotting in the first subplot (ax1)\n",
    "    #ax1.plot(range(1, len(loss_values) + 1), loss_values, marker='o')\n",
    "    #ax1.set_title('Subproblem Loss')\n",
    "    #ax1.set_yscale('log')\n",
    "    #ax1.set_xlabel('Epoch')\n",
    "    #ax1.set_ylabel('Loss')\n",
    "    #ax1.grid(True)\n",
    "\n",
    "    # Plotting in the second subplot (ax2)\n",
    "    #ax2.plot(range(1, len(sub_problem_gradient) + 1), sub_problem_gradient, marker='o')\n",
    "    #ax2.set_title('Subproblem Gradient')\n",
    "    #ax2.set_yscale('log')\n",
    "    #ax2.set_xlabel('Epoch')\n",
    "    #ax2.set_ylabel('Loss')\n",
    "    #ax2.grid(True)\n",
    "\n",
    "    # Adjust the layout to prevent overlap\n",
    "    #plt.tight_layout()\n",
    "\n",
    "    # Display the plot\n",
    "    #plt.show()\n",
    "\n",
    "    weights = cost_function.linear.weight.data.numpy().reshape((-1, 1)) #Return weights as numpy array\n",
    "\n",
    "    #return weights and bias and loss metrics\n",
    "    if uses_bias:\n",
    "        return weights, cost_function.linear.bias.item(), loss_values, gap_to_optimality, nmse_values, corr_values, R2_values,sub_problem_gradient,loss_values\n",
    "    else:\n",
    "        return weights, 0, loss_values, gap_to_optimality, nmse_values, corr_values, R2_values,sub_problem_gradient,loss_values\n",
    "    \n",
    "\n",
    "#Optimize a Cost Function via Gradient Descent with Exact Line Search\n",
    "#X: Shape n x d where n is the number of samples and d is the number of features\n",
    "#Y: Shape n x 1 where n is the number of samples\n",
    "#cost_function_code: 0 for Normal Least Squares, 1 for Ridge Regression\n",
    "#hypers: hyperparameters\n",
    "#p_star: estimated optimal value\n",
    "#W_true: true weights\n",
    "def GD2(X: np.ndarray, Y: np.ndarray, cost_function_code = 1, hypers = {}, p_star = 0, W_true = None):\n",
    "    hypers = defaultdict(int, hypers) #Convert hypers to defaultdict\n",
    "    \n",
    "    #Get necessary hyperparameters\n",
    "    uses_bias = hypers['bias'] #determine whether the bias term is needed\n",
    "    lmbd = hypers['lambda'] #Lambda for ridge regression\n",
    "    epochs = hypers['epochs'] #number of epochs\n",
    "    \n",
    "    #Initialize Cost Function\n",
    "    if cost_function_code == 0:\n",
    "        cost_function = LeastSquares(X.shape[1], uses_bias)\n",
    "    elif cost_function_code == 1:\n",
    "        cost_function = RidgeRegression(X.shape[1], lmbd, uses_bias)\n",
    "    \n",
    "    #Convert X and Y to pytorch tensors\n",
    "    X_tensor = torch.tensor(X, dtype = torch.float32)\n",
    "    Y_tensor = torch.tensor(Y, dtype = torch.float32)\n",
    "    \n",
    "    #If W_true is None, set it to a zero vector\n",
    "    #if not isinstance(W_true, np.ndarray):\n",
    "    #    W_true = np.zeros(shape = (X.shape[1], 1))\n",
    "        \n",
    "    #Store batch loss values\n",
    "    loss_values = []\n",
    "    \n",
    "    #Store gap to optimality\n",
    "    gap_to_optimality = []\n",
    "    \n",
    "    #Store Metric Values \n",
    "    #nee_values = []\n",
    "    nmse_values = []\n",
    "    corr_values = []\n",
    "    R2_values = []\n",
    "\n",
    "    #Training Loop\n",
    "    for epoch in range(epochs):\n",
    "        # Zero the gradients\n",
    "        for param in cost_function.parameters():\n",
    "            if param.grad is not None:\n",
    "                param.grad.zero_()\n",
    "            \n",
    "        # Compute loss\n",
    "        loss = cost_function.evaluate(X_tensor, Y_tensor, 'sum')\n",
    "\n",
    "        # Backward pass to compute gradient\n",
    "        loss.backward()\n",
    "\n",
    "        # Update parameters\n",
    "        t = exact_line_search_RR(X, Y, lmbd, cost_function, uses_bias)\n",
    "        print(f\"Value of t is: {t}\")\n",
    "        \n",
    "        # Manually update the weights and biases\n",
    "        with torch.no_grad():\n",
    "            for param in cost_function.parameters():\n",
    "                param -= t * param.grad\n",
    "        \n",
    "        #Print and Store loss values\n",
    "        loss_value = cost_function.evaluate(X_tensor, Y_tensor, 'sum').item()\n",
    "        loss_values.append(loss_value)\n",
    "        gap_to_optimality.append(loss_value - p_star)\n",
    "        \n",
    "        #Calculate Metrics\n",
    "        weights = cost_function.linear.weight.data.numpy().reshape((-1, 1))\n",
    "        bias = cost_function.linear.bias.item() if uses_bias else 0\n",
    "        X_numpy = X_tensor.numpy()\n",
    "        Y_predicted = X_numpy @ weights + bias\n",
    "        Y_numpy = Y_tensor.numpy()\n",
    "        \n",
    "        #nee = ((np.linalg.norm(weights - W_true)) ** 2) /  ((np.linalg.norm(W_true)) ** 2)\n",
    "        nmse = np.sum(np.square((Y_predicted - Y_numpy))) / np.sum(np.square(Y_numpy))\n",
    "        correlation = np.corrcoef(Y_predicted.flatten(), Y_numpy.flatten())[0, 1]\n",
    "        R2_score = r2_score(Y_numpy, Y_predicted)\n",
    "        \n",
    "        #nee_values.append(nee)\n",
    "        nmse_values.append(nmse)\n",
    "        corr_values.append(correlation)\n",
    "        R2_values.append(R2_score)\n",
    "                \n",
    "        #print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss_value:.4f}, Gap to Optimality: {gap_to_optimality[-1]:.4f}, NMSE: {nmse}, Correlation: {correlation}, R2: {R2_score}')\n",
    "        \n",
    "        # Stopping Criteria\n",
    "        criteria_satisfied = True\n",
    "        for name, param in cost_function.named_parameters():\n",
    "            if param.grad is not None:\n",
    "                print(f\"Gradient Norm for {name}: {torch.norm(param.grad)}\")\n",
    "                criteria_satisfied = criteria_satisfied and (torch.norm(param.grad) <= 0.001)\n",
    "            else:\n",
    "                print(f\"No gradient Norm for {name}\")\n",
    "        \n",
    "        if criteria_satisfied:\n",
    "            break\n",
    "\n",
    "    weights = cost_function.linear.weight.data.numpy().reshape((-1, 1)) #Return weights as numpy array\n",
    "\n",
    "    #return weights and bias and loss metrics\n",
    "    if uses_bias:\n",
    "        return weights, cost_function.linear.bias.item(), loss_values, gap_to_optimality, nmse_values, corr_values, R2_values\n",
    "    else:\n",
    "        return weights, 0, loss_values, gap_to_optimality, nmse_values, corr_values, R2_values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_sgd(X_train: np.ndarray, Y_train: np.ndarray, X_test: np.ndarray, Y_test: np.ndarray, lambda1, hypers,Y_train_mean,lsr_tensor_SGD,B_tensored = None,intercept = False):\n",
    "  hypers['weight_decay'] = lambda1\n",
    "\n",
    "  \n",
    "  #Define LSR Tensor Hyperparameters\n",
    "  ranks = hypers['ranks']\n",
    "  separation_rank = hypers['separation_rank']\n",
    "  LSR_tensor_dot_shape = tuple(X_train.shape)[1:]\n",
    "  need_intercept = intercept\n",
    "\n",
    "  #Construct LSR Tensor\n",
    "  lsr_tensor = lsr_tensor_SGD\n",
    "  lsr_tensor, objective_function_values, gradient_values,iterate_differences,epoch_level_gradients,epoch_level_function,tensor_iteration,factor_core_iterate = lsr_bcd_regression(lsr_tensor, X_train, Y_train, hypers,intercept = need_intercept)\n",
    "  expanded_lsr = lsr_tensor.expand_to_tensor()\n",
    "  expanded_lsr = np.reshape(expanded_lsr, X_train[0].shape, order = 'F')\n",
    "  print(expanded_lsr.shape)\n",
    "  Y_test_predicted = inner_product(np.transpose(X_test, (0, 2, 1)), expanded_lsr.flatten(order ='F')) + lsr_tensor.b + Y_train_mean\n",
    "\n",
    "  print('---------------------------Testing with Best Lambda------------------------------')\n",
    "  #print(f\"Y_test_predicted: {Y_test_predicted.flatten()}, Y_test: {Y_test.flatten()}\")\n",
    "  test_nmse_loss = np.sum(np.square((Y_test_predicted.flatten() - Y_test.flatten()))) / np.sum(np.square(Y_test.flatten()))\n",
    "  if B_tensored is not None:\n",
    "    normalized_estimation_error = ((np.linalg.norm(expanded_lsr - B_tensored)) ** 2) /  ((np.linalg.norm(B_tensored)) ** 2)\n",
    "  test_R2_loss = R2(Y_test.flatten(), Y_test_predicted.flatten())\n",
    "  test_correlation = np.corrcoef(Y_test_predicted.flatten(), Y_test.flatten())[0, 1]\n",
    "\n",
    "  print(\"Y Test Predicted: \", Y_test_predicted.flatten())\n",
    "  print(\"Y Test Actual: \", Y_test.flatten())\n",
    "\n",
    "  if B_tensored is not None:\n",
    "    return normalized_estimation_error, test_nmse_loss, test_R2_loss, test_correlation, objective_function_values,gradient_values,iterate_differences,epoch_level_gradients,epoch_level_function,tensor_iteration,factor_core_iterate\n",
    "  else:\n",
    "    normalized_estimation_error = np.inf\n",
    "    return normalized_estimation_error, test_nmse_loss, test_R2_loss, test_correlation, objective_function_values,gradient_values,iterate_differences,epoch_level_gradients,epoch_level_function,tensor_iteration,factor_core_iterate\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting the regression implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Setting up global parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_dimensions = np.array([32, 32])\n",
    "tensor_mode_ranks = np.array([4, 4])\n",
    "separation_rank = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample mean for each feature (across samples): [4.99871322 4.98420917 5.00873521 ... 5.01840176 4.98600789 5.01023623]\n",
      "Sample variance for each feature (across samples): None\n",
      "Response Average: 83.56008733093063\n"
     ]
    }
   ],
   "source": [
    "n_train = 650\n",
    "n_test = 100\n",
    "\n",
    "#Subset X_train and Y_train\n",
    "X_train = X_train_Full[0:(n_train),:,:]\n",
    "Y_train = Y_train_Full[0:(n_train)]\n",
    "\n",
    "#Subset X_test and Y_test\n",
    "X_test = X_test_Full[0:(n_test),:,:]\n",
    "Y_test = Y_test_Full[0:(n_test)]\n",
    "\n",
    "\n",
    "#Preprocessing\n",
    "\n",
    "# Reshape the 3D array to a 2D array where each row represents a sample\n",
    "# The shape of the original 3D array is (n_samples, n_features_per_sample, n_dimensions)\n",
    "# We reshape it to (n_samples, n_features_per_sample * n_dimensions)\n",
    "\n",
    "\n",
    "X_train_2D = X_train.reshape(n_train, -1)\n",
    "X_test_2D = X_test.reshape(n_test,-1)\n",
    "\n",
    "\n",
    "# Initialize StandardScaler\n",
    "scaler = StandardScaler(with_std = False) #standard scalar only\n",
    "\n",
    "# Fit scaler on train data and transform train data\n",
    "X_train_scaled = scaler.fit_transform(X_train_2D)\n",
    "# Transform test data using the scaler fitted on train data\n",
    "X_test_scaled = scaler.transform(X_test_2D)\n",
    "\n",
    "# Reshape the scaled data back to 3D\n",
    "X_train = X_train_scaled.reshape(n_train, tensor_dimensions[0],tensor_dimensions[1])\n",
    "X_test  = X_test_scaled.reshape(n_test, tensor_dimensions[0],tensor_dimensions[1])\n",
    "\n",
    "#average response value\n",
    "Y_train_mean = np.mean(Y_train)\n",
    "#Mean centering y_train and y_test\n",
    "Y_train = Y_train - Y_train_mean\n",
    "\n",
    "\n",
    "print(\"Sample mean for each feature (across samples):\",scaler.mean_)\n",
    "print(\"Sample variance for each feature (across samples):\",scaler.var_)\n",
    "print('Response Average:',Y_train_mean)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Intializing the tensor object "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intializing the tensor object \n",
    "\n",
    "hypers = {'max_iter': 50, 'threshold': 1e-8, 'ranks': tuple(tensor_mode_ranks), 'separation_rank': separation_rank}\n",
    "\n",
    "ranks = hypers['ranks']\n",
    "separation_rank = hypers['separation_rank']\n",
    "LSR_tensor_dot_shape = tuple(X_train.shape)[1:]\n",
    "need_intercept = False\n",
    "\n",
    "#initializing the tensor object\n",
    "#lsr_tensor = LSR_tensor_dot(shape = LSR_tensor_dot_shape, ranks = ranks, separation_rank = separation_rank, intercept = need_intercept)\n",
    "\n",
    "#regularization parameter\n",
    "lambda1 = 50\n",
    "\n",
    "\n",
    "#saving the initializer\n",
    "#formatted_time = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "#pkl_file = f\"/Users/lakrama/Neuro Project Codes/LSR-Tensor-Ridge-Regression_All_Data/Platforms for Experments/Data/Experiments_Isolating_Sub_Problems/Initializers_ExecutionTime_intercept_5_{formatted_time}, tensor_dimensions:{tensor_dimensions}, tensor_mode_= ranks:{tensor_mode_ranks}, separation_rank:{separation_rank}.pkl\"\n",
    "\n",
    "#with open(pkl_file, \"wb\") as file:\n",
    "#      dill.dump((lsr_tensor), file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the initializer\n",
    "\n",
    "import pickle\n",
    "pkl_file = \"/Users/lakrama/Neuro Project Codes/LSR-Tensor-Ridge-Regression_All_Data/Platforms for Experments/Data/Experiments_Isolating_Sub_Problems/Initializers_ExecutionTime_intercept_5_2024-06-23 21:06:37, tensor_dimensions:[32 32], tensor_mode_= ranks:[4 4], separation_rank:2.pkl\"\n",
    "file= open(pkl_file, 'rb')\n",
    "lsr_tensor_initializer = pickle.load(file)\n",
    "file.close()\n",
    "\n",
    "lsr_tensor = copy.deepcopy(lsr_tensor_initializer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Trainning and Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------------------Learning Rate 1e-06 -----------------------------------------\n",
      "Objective Function Value: 4831437.893446013\n",
      "--------------------------------------------------------------BCD iteration 0 --------------------------------------------------------------\n",
      "---------------------------------------------Sep 0 Factor 0 -------------------------------------------------\n",
      "Iteration: 0, Separation Rank: 0, Factor Matrix: 0, Objective Function Value: 4233888.149107747\n",
      "---------------------------------------------Sep 0 Factor 1 -------------------------------------------------\n",
      "Iteration: 0, Separation Rank: 0, Factor Matrix: 1, Objective Function Value: 3210320.131039708\n",
      "---------------------------------------------Sep 1 Factor 0 -------------------------------------------------\n",
      "Iteration: 0, Separation Rank: 1, Factor Matrix: 0, Objective Function Value: 3743681.83633678\n",
      "---------------------------------------------Sep 1 Factor 1 -------------------------------------------------\n",
      "Iteration: 0, Separation Rank: 1, Factor Matrix: 1, Objective Function Value: 3414681.423897649\n",
      "---------------------------------------------Core-------------------------------------------------\n",
      "Epoch [1/100], Loss: 97793.3828, Gap to Optimality: 97793.3828, NMSE: 0.28343871235847473, Correlation: 0.879600616125385, R2: 0.7165612726214614\n",
      "Epoch [2/100], Loss: 31231.0938, Gap to Optimality: 31231.0938, NMSE: 0.09044444561004639, Correlation: 0.9651297203203273, R2: 0.9095555538239244\n",
      "Epoch [3/100], Loss: 11786.4502, Gap to Optimality: 11786.4502, NMSE: 0.034054599702358246, Correlation: 0.986245057297772, R2: 0.9659453994404257\n",
      "Epoch [4/100], Loss: 4967.7500, Gap to Optimality: 4967.7500, NMSE: 0.014275485649704933, Correlation: 0.9939294680410682, R2: 0.9857245152292925\n",
      "Epoch [5/100], Loss: 2260.6821, Gap to Optimality: 2260.6821, NMSE: 0.006420864723622799, Correlation: 0.9971721176079705, R2: 0.9935791349992827\n",
      "Epoch [6/100], Loss: 1089.7490, Gap to Optimality: 1089.7490, NMSE: 0.0030221326742321253, Correlation: 0.998634013509892, R2: 0.9969778675997738\n",
      "Epoch [7/100], Loss: 558.4675, Gap to Optimality: 558.4675, NMSE: 0.0014794198796153069, Correlation: 0.9993233328092651, R2: 0.9985205801403018\n",
      "Epoch [8/100], Loss: 306.8796, Gap to Optimality: 306.8796, NMSE: 0.0007484709494747221, Correlation: 0.9996560767529089, R2: 0.9992515291212286\n",
      "Epoch [9/100], Loss: 183.5273, Gap to Optimality: 183.5273, NMSE: 0.00038984723505564034, Correlation: 0.9998215306030948, R2: 0.9996101527610256\n",
      "Epoch [10/100], Loss: 121.4448, Gap to Optimality: 121.4448, NMSE: 0.00020918698282912374, Correlation: 0.9999051678136863, R2: 0.9997908130175166\n",
      "Epoch [11/100], Loss: 89.2956, Gap to Optimality: 89.2956, NMSE: 0.00011552226351341233, Correlation: 0.9999485295588021, R2: 0.9998844777330822\n",
      "Epoch [12/100], Loss: 72.2177, Gap to Optimality: 72.2177, NMSE: 6.56819247524254e-05, Correlation: 0.9999714101355579, R2: 0.9999343180755332\n",
      "Epoch [13/100], Loss: 62.9926, Gap to Optimality: 62.9926, NMSE: 3.8715214031981304e-05, Correlation: 0.9999837685933803, R2: 0.9999612847892622\n",
      "Epoch [14/100], Loss: 57.8812, Gap to Optimality: 57.8812, NMSE: 2.3742139092064463e-05, Correlation: 0.9999906028660321, R2: 0.999976257863768\n",
      "Epoch [15/100], Loss: 54.9785, Gap to Optimality: 54.9785, NMSE: 1.5213626284094062e-05, Correlation: 0.9999944329490087, R2: 0.999984786373541\n",
      "Epoch [16/100], Loss: 53.3347, Gap to Optimality: 53.3347, NMSE: 1.0374325029260945e-05, Correlation: 0.9999966194011827, R2: 0.9999896256751759\n",
      "Epoch [17/100], Loss: 52.3893, Gap to Optimality: 52.3893, NMSE: 7.5849093263968825e-06, Correlation: 0.9999978755326415, R2: 0.9999924150909845\n",
      "Epoch [18/100], Loss: 51.8444, Gap to Optimality: 51.8444, NMSE: 5.975097337795887e-06, Correlation: 0.9999986235896202, R2: 0.999994024902881\n",
      "Epoch [19/100], Loss: 51.4986, Gap to Optimality: 51.4986, NMSE: 4.945613909512758e-06, Correlation: 0.9999990568316423, R2: 0.9999950543857751\n",
      "Epoch [20/100], Loss: 51.2681, Gap to Optimality: 51.2681, NMSE: 4.253584393154597e-06, Correlation: 0.9999993224520051, R2: 0.9999957464158037\n",
      "Epoch [21/100], Loss: 51.1454, Gap to Optimality: 51.1454, NMSE: 3.886656941176625e-06, Correlation: 0.9999994881984157, R2: 0.9999961133432288\n",
      "Epoch [22/100], Loss: 51.0560, Gap to Optimality: 51.0560, NMSE: 3.6160611216473626e-06, Correlation: 0.9999995901637505, R2: 0.9999963839387419\n",
      "Epoch [23/100], Loss: 51.0141, Gap to Optimality: 51.0141, NMSE: 3.4907545796158956e-06, Correlation: 0.9999996555660473, R2: 0.999996509245194\n",
      "Epoch [24/100], Loss: 50.9760, Gap to Optimality: 50.9760, NMSE: 3.3743544918252155e-06, Correlation: 0.9999996942031092, R2: 0.9999966256456347\n",
      "Epoch [25/100], Loss: 50.9372, Gap to Optimality: 50.9372, NMSE: 3.253512204537401e-06, Correlation: 0.9999997172518422, R2: 0.9999967464878079\n",
      "Epoch [26/100], Loss: 50.9264, Gap to Optimality: 50.9264, NMSE: 3.220686949134688e-06, Correlation: 0.9999997372496646, R2: 0.9999967793132409\n",
      "Epoch [27/100], Loss: 50.9074, Gap to Optimality: 50.9074, NMSE: 3.1612623843102483e-06, Correlation: 0.9999997472252898, R2: 0.9999968387376587\n",
      "Epoch [28/100], Loss: 50.8972, Gap to Optimality: 50.8972, NMSE: 3.129299784632167e-06, Correlation: 0.999999751834868, R2: 0.9999968707005479\n",
      "Epoch [29/100], Loss: 50.8916, Gap to Optimality: 50.8916, NMSE: 3.1118825063458644e-06, Correlation: 0.9999997550968239, R2: 0.9999968881175204\n",
      "Epoch [30/100], Loss: 50.8879, Gap to Optimality: 50.8879, NMSE: 3.100488811469404e-06, Correlation: 0.9999997610681866, R2: 0.9999968995110639\n",
      "Epoch [31/100], Loss: 50.8787, Gap to Optimality: 50.8787, NMSE: 3.0712251373188337e-06, Correlation: 0.9999997586673082, R2: 0.9999969287751385\n",
      "Epoch [32/100], Loss: 50.8950, Gap to Optimality: 50.8950, NMSE: 3.1232914352585794e-06, Correlation: 0.9999997634379723, R2: 0.9999968767086567\n",
      "Epoch [33/100], Loss: 50.8852, Gap to Optimality: 50.8852, NMSE: 3.0920755307306536e-06, Correlation: 0.9999997638133397, R2: 0.9999969079244287\n",
      "Epoch [34/100], Loss: 50.9024, Gap to Optimality: 50.9024, NMSE: 3.1467125154449604e-06, Correlation: 0.9999997683554175, R2: 0.9999968532874501\n",
      "Epoch [35/100], Loss: 50.8866, Gap to Optimality: 50.8866, NMSE: 3.096443379035918e-06, Correlation: 0.9999997691068317, R2: 0.9999969035563541\n",
      "Epoch [36/100], Loss: 50.8869, Gap to Optimality: 50.8869, NMSE: 3.097382659689174e-06, Correlation: 0.9999997731791467, R2: 0.9999969026171888\n",
      "Epoch [37/100], Loss: 50.8781, Gap to Optimality: 50.8781, NMSE: 3.0694156976096565e-06, Correlation: 0.9999997718376822, R2: 0.9999969305841707\n",
      "Epoch [38/100], Loss: 50.8847, Gap to Optimality: 50.8847, NMSE: 3.090305426667328e-06, Correlation: 0.9999997719639345, R2: 0.999996909694577\n",
      "Epoch [39/100], Loss: 50.8789, Gap to Optimality: 50.8789, NMSE: 3.072069148402079e-06, Correlation: 0.9999997717647761, R2: 0.9999969279308497\n",
      "Epoch [40/100], Loss: 50.8811, Gap to Optimality: 50.8811, NMSE: 3.0791861718171276e-06, Correlation: 0.9999997693404447, R2: 0.9999969208137406\n",
      "Epoch [41/100], Loss: 50.8913, Gap to Optimality: 50.8913, NMSE: 3.111389787591179e-06, Correlation: 0.9999997693362619, R2: 0.9999968886104024\n",
      "Epoch [42/100], Loss: 50.8942, Gap to Optimality: 50.8942, NMSE: 3.1205813684209716e-06, Correlation: 0.999999776360077, R2: 0.9999968794185862\n",
      "Epoch [43/100], Loss: 50.8931, Gap to Optimality: 50.8931, NMSE: 3.1172387480182806e-06, Correlation: 0.9999997719437737, R2: 0.9999968827612814\n",
      "Epoch [44/100], Loss: 50.8888, Gap to Optimality: 50.8888, NMSE: 3.1035131087264745e-06, Correlation: 0.9999997736214129, R2: 0.9999968964868461\n",
      "Epoch [45/100], Loss: 50.8943, Gap to Optimality: 50.8943, NMSE: 3.1208871860144427e-06, Correlation: 0.9999997738615188, R2: 0.9999968791130688\n",
      "Epoch [46/100], Loss: 50.8970, Gap to Optimality: 50.8970, NMSE: 3.1295576263801195e-06, Correlation: 0.9999997743655517, R2: 0.9999968704427525\n",
      "Epoch [47/100], Loss: 50.8894, Gap to Optimality: 50.8894, NMSE: 3.1053712064021965e-06, Correlation: 0.9999997756799915, R2: 0.999996894628666\n",
      "Epoch [48/100], Loss: 50.8905, Gap to Optimality: 50.8905, NMSE: 3.1088693503988907e-06, Correlation: 0.99999977465948, R2: 0.9999968911307928\n",
      "Epoch [49/100], Loss: 50.8923, Gap to Optimality: 50.8923, NMSE: 3.1145368666329887e-06, Correlation: 0.9999997713978631, R2: 0.9999968854632522\n",
      "Epoch [50/100], Loss: 50.8871, Gap to Optimality: 50.8871, NMSE: 3.0979488201410277e-06, Correlation: 0.9999997703204172, R2: 0.9999969020513536\n",
      "Epoch [51/100], Loss: 50.9088, Gap to Optimality: 50.9088, NMSE: 3.1671168017055606e-06, Correlation: 0.9999997753577256, R2: 0.9999968328830773\n",
      "Epoch [52/100], Loss: 50.8869, Gap to Optimality: 50.8869, NMSE: 3.0974249511928065e-06, Correlation: 0.999999775501707, R2: 0.9999969025748685\n",
      "Epoch [53/100], Loss: 50.8810, Gap to Optimality: 50.8810, NMSE: 3.0785520266363164e-06, Correlation: 0.999999771206856, R2: 0.9999969214479708\n",
      "Epoch [54/100], Loss: 50.8859, Gap to Optimality: 50.8859, NMSE: 3.0943788260628935e-06, Correlation: 0.999999771154055, R2: 0.9999969056210997\n",
      "Epoch [55/100], Loss: 50.8895, Gap to Optimality: 50.8895, NMSE: 3.105584937657113e-06, Correlation: 0.999999772711263, R2: 0.9999968944154398\n",
      "Epoch [56/100], Loss: 50.9009, Gap to Optimality: 50.9009, NMSE: 3.1420556751982076e-06, Correlation: 0.9999997759254129, R2: 0.999996857944276\n",
      "Epoch [57/100], Loss: 50.8856, Gap to Optimality: 50.8856, NMSE: 3.093265377174248e-06, Correlation: 0.9999997730020311, R2: 0.9999969067346897\n",
      "Epoch [58/100], Loss: 50.8912, Gap to Optimality: 50.8912, NMSE: 3.1111233056435594e-06, Correlation: 0.9999997695673006, R2: 0.9999968888768573\n",
      "Epoch [59/100], Loss: 50.8859, Gap to Optimality: 50.8859, NMSE: 3.0942978810344357e-06, Correlation: 0.9999997689352306, R2: 0.9999969057022174\n",
      "Epoch [60/100], Loss: 50.8804, Gap to Optimality: 50.8804, NMSE: 3.0768633223487996e-06, Correlation: 0.9999997760820026, R2: 0.9999969231365743\n",
      "Epoch [61/100], Loss: 50.8726, Gap to Optimality: 50.8726, NMSE: 3.0519724987243535e-06, Correlation: 0.9999997745579595, R2: 0.9999969480273428\n",
      "Epoch [62/100], Loss: 50.8769, Gap to Optimality: 50.8769, NMSE: 3.0655417049274547e-06, Correlation: 0.9999997708746057, R2: 0.9999969344581318\n",
      "Epoch [63/100], Loss: 50.8925, Gap to Optimality: 50.8925, NMSE: 3.1150595987128327e-06, Correlation: 0.9999997767894099, R2: 0.9999968849405921\n",
      "Epoch [64/100], Loss: 50.9076, Gap to Optimality: 50.9076, NMSE: 3.1632073387299897e-06, Correlation: 0.9999997765597148, R2: 0.9999968367927861\n",
      "Epoch [65/100], Loss: 50.8948, Gap to Optimality: 50.8948, NMSE: 3.1224938084051246e-06, Correlation: 0.9999997727409905, R2: 0.9999968775064761\n",
      "Epoch [66/100], Loss: 50.8922, Gap to Optimality: 50.8922, NMSE: 3.1142515126703074e-06, Correlation: 0.9999997724324086, R2: 0.9999968857486216\n",
      "Epoch [67/100], Loss: 50.8729, Gap to Optimality: 50.8729, NMSE: 3.0528185561706778e-06, Correlation: 0.9999997694644038, R2: 0.999996947181456\n",
      "Epoch [68/100], Loss: 50.8844, Gap to Optimality: 50.8844, NMSE: 3.089454821747495e-06, Correlation: 0.9999997709778741, R2: 0.9999969105449679\n",
      "Epoch [69/100], Loss: 50.8780, Gap to Optimality: 50.8780, NMSE: 3.069059403060237e-06, Correlation: 0.9999997689603551, R2: 0.9999969309404654\n",
      "Epoch [70/100], Loss: 50.8813, Gap to Optimality: 50.8813, NMSE: 3.0796322789683472e-06, Correlation: 0.9999997704759395, R2: 0.999996920367646\n",
      "Epoch [71/100], Loss: 50.8800, Gap to Optimality: 50.8800, NMSE: 3.075553422604571e-06, Correlation: 0.9999997755197303, R2: 0.9999969244465671\n",
      "Epoch [72/100], Loss: 50.8879, Gap to Optimality: 50.8879, NMSE: 3.100646381426486e-06, Correlation: 0.9999997743377671, R2: 0.9999968993535653\n",
      "Epoch [73/100], Loss: 50.8900, Gap to Optimality: 50.8900, NMSE: 3.1074616799742216e-06, Correlation: 0.9999997708956626, R2: 0.9999968925382783\n",
      "Epoch [74/100], Loss: 50.8867, Gap to Optimality: 50.8867, NMSE: 3.0967087241151603e-06, Correlation: 0.9999997744017511, R2: 0.9999969032913357\n",
      "Epoch [75/100], Loss: 50.8680, Gap to Optimality: 50.8680, NMSE: 3.0372691526281415e-06, Correlation: 0.999999772231734, R2: 0.9999969627311044\n",
      "Epoch [76/100], Loss: 50.8731, Gap to Optimality: 50.8731, NMSE: 3.0536848498741165e-06, Correlation: 0.9999997730115329, R2: 0.9999969463151067\n",
      "Epoch [77/100], Loss: 50.8762, Gap to Optimality: 50.8762, NMSE: 3.0632731977675576e-06, Correlation: 0.9999997731667449, R2: 0.9999969367267005\n",
      "Epoch [78/100], Loss: 50.8741, Gap to Optimality: 50.8741, NMSE: 3.056743025808828e-06, Correlation: 0.999999774431548, R2: 0.9999969432569084\n",
      "Epoch [79/100], Loss: 50.8672, Gap to Optimality: 50.8672, NMSE: 3.0347134725161595e-06, Correlation: 0.9999997723563168, R2: 0.9999969652864742\n",
      "Epoch [80/100], Loss: 50.8704, Gap to Optimality: 50.8704, NMSE: 3.0449639325524913e-06, Correlation: 0.9999997751398768, R2: 0.9999969550360929\n",
      "Epoch [81/100], Loss: 50.8756, Gap to Optimality: 50.8756, NMSE: 3.061556981265312e-06, Correlation: 0.999999774776416, R2: 0.9999969384429537\n",
      "Epoch [82/100], Loss: 50.8774, Gap to Optimality: 50.8774, NMSE: 3.06709375763603e-06, Correlation: 0.9999997749540008, R2: 0.9999969329063406\n",
      "Epoch [83/100], Loss: 50.8884, Gap to Optimality: 50.8884, NMSE: 3.102226401097141e-06, Correlation: 0.9999997760752786, R2: 0.9999968977735538\n",
      "Epoch [84/100], Loss: 50.8863, Gap to Optimality: 50.8863, NMSE: 3.0953920031606685e-06, Correlation: 0.9999997758658885, R2: 0.9999969046080891\n",
      "Epoch [85/100], Loss: 50.8910, Gap to Optimality: 50.8910, NMSE: 3.11063877234119e-06, Correlation: 0.9999997725127926, R2: 0.9999968893611993\n",
      "Epoch [86/100], Loss: 50.8873, Gap to Optimality: 50.8873, NMSE: 3.0986202546046115e-06, Correlation: 0.999999772833284, R2: 0.9999969013797098\n",
      "Epoch [87/100], Loss: 50.8737, Gap to Optimality: 50.8737, NMSE: 3.055522711292724e-06, Correlation: 0.9999997711434786, R2: 0.999996944477267\n",
      "Epoch [88/100], Loss: 50.8597, Gap to Optimality: 50.8597, NMSE: 3.010740556419478e-06, Correlation: 0.9999997696457141, R2: 0.9999969892595901\n",
      "Epoch [89/100], Loss: 50.8632, Gap to Optimality: 50.8632, NMSE: 3.0219491691241274e-06, Correlation: 0.9999997722230343, R2: 0.9999969780510174\n",
      "Epoch [90/100], Loss: 50.8713, Gap to Optimality: 50.8713, NMSE: 3.0478465760097606e-06, Correlation: 0.9999997719370268, R2: 0.999996952153617\n",
      "Epoch [91/100], Loss: 50.8770, Gap to Optimality: 50.8770, NMSE: 3.065910277655348e-06, Correlation: 0.9999997690416841, R2: 0.9999969340895539\n",
      "Epoch [92/100], Loss: 50.8799, Gap to Optimality: 50.8799, NMSE: 3.0752546535950387e-06, Correlation: 0.9999997709444375, R2: 0.9999969247455607\n",
      "Epoch [93/100], Loss: 50.8747, Gap to Optimality: 50.8747, NMSE: 3.0587357287004124e-06, Correlation: 0.9999997746584411, R2: 0.9999969412646609\n",
      "Epoch [94/100], Loss: 50.8915, Gap to Optimality: 50.8915, NMSE: 3.1118831884668907e-06, Correlation: 0.9999997785196707, R2: 0.9999968881168004\n",
      "Epoch [95/100], Loss: 50.8761, Gap to Optimality: 50.8761, NMSE: 3.06318474940781e-06, Correlation: 0.9999997761496823, R2: 0.9999969368150178\n",
      "Epoch [96/100], Loss: 50.8753, Gap to Optimality: 50.8753, NMSE: 3.060307790292427e-06, Correlation: 0.9999997796667, R2: 0.9999969396923761\n",
      "Epoch [97/100], Loss: 50.8774, Gap to Optimality: 50.8774, NMSE: 3.067074658247293e-06, Correlation: 0.9999997766338231, R2: 0.9999969329256283\n",
      "Epoch [98/100], Loss: 50.8722, Gap to Optimality: 50.8722, NMSE: 3.0506751045322744e-06, Correlation: 0.9999997728395481, R2: 0.9999969493250092\n",
      "Epoch [99/100], Loss: 50.8815, Gap to Optimality: 50.8815, NMSE: 3.080371925534564e-06, Correlation: 0.9999997753346246, R2: 0.9999969196280394\n",
      "Epoch [100/100], Loss: 50.8857, Gap to Optimality: 50.8857, NMSE: 3.093755822192179e-06, Correlation: 0.9999997727003863, R2: 0.9999969062442592\n",
      "Final gradient of the subproblem Core : 54.46720886230469\n",
      "--------------------------------------------------------------BCD iteration 1 --------------------------------------------------------------\n",
      "---------------------------------------------Sep 0 Factor 0 -------------------------------------------------\n",
      "Iteration: 1, Separation Rank: 0, Factor Matrix: 0, Objective Function Value: 22200.022105352855\n",
      "---------------------------------------------Sep 0 Factor 1 -------------------------------------------------\n",
      "Iteration: 1, Separation Rank: 0, Factor Matrix: 1, Objective Function Value: 22200.022105352855\n",
      "---------------------------------------------Sep 1 Factor 0 -------------------------------------------------\n",
      "Iteration: 1, Separation Rank: 1, Factor Matrix: 0, Objective Function Value: 22200.022105352855\n",
      "---------------------------------------------Sep 1 Factor 1 -------------------------------------------------\n",
      "Iteration: 1, Separation Rank: 1, Factor Matrix: 1, Objective Function Value: 22200.022105352855\n",
      "---------------------------------------------Core-------------------------------------------------\n",
      "Epoch [1/100], Loss: 143037.1719, Gap to Optimality: 143037.1719, NMSE: 0.4146154820919037, Correlation: 0.7966939203142439, R2: 0.5853844732072031\n",
      "Epoch [2/100], Loss: 48013.2461, Gap to Optimality: 48013.2461, NMSE: 0.13911172747612, Correlation: 0.944639120883782, R2: 0.8608882677156185\n",
      "Epoch [3/100], Loss: 18788.7305, Gap to Optimality: 18788.7305, NMSE: 0.054366543889045715, Correlation: 0.978185501914045, R2: 0.9456334545944248\n",
      "Epoch [4/100], Loss: 8202.4492, Gap to Optimality: 8202.4492, NMSE: 0.023661937564611435, Correlation: 0.9901404389182548, R2: 0.9763380634054674\n",
      "Epoch [5/100], Loss: 3860.3833, Gap to Optimality: 3860.3833, NMSE: 0.011064771562814713, Correlation: 0.9952105951017324, R2: 0.9889352292621135\n",
      "Epoch [6/100], Loss: 1923.3533, Gap to Optimality: 1923.3533, NMSE: 0.005443471949547529, Correlation: 0.9975950863178223, R2: 0.9945565283256131\n",
      "Epoch [7/100], Loss: 1002.7626, Gap to Optimality: 1002.7626, NMSE: 0.0027707673143595457, Correlation: 0.9987530412346147, R2: 0.9972292324795526\n",
      "Epoch [8/100], Loss: 547.1769, Gap to Optimality: 547.1769, NMSE: 0.0014474594499915838, Correlation: 0.9993415978097088, R2: 0.9985525406356189\n",
      "Epoch [9/100], Loss: 315.1564, Gap to Optimality: 315.1564, NMSE: 0.0007731548976153135, Correlation: 0.9996479617104183, R2: 0.99922684513971\n",
      "Epoch [10/100], Loss: 194.1470, Gap to Optimality: 194.1470, NMSE: 0.00042119811405427754, Correlation: 0.9998089336438458, R2: 0.999578801888015\n",
      "Epoch [11/100], Loss: 129.8628, Gap to Optimality: 129.8628, NMSE: 0.00023404057719744742, Correlation: 0.9998948119150755, R2: 0.999765959417536\n",
      "Epoch [12/100], Loss: 95.0491, Gap to Optimality: 95.0491, NMSE: 0.00013254636724013835, Correlation: 0.9999413189927618, R2: 0.9998674536286647\n",
      "Epoch [13/100], Loss: 76.0529, Gap to Optimality: 76.0529, NMSE: 7.70955448388122e-05, Correlation: 0.9999669223556424, R2: 0.9999229044606037\n",
      "Epoch [14/100], Loss: 65.3802, Gap to Optimality: 65.3802, NMSE: 4.5852539187762886e-05, Correlation: 0.9999809377750737, R2: 0.9999541474602094\n",
      "Epoch [15/100], Loss: 59.3911, Gap to Optimality: 59.3911, NMSE: 2.8284110157983378e-05, Correlation: 0.9999888457785033, R2: 0.9999717158909177\n",
      "Epoch [16/100], Loss: 56.0247, Gap to Optimality: 56.0247, NMSE: 1.839121432567481e-05, Correlation: 0.9999933677529907, R2: 0.9999816087839174\n",
      "Epoch [17/100], Loss: 54.0158, Gap to Optimality: 54.0158, NMSE: 1.2456383046810515e-05, Correlation: 0.999995919888651, R2: 0.9999875436168265\n",
      "Epoch [18/100], Loss: 52.8366, Gap to Optimality: 52.8366, NMSE: 8.96196343092015e-06, Correlation: 0.9999974315610094, R2: 0.9999910380361824\n",
      "Epoch [19/100], Loss: 52.1206, Gap to Optimality: 52.1206, NMSE: 6.829349786130479e-06, Correlation: 0.9999983242258397, R2: 0.9999931706506839\n",
      "Epoch [20/100], Loss: 51.6821, Gap to Optimality: 51.6821, NMSE: 5.516305009223288e-06, Correlation: 0.9999988532488598, R2: 0.9999944836951301\n",
      "Epoch [21/100], Loss: 51.4157, Gap to Optimality: 51.4157, NMSE: 4.715235718322219e-06, Correlation: 0.9999991777884499, R2: 0.9999952847641219\n",
      "Epoch [22/100], Loss: 51.2334, Gap to Optimality: 51.2334, NMSE: 4.162644472671673e-06, Correlation: 0.9999993865404863, R2: 0.9999958373555642\n",
      "Epoch [23/100], Loss: 51.1231, Gap to Optimality: 51.1231, NMSE: 3.827160526270745e-06, Correlation: 0.9999995184191303, R2: 0.9999961728394705\n",
      "Epoch [24/100], Loss: 51.0554, Gap to Optimality: 51.0554, NMSE: 3.6208955407346366e-06, Correlation: 0.9999996040435255, R2: 0.9999963791043536\n",
      "Epoch [25/100], Loss: 50.9818, Gap to Optimality: 50.9818, NMSE: 3.392014377823216e-06, Correlation: 0.999999653782214, R2: 0.9999966079858362\n",
      "Epoch [26/100], Loss: 50.9556, Gap to Optimality: 50.9556, NMSE: 3.311804221084458e-06, Correlation: 0.9999996923732459, R2: 0.9999966881956209\n",
      "Epoch [27/100], Loss: 50.9278, Gap to Optimality: 50.9278, NMSE: 3.2252601158688776e-06, Correlation: 0.9999997186529344, R2: 0.999996774739743\n",
      "Epoch [28/100], Loss: 50.9164, Gap to Optimality: 50.9164, NMSE: 3.1899980967864394e-06, Correlation: 0.9999997337257509, R2: 0.9999968100020781\n",
      "Epoch [29/100], Loss: 50.9044, Gap to Optimality: 50.9044, NMSE: 3.1522447443421697e-06, Correlation: 0.99999974143471, R2: 0.9999968477553429\n",
      "Epoch [30/100], Loss: 50.8896, Gap to Optimality: 50.8896, NMSE: 3.1054382816364523e-06, Correlation: 0.9999997474910561, R2: 0.999996894561908\n",
      "Epoch [31/100], Loss: 50.9100, Gap to Optimality: 50.9100, NMSE: 3.170770924043609e-06, Correlation: 0.9999997571711914, R2: 0.9999968292288443\n",
      "Epoch [32/100], Loss: 50.9063, Gap to Optimality: 50.9063, NMSE: 3.1588986075803405e-06, Correlation: 0.9999997642673489, R2: 0.9999968411014618\n",
      "Epoch [33/100], Loss: 50.8967, Gap to Optimality: 50.8967, NMSE: 3.1285542263503885e-06, Correlation: 0.999999767431544, R2: 0.999996871445614\n",
      "Epoch [34/100], Loss: 50.8985, Gap to Optimality: 50.8985, NMSE: 3.1342776765086455e-06, Correlation: 0.999999768463866, R2: 0.9999968657224946\n",
      "Epoch [35/100], Loss: 50.8920, Gap to Optimality: 50.8920, NMSE: 3.113639195362339e-06, Correlation: 0.9999997716313271, R2: 0.999996886360584\n",
      "Epoch [36/100], Loss: 50.8859, Gap to Optimality: 50.8859, NMSE: 3.094144176429836e-06, Correlation: 0.9999997741198223, R2: 0.9999969058559645\n",
      "Epoch [37/100], Loss: 50.8790, Gap to Optimality: 50.8790, NMSE: 3.0723469990334706e-06, Correlation: 0.9999997763591615, R2: 0.9999969276529901\n",
      "Epoch [38/100], Loss: 50.8796, Gap to Optimality: 50.8796, NMSE: 3.0742389753868338e-06, Correlation: 0.9999997700004348, R2: 0.9999969257609813\n",
      "Epoch [39/100], Loss: 50.8839, Gap to Optimality: 50.8839, NMSE: 3.08815970129217e-06, Correlation: 0.9999997693607016, R2: 0.9999969118402099\n",
      "Epoch [40/100], Loss: 50.9022, Gap to Optimality: 50.9022, NMSE: 3.146100880258018e-06, Correlation: 0.9999997755986119, R2: 0.9999968538993532\n",
      "Epoch [41/100], Loss: 50.9012, Gap to Optimality: 50.9012, NMSE: 3.1428378406417323e-06, Correlation: 0.9999997766017212, R2: 0.9999968571620337\n",
      "Epoch [42/100], Loss: 50.9011, Gap to Optimality: 50.9011, NMSE: 3.1426070563611574e-06, Correlation: 0.9999997770911131, R2: 0.9999968573927236\n",
      "Epoch [43/100], Loss: 50.8879, Gap to Optimality: 50.8879, NMSE: 3.100578624071204e-06, Correlation: 0.9999997725364144, R2: 0.9999968994214771\n",
      "Epoch [44/100], Loss: 50.8888, Gap to Optimality: 50.8888, NMSE: 3.1033850973472e-06, Correlation: 0.9999997711090596, R2: 0.9999968966145774\n",
      "Epoch [45/100], Loss: 50.8812, Gap to Optimality: 50.8812, NMSE: 3.0793698897468857e-06, Correlation: 0.9999997696329866, R2: 0.9999969206300319\n",
      "Epoch [46/100], Loss: 50.8888, Gap to Optimality: 50.8888, NMSE: 3.103498102063895e-06, Correlation: 0.9999997704884798, R2: 0.9999968965018436\n",
      "Epoch [47/100], Loss: 50.8878, Gap to Optimality: 50.8878, NMSE: 3.1002427931525744e-06, Correlation: 0.9999997672044333, R2: 0.9999968997575241\n",
      "Epoch [48/100], Loss: 50.8933, Gap to Optimality: 50.8933, NMSE: 3.117847427347442e-06, Correlation: 0.9999997735761158, R2: 0.9999968821526523\n",
      "Epoch [49/100], Loss: 50.8957, Gap to Optimality: 50.8957, NMSE: 3.12558222503867e-06, Correlation: 0.9999997717640922, R2: 0.9999968744176916\n",
      "Epoch [50/100], Loss: 50.8882, Gap to Optimality: 50.8882, NMSE: 3.1017416404210962e-06, Correlation: 0.9999997739550242, R2: 0.9999968982585644\n",
      "Epoch [51/100], Loss: 50.9012, Gap to Optimality: 50.9012, NMSE: 3.1427646263182396e-06, Correlation: 0.9999997785215313, R2: 0.9999968572356462\n",
      "Epoch [52/100], Loss: 50.8767, Gap to Optimality: 50.8767, NMSE: 3.0648945994471433e-06, Correlation: 0.9999997712961964, R2: 0.9999969351051863\n",
      "Epoch [53/100], Loss: 50.8803, Gap to Optimality: 50.8803, NMSE: 3.0764324492338346e-06, Correlation: 0.9999997710227483, R2: 0.9999969235676115\n",
      "Epoch [54/100], Loss: 50.8789, Gap to Optimality: 50.8789, NMSE: 3.0721555503987474e-06, Correlation: 0.9999997737766055, R2: 0.9999969278442573\n",
      "Epoch [55/100], Loss: 50.8923, Gap to Optimality: 50.8923, NMSE: 3.1145752927841386e-06, Correlation: 0.9999997763063586, R2: 0.9999968854245566\n",
      "Epoch [56/100], Loss: 50.9064, Gap to Optimality: 50.9064, NMSE: 3.159384277751087e-06, Correlation: 0.9999997740645098, R2: 0.9999968406157678\n",
      "Epoch [57/100], Loss: 50.9023, Gap to Optimality: 50.9023, NMSE: 3.146293693134794e-06, Correlation: 0.9999997746283883, R2: 0.9999968537062273\n",
      "Epoch [58/100], Loss: 50.9022, Gap to Optimality: 50.9022, NMSE: 3.14606086249114e-06, Correlation: 0.9999997758101767, R2: 0.9999968539392509\n",
      "Epoch [59/100], Loss: 50.8929, Gap to Optimality: 50.8929, NMSE: 3.116389962087851e-06, Correlation: 0.9999997740675588, R2: 0.9999968836104105\n",
      "Epoch [60/100], Loss: 50.8841, Gap to Optimality: 50.8841, NMSE: 3.0886835702403914e-06, Correlation: 0.9999997740233274, R2: 0.9999969113163053\n",
      "Epoch [61/100], Loss: 50.8673, Gap to Optimality: 50.8673, NMSE: 3.035069767065579e-06, Correlation: 0.9999997738202975, R2: 0.9999969649300209\n",
      "Epoch [62/100], Loss: 50.8678, Gap to Optimality: 50.8678, NMSE: 3.036730731764692e-06, Correlation: 0.999999773051676, R2: 0.9999969632695301\n",
      "Epoch [63/100], Loss: 50.8573, Gap to Optimality: 50.8573, NMSE: 3.003173105753376e-06, Correlation: 0.9999997697575804, R2: 0.9999969968267989\n",
      "Epoch [64/100], Loss: 50.8623, Gap to Optimality: 50.8623, NMSE: 3.0191374662535964e-06, Correlation: 0.9999997714161151, R2: 0.9999969808626352\n",
      "Epoch [65/100], Loss: 50.8764, Gap to Optimality: 50.8764, NMSE: 3.064101974814548e-06, Correlation: 0.9999997728057661, R2: 0.9999969358981939\n",
      "Epoch [66/100], Loss: 50.8851, Gap to Optimality: 50.8851, NMSE: 3.091557573497994e-06, Correlation: 0.9999997753633573, R2: 0.9999969084426449\n",
      "Epoch [67/100], Loss: 50.8847, Gap to Optimality: 50.8847, NMSE: 3.090413201789488e-06, Correlation: 0.9999997727466021, R2: 0.9999969095868279\n",
      "Epoch [68/100], Loss: 50.8795, Gap to Optimality: 50.8795, NMSE: 3.0738647183170542e-06, Correlation: 0.9999997730276119, R2: 0.99999692613548\n",
      "Epoch [69/100], Loss: 50.8701, Gap to Optimality: 50.8701, NMSE: 3.043866399821127e-06, Correlation: 0.9999997700238733, R2: 0.999996956133811\n",
      "Epoch [70/100], Loss: 50.8851, Gap to Optimality: 50.8851, NMSE: 3.0917108233552426e-06, Correlation: 0.9999997674575687, R2: 0.9999969082892811\n",
      "Epoch [71/100], Loss: 50.8663, Gap to Optimality: 50.8663, NMSE: 3.031992719115806e-06, Correlation: 0.9999997660646787, R2: 0.9999969680075748\n",
      "Epoch [72/100], Loss: 50.8777, Gap to Optimality: 50.8777, NMSE: 3.068259729843703e-06, Correlation: 0.9999997662875333, R2: 0.9999969317401511\n",
      "Epoch [73/100], Loss: 50.8758, Gap to Optimality: 50.8758, NMSE: 3.062054020119831e-06, Correlation: 0.9999997696404207, R2: 0.9999969379461899\n",
      "Epoch [74/100], Loss: 50.8801, Gap to Optimality: 50.8801, NMSE: 3.0757157674088376e-06, Correlation: 0.9999997706030481, R2: 0.9999969242841769\n",
      "Epoch [75/100], Loss: 50.9021, Gap to Optimality: 50.9021, NMSE: 3.145648861391237e-06, Correlation: 0.9999997781155162, R2: 0.9999968543511187\n",
      "Epoch [76/100], Loss: 50.8812, Gap to Optimality: 50.8812, NMSE: 3.0792020879744086e-06, Correlation: 0.9999997709243768, R2: 0.9999969207979782\n",
      "Epoch [77/100], Loss: 50.8787, Gap to Optimality: 50.8787, NMSE: 3.071291530432063e-06, Correlation: 0.9999997718946755, R2: 0.9999969287085597\n",
      "Epoch [78/100], Loss: 50.8891, Gap to Optimality: 50.8891, NMSE: 3.1044492061482742e-06, Correlation: 0.9999997707494316, R2: 0.9999968955505275\n",
      "Epoch [79/100], Loss: 50.8890, Gap to Optimality: 50.8890, NMSE: 3.1042363843880594e-06, Correlation: 0.9999997718225124, R2: 0.9999968957635396\n",
      "Epoch [80/100], Loss: 50.8852, Gap to Optimality: 50.8852, NMSE: 3.091984353886801e-06, Correlation: 0.9999997714543628, R2: 0.999996908015737\n",
      "Epoch [81/100], Loss: 50.8894, Gap to Optimality: 50.8894, NMSE: 3.105496034550015e-06, Correlation: 0.999999771202165, R2: 0.9999968945039129\n",
      "Epoch [82/100], Loss: 50.8861, Gap to Optimality: 50.8861, NMSE: 3.0948626772442367e-06, Correlation: 0.9999997720095418, R2: 0.9999969051373984\n",
      "Epoch [83/100], Loss: 50.8788, Gap to Optimality: 50.8788, NMSE: 3.0717096706212033e-06, Correlation: 0.9999997723996875, R2: 0.9999969282903022\n",
      "Epoch [84/100], Loss: 50.8883, Gap to Optimality: 50.8883, NMSE: 3.1019383186503546e-06, Correlation: 0.9999997731860901, R2: 0.9999968980619784\n",
      "Epoch [85/100], Loss: 50.8830, Gap to Optimality: 50.8830, NMSE: 3.0852424970362335e-06, Correlation: 0.9999997688902293, R2: 0.9999969147572246\n",
      "Epoch [86/100], Loss: 50.8975, Gap to Optimality: 50.8975, NMSE: 3.131077392026782e-06, Correlation: 0.9999997735382097, R2: 0.9999968689227812\n",
      "Epoch [87/100], Loss: 50.8888, Gap to Optimality: 50.8888, NMSE: 3.103386234215577e-06, Correlation: 0.9999997702996547, R2: 0.9999968966134204\n",
      "Epoch [88/100], Loss: 50.8864, Gap to Optimality: 50.8864, NMSE: 3.0958988190832315e-06, Correlation: 0.9999997713294896, R2: 0.9999969041012932\n",
      "Epoch [89/100], Loss: 50.8986, Gap to Optimality: 50.8986, NMSE: 3.1347974527307088e-06, Correlation: 0.9999997784143159, R2: 0.999996865202747\n",
      "Epoch [90/100], Loss: 50.8713, Gap to Optimality: 50.8713, NMSE: 3.047696964131319e-06, Correlation: 0.999999768555517, R2: 0.9999969523030606\n",
      "Epoch [91/100], Loss: 50.8702, Gap to Optimality: 50.8702, NMSE: 3.0443966352322605e-06, Correlation: 0.9999997691658545, R2: 0.9999969556034763\n",
      "Epoch [92/100], Loss: 50.8957, Gap to Optimality: 50.8957, NMSE: 3.1255574413080467e-06, Correlation: 0.9999997703361623, R2: 0.9999968744427075\n",
      "Epoch [93/100], Loss: 50.8699, Gap to Optimality: 50.8699, NMSE: 3.0432900075538782e-06, Correlation: 0.9999997683385201, R2: 0.9999969567101757\n",
      "Epoch [94/100], Loss: 50.8819, Gap to Optimality: 50.8819, NMSE: 3.081528575421544e-06, Correlation: 0.999999772182004, R2: 0.9999969184716191\n",
      "Epoch [95/100], Loss: 50.8855, Gap to Optimality: 50.8855, NMSE: 3.0929797958378913e-06, Correlation: 0.9999997748918924, R2: 0.9999969070201714\n",
      "Epoch [96/100], Loss: 50.8853, Gap to Optimality: 50.8853, NMSE: 3.0925293685868382e-06, Correlation: 0.9999997717717385, R2: 0.9999969074707877\n",
      "Epoch [97/100], Loss: 50.8930, Gap to Optimality: 50.8930, NMSE: 3.1168144687399035e-06, Correlation: 0.999999775939765, R2: 0.9999968831859047\n",
      "Epoch [98/100], Loss: 50.9150, Gap to Optimality: 50.9150, NMSE: 3.186659796483582e-06, Correlation: 0.9999997786537359, R2: 0.9999968133401885\n",
      "Epoch [99/100], Loss: 50.9072, Gap to Optimality: 50.9072, NMSE: 3.162013172186562e-06, Correlation: 0.9999997777442663, R2: 0.9999968379872454\n",
      "Epoch [100/100], Loss: 50.8807, Gap to Optimality: 50.8807, NMSE: 3.077630935877096e-06, Correlation: 0.9999997759050537, R2: 0.9999969223689881\n",
      "Final gradient of the subproblem Core : 53.4962272644043\n",
      "The threshold activated2.331296491320245e-05\n",
      "(32, 32)\n",
      "---------------------------Testing with Best Lambda------------------------------\n",
      "Y Test Predicted:  [ 67.53889796  64.20723493 100.88365341 106.7620092   66.56522161\n",
      "  96.79184276 112.72214427  87.07489304  58.93417897  60.77985387\n",
      "  65.78999045  64.18776563  82.79147082  68.58358121 142.37517965\n",
      "  97.98634271  98.31419728  55.46899552  96.73259014  87.27101257\n",
      "  48.05614658  91.13593925  73.603821    77.47433363  47.03430409\n",
      "  79.38556699  84.71294294  45.53625254  87.564364    73.48932449\n",
      "  86.90931033  88.25604147 124.59273092  81.4381231   87.5440148\n",
      "  82.76663671  98.71568773  69.0812521   80.61212089  93.06440436\n",
      "  68.43814494  87.28189397  62.06419269  57.39379069  56.06265799\n",
      "  89.30549457 110.49283833 134.53183979  43.31888576  74.49618261\n",
      "  96.2097746   87.90059666  79.22738479  90.7184306  125.82215934\n",
      " 160.24479909  60.54980786  86.89392464  89.56372414  85.50660539\n",
      "  79.97227961  95.5329292   57.13773167  50.42440359  70.88638785\n",
      " 131.163236    94.48775385  83.17069335  62.95110607  84.07117477\n",
      "  76.26749881  96.45323548  77.09738134  65.79596583  71.80836856\n",
      "  65.18813333  61.0545976   68.75919454  66.4484935  104.92238478\n",
      "  79.76091039  91.81036034 110.40948834  62.26639411  57.64487953\n",
      "  98.11197171  67.86021907  72.13927573 127.79420378  63.34699786\n",
      " 103.66629245 149.95922307  67.44132552  92.38805481 123.61790531\n",
      "  46.97164191  64.68057638 108.1723323  101.10736496 113.34286045]\n",
      "Y Test Actual:  [ 67.54932471  64.19367989 100.89907103 106.80033802  66.5296898\n",
      "  96.80085769 112.75716496  87.08379176  58.86645104  60.75871896\n",
      "  65.76651181  64.14603363  82.80477245  68.55912818 142.45923938\n",
      "  98.00053671  98.36489423  55.40988723  96.73410604  87.25275894\n",
      "  47.97392502  91.14302569  73.57729565  77.46144622  46.97758628\n",
      "  79.38435102  84.7040345   45.4879342   87.55672081  73.49886432\n",
      "  86.90598491  88.25175908 124.63253198  81.43265772  87.55287141\n",
      "  82.8080788   98.72768177  69.0468243   80.62856336  93.08545506\n",
      "  68.40807412  87.26847367  62.04717769  57.33990441  56.01334774\n",
      "  89.31934273 110.55219975 134.61307846  43.26935217  74.47617341\n",
      "  96.21843559  87.89937403  79.21862917  90.71786386 125.8824493\n",
      " 160.38439066  60.50387741  86.87508783  89.5739495   85.50880057\n",
      "  79.97185552  95.54965657  57.09355445  50.35928763  70.86185745\n",
      " 131.24078613  94.50837386  83.15863626  62.93422433  84.09831139\n",
      "  76.26182003  96.48126457  77.07300387  65.77838739  71.76861932\n",
      "  65.14937109  61.0440168   68.74400314  66.41967439 104.95340089\n",
      "  79.75508697  91.80406117 110.44770514  62.24491669  57.60974417\n",
      "  98.13365195  67.82489044  72.14320497 127.85744229  63.30804812\n",
      " 103.70015697 150.07489827  67.42743244  92.3906977  123.66379363\n",
      "  46.92967844  64.66629945 108.22935093 101.14612067 113.38031778]\n",
      "Error Report on Testing _ With best Lambda\n",
      "SGD_Alpha chosen for model:  50\n",
      "SGD_Test Normalized Estimation Error:  2.9726482445033966e-06\n",
      "SGD_Test NMSE Loss:  1.95044466109482e-07\n",
      "SGD_Test R2 Loss:  0.9999972958280611\n",
      "SGD_Test Correlation:  0.9999998215538607\n",
      "Objective Function Values 22200.022105352855\n"
     ]
    }
   ],
   "source": [
    "learning_rates = [0.000001]\n",
    "\n",
    "for idx,lr in enumerate (learning_rates):   \n",
    "    \n",
    "    print('')\n",
    "    print('-------------------------------------Learning Rate',lr,'-----------------------------------------')\n",
    "    lsr_tensor_SGD = copy.deepcopy(lsr_tensor_initializer)\n",
    "    learning_rate = lr\n",
    "    epochs = 100\n",
    "    batch_size = 64\n",
    "\n",
    "    momentum = 0\n",
    "    nesterov = False\n",
    "    decay_factor = 0\n",
    "    hypers = {'max_iter': 50, 'threshold': 1e-4, 'ranks': tuple(tensor_mode_ranks), 'separation_rank': separation_rank,'learning_rate':learning_rate,'epochs':epochs,'batch_size': batch_size, 'momentum':momentum, 'nesterov': nesterov, 'decay_factor': decay_factor}\n",
    "\n",
    "    normalized_estimation_error_SGD, test_nmse_loss_SGD, test_R2_loss_SGD, test_correlation_SGD, objective_function_values_SGD,gradient_values_SGD,iterate_differences_SGD,epoch_level_gradients_SGD,epoch_level_function,tensor_iteration_SGD,factor_core_iterate_SGD = train_test_sgd(X_train, Y_train, X_test, Y_test, lambda1, hypers, Y_train_mean,lsr_tensor_SGD,B_tensored,intercept= False)\n",
    "\n",
    "  \n",
    "    #Get current time and store in variable\n",
    "    formatted_time = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    max_iter = hypers['max_iter']\n",
    "    pkl_file = f\"/Users/lakrama/Neuro Project Codes/LSR-Tensor-Ridge-Regression_All_Data/Platforms for Experments/Data/Experiments_Isolating_Sub_Problems/SGD_learning_rate{learning_rate}_intercept5_,ExecutionTime{formatted_time}, n_train_{n_train},n_test_{n_test}, tensor_dimensions:{tensor_dimensions}, tensor_mode_= ranks:{tensor_mode_ranks}, separation_rank:{separation_rank}, max_iter={max_iter}.pkl\"\n",
    "\n",
    "    with open(pkl_file, \"wb\") as file:\n",
    "      dill.dump((lsr_tensor_SGD,lambda1, normalized_estimation_error_SGD, test_nmse_loss_SGD, test_R2_loss_SGD, test_correlation_SGD, objective_function_values_SGD,gradient_values_SGD, iterate_differences_SGD,epoch_level_gradients_SGD,epoch_level_function,tensor_iteration_SGD,factor_core_iterate_SGD), file)\n",
    "\n",
    "\n",
    "    print(\"Error Report on Testing _ With best Lambda\")\n",
    "    print(\"SGD_Alpha chosen for model: \", lambda1)\n",
    "    print(\"SGD_Test Normalized Estimation Error: \", normalized_estimation_error_SGD)\n",
    "    print(\"SGD_Test NMSE Loss: \", test_nmse_loss_SGD)\n",
    "    print(\"SGD_Test R2 Loss: \", test_R2_loss_SGD)\n",
    "    print(\"SGD_Test Correlation: \", test_correlation_SGD)\n",
    "    print(\"Objective Function Values\", objective_function_values_SGD[0,1,2])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#problems\n",
    "\n",
    "#Need to look at the subproblem gradient--> is it zero\n",
    "#Now I am using an averaged gradient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchtensor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
