{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Needed Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "###system \n",
    "import sys\n",
    "\n",
    "\n",
    "###loading and saving data\n",
    "import pickle\n",
    "import dill\n",
    "import datetime\n",
    "\n",
    "\n",
    "###bases\n",
    "import numpy as np\n",
    "\n",
    "import copy\n",
    "\n",
    "#preprocessing \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading all the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 32, 32)\n",
      "(5000,)\n",
      "(1000, 32, 32)\n",
      "(1000,)\n",
      "(32, 32)\n",
      "(4, 4)\n",
      "(6000,)\n",
      "factor matrix 1_1:(32, 4)\n",
      "factor matrix 1_1:(32, 4)\n",
      "factor matrix 1_1:(32, 4)\n",
      "factor matrix 1_1:(32, 4)\n"
     ]
    }
   ],
   "source": [
    "file= open(\"/Users/lakrama/Neuro Project Codes/Datasets/Data_Sets/Synthetic Data/Uncentered X/Bounded_Var_Time:2024-04-15 19:35:40, intercept:5,n_train:5000, n_test:1000, tensor_dimensions:[32 32], tensor_mode_ranks:[4 4], separation_rank:2.pkl\", 'rb')\n",
    "data = pickle.load(file)\n",
    "file.close()\n",
    "\n",
    "X_train_Full = data[0]\n",
    "print(data[0].shape)\n",
    "\n",
    "Y_train_Full = data[1]\n",
    "print(data[1].shape)\n",
    "\n",
    "X_test_Full = data[2]\n",
    "print(data[2].shape)\n",
    "\n",
    "Y_test_Full = data[3]\n",
    "print(data[3].shape)\n",
    "\n",
    "B_tensored = data[4]\n",
    "print(data[4].shape)\n",
    "\n",
    "G1 = data[5]\n",
    "print(data[5].shape)\n",
    "\n",
    "all_factormatrices = data[6]\n",
    "#print(data[6].shape)\n",
    "\n",
    "Y_train_nonoise = data[7]\n",
    "print(data[7].shape)\n",
    "\n",
    "#all the factor matrices \n",
    "B_1_1 = all_factormatrices[0][0]\n",
    "print(f'factor matrix 1_1:{B_1_1.shape}')\n",
    "B_1_2 = all_factormatrices[0][1]\n",
    "print(f'factor matrix 1_1:{B_1_2.shape}')\n",
    "B_2_1 = all_factormatrices[1][0]\n",
    "print(f'factor matrix 1_1:{B_2_1.shape}')\n",
    "B_2_2 = all_factormatrices[0][0]\n",
    "print(f'factor matrix 1_1:{B_2_2.shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All Needed Scripts "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LSR Tensor Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Low Separation Rank Tensor Decomposition\n",
    "class LSR_tensor_dot():\n",
    "    # Constructor\n",
    "    def __init__(self, shape, ranks, separation_rank, dtype = np.float32, intercept = False ,initialize = True):\n",
    "        super(LSR_tensor_dot, self).__init__()\n",
    "        self.shape = shape\n",
    "        self.ranks = ranks\n",
    "        self.separation_rank = separation_rank\n",
    "        self.dtype = dtype\n",
    "        self.order = len(shape)\n",
    "        self.init_params(initialize)\n",
    "        self.init_params(intercept)\n",
    "\n",
    "    # Initialize Parameters\n",
    "    def init_params(self, intercept = False ,initialize = True):\n",
    "        # Initialize core tensor as independent standard gaussians\n",
    "        if not initialize:\n",
    "            self.core_tensor = np.zeros(shape = self.ranks)\n",
    "        else:\n",
    "            self.core_tensor = np.random.normal(size = self.ranks)\n",
    "\n",
    "        # Set up Factor Matrices\n",
    "        self.factor_matrices = []\n",
    "\n",
    "        # Initialize all factor matrices\n",
    "        for s in range(self.separation_rank):\n",
    "            factors_s = []\n",
    "            for k in range(self.order):\n",
    "                if not initialize:\n",
    "                    factor_matrix_B = np.eye(self.shape[k])[:, self.ranks[k]]\n",
    "                    factors_s.append(factor_matrix_B)\n",
    "                else:\n",
    "                    factor_matrix_A = np.random.normal(0,1,size= (self.shape[k], self.ranks[k]))\n",
    "                    factors_s.append(factor_matrix_A)\n",
    "\n",
    "            self.factor_matrices.append(factors_s)\n",
    "\n",
    "        if intercept:\n",
    "          ('intercept is initialized')\n",
    "          self.b = np. random.normal(0,1)\n",
    "        else:\n",
    "          (print('intercept is not initialized'))\n",
    "          self.b = 0\n",
    "\n",
    "    # Expand core tensor and factor matrices to full tensor, optionally excluding\n",
    "    # a given term from the separation rank decomposition\n",
    "    def expand_to_tensor(self, skip_term = None):\n",
    "        full_lsr_tensor = np.zeros(shape = self.shape)\n",
    "\n",
    "        #Calculate Expanded Tensor\n",
    "        for s, term_s_factors in enumerate(self.factor_matrices):\n",
    "            if s == skip_term: continue\n",
    "            expanded_tucker_term = term_s_factors[0] @ self.core_tensor @ term_s_factors[1].T\n",
    "            full_lsr_tensor += expanded_tucker_term\n",
    "\n",
    "        #Column Wise Flatten full_lsr_tensor\n",
    "        full_lsr_tensor = full_lsr_tensor.flatten(order = 'F')\n",
    "        return full_lsr_tensor\n",
    "\n",
    "    # Absorb all factor matrices and core tensor into the input tensor except for matrix s, k\n",
    "    # Used during a factor matrix update step of block coordiante descent\n",
    "    def bcd_factor_update_x_y(self, s, k, x, y):\n",
    "        #Take x and swap axes 1 and 2 so that vectorization occurs \"COLUMN WISE\"\n",
    "        x_transpose = np.transpose(x, (0, 2, 1))\n",
    "        x_transpose_vectorized = np.reshape(x_transpose, newshape = (x_transpose.shape[0], -1))\n",
    "\n",
    "        #if we are unfolding along mode 1, use x. Else, if we are unfolding along mode, use x_transpose\n",
    "        x_partial_unfold = x if k == 0 else x_transpose\n",
    "\n",
    "        #If k = 0(skip first factor matrix), we have 2nd factor matrix. If k= 1(skip second factor matrix), we have first factor matrix\n",
    "        kronecker_term = self.factor_matrices[s][1] if k == 0 else self.factor_matrices[s][0]\n",
    "\n",
    "        #if k = 0, G^T. Else if k = 1, put G\n",
    "        core_tensor_term = self.core_tensor.T if k == 0 else self.core_tensor\n",
    "\n",
    "        omega = x_partial_unfold @ kronecker_term @ core_tensor_term\n",
    "        omega = np.transpose(omega, (0, 2, 1))\n",
    "        omega = np.reshape(omega, newshape = (omega.shape[0], -1))\n",
    "\n",
    "        X_tilde = omega\n",
    "        y_tilde = y\n",
    "\n",
    "        if self.separation_rank == 1:\n",
    "            pass\n",
    "        else:\n",
    "            gamma = np.dot(x_transpose_vectorized,self.expand_to_tensor(skip_term = s))\n",
    "            #gamma = gamma.reshape(-1,1)\n",
    "            y_tilde = y - gamma\n",
    "\n",
    "        return X_tilde, y_tilde\n",
    "\n",
    "    # Absorb all factor matrices the input tensor (not the core tensor)\n",
    "    # Used during a core tensor update step of block coordiante descent\n",
    "    def bcd_core_update_x_y(self, x, y):\n",
    "        #Take x and swap axes 1 and 2 so that vectorization occurs \"COLUMN WISE\"\n",
    "        x_transpose = np.transpose(x, (0, 2, 1))\n",
    "        x_transpose_vectorized = np.reshape(x_transpose, newshape = (x_transpose.shape[0], -1))\n",
    "\n",
    "        #Calculate y_tilde\n",
    "        y_tilde = y\n",
    "\n",
    "        #Calculate Kronecker Factor Sum\n",
    "        kron_factor_sum = 0\n",
    "        for term_s_factors in self.factor_matrices:\n",
    "            kron_factor_sum += np.kron(term_s_factors[1], term_s_factors[0])\n",
    "\n",
    "        #Return Core Update\n",
    "        return (kron_factor_sum.T @ x_transpose_vectorized.T).T, y_tilde\n",
    "\n",
    "\n",
    "    #Retrieve factor matrix\n",
    "    def get_factor_matrix(self, s, k):\n",
    "      return self.factor_matrices[s][k]\n",
    "\n",
    "    #Update factor matrix\n",
    "    def update_factor_matrix(self, s, k, updated_factor_matrix: np.ndarray):\n",
    "      self.factor_matrices[s][k] = updated_factor_matrix\n",
    "\n",
    "    def update_intercept(self,updated_b):\n",
    "      self.b = updated_b\n",
    "\n",
    "    #Retrieve Core Matrix\n",
    "    def get_core_matrix(self):\n",
    "      return self.core_tensor\n",
    "\n",
    "    #Update core matrix\n",
    "    def update_core_matrix(self, updated_core_matrix: np.ndarray):\n",
    "      self.core_tensor = updated_core_matrix\n",
    "\n",
    "    #Retrive intercept\n",
    "    def get_intercept(self):\n",
    "      return self.b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BCD Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lsr_ten: LSR Tensor\n",
    "#training_data: X\n",
    "#training_labels: Y\n",
    "#hypers: hyperparameters\n",
    "def lsr_bcd_regression(lsr_ten, training_data: np.ndarray, training_labels: np.ndarray, hypers: dict,intercept = False):\n",
    "    #Get LSR Tensor Information and other hyperparameters\n",
    "    shape, ranks, sep_rank, order = lsr_ten.shape, lsr_ten.ranks, lsr_ten.separation_rank, lsr_ten.order\n",
    "    lambda1 = hypers[\"weight_decay\"]\n",
    "    max_iter = hypers[\"max_iter\"]\n",
    "    threshold = hypers[\"threshold\"]\n",
    "    lr        = hypers[\"learning_rate\"]\n",
    "    epochs    = hypers[\"epochs\"]\n",
    "    batch_size = hypers[\"batch_size\"]\n",
    "    decay_factor = hypers[\"decay_factor\"]\n",
    "    b_intercept = intercept\n",
    "\n",
    "    #Create models for each factor matrix and core matrix\n",
    "    #factor_matrix_models = [[Ridge(alpha = lambda1, solver = 'svd', fit_intercept = intercept) for k in range(len(ranks))] for s in range(sep_rank)]\n",
    "    #core_tensor_model = Ridge(alpha = lambda1, solver = 'svd', fit_intercept = intercept)\n",
    "\n",
    "    #Store objective function values\n",
    "    objective_function_values = np.ones(shape = (max_iter, sep_rank, len(ranks) + 1)) * np.inf\n",
    "\n",
    "    X, y = training_data, training_labels\n",
    "    if intercept: b_start = lsr_ten.get_intercept()\n",
    "    expanded_lsr_start  = lsr_ten.expand_to_tensor()\n",
    "    expanded_lsr_start  = np.reshape(expanded_lsr_start, X[0].shape, order = 'F')\n",
    "    objective_function_value_star = objective_function_tensor_sep(y, X, expanded_lsr_start,lsr_ten, lambda1, b if intercept else None)\n",
    "    print('Objective Function Value:',objective_function_value_star)\n",
    "\n",
    "    #Normalized Estimation Error\n",
    "    iterations_normalized_estimation_error = np.zeros(shape = (max_iter,))\n",
    "    \n",
    "    #Gradient Values\n",
    "    gradient_values = np.ones(shape = (max_iter, sep_rank, len(ranks) + 1)) * np.inf\n",
    "\n",
    "    #Epoch Level Function Values \n",
    "    epoch_level_function_values = np.ones(shape = (max_iter,sep_rank,len(ranks)+1,epochs))*np.inf\n",
    "\n",
    "    #Epoch Level Gradients\n",
    "    epoch_gradient_values = np.ones(shape = (max_iter,sep_rank,len(ranks)+1,epochs))*np.inf\n",
    "\n",
    "    #saving the tensor\n",
    "    tensor_iteration = []\n",
    "    #saving iterate-wise data\n",
    "    factor_core_iterates = []\n",
    "\n",
    "    #iterate differences \n",
    "    iterate_difference = np.ones(shape = (max_iter, sep_rank, len(ranks) + 1)) * np.inf\n",
    "\n",
    "    #Run at most max_iter iterations of Block Coordinate Descent\n",
    "    for iteration in range(max_iter):\n",
    "        #print('')\n",
    "        print('--------------------------------------------------------------BCD iteration',iteration,'--------------------------------------------------------------')\n",
    "        factor_residuals = np.zeros(shape = (sep_rank, len(ranks)))\n",
    "        core_residual = 0\n",
    "\n",
    "        #Store updates to factor matrices and core tensor\n",
    "        updated_factor_matrices = np.empty((sep_rank, len(ranks)), dtype=object)\n",
    "        updated_core_tensor = None\n",
    "\n",
    "        #Iterate over the Factor Matrices.\n",
    "        for s in range(sep_rank):\n",
    "            for k in range(len(ranks)):\n",
    "                \n",
    "                #Absorb Factor Matrices into X aside from (s, k) to get X_tilde\n",
    "                print('---------------------------------------------Sep',s,'Factor',k,'-------------------------------------------------')\n",
    "                X, y = training_data, training_labels\n",
    "                X_tilde, y_tilde = lsr_ten.bcd_factor_update_x_y(s, k, X, y) #y tilde should now be y-b-<Q,X>\n",
    "                \n",
    "\n",
    "                #Solve the sub-problem pertaining to the factor tensor\n",
    "                hypers = {'lambda': lambda1, 'lr': lr, 'epochs': epochs, 'batch_size': batch_size, 'bias': b_intercept, 'decay_factor': decay_factor}\n",
    "                #weights, bias, loss_values, gap_to_optimality, nmse_values, corr_values, R2_values,sub_problem_gradient = SGD(X_tilde, y_tilde.reshape(-1,1), cost_function_code = 1, hypers = hypers , optimizer_code = 0, p_star = 0)\n",
    "                \n",
    "                sub_problem_gradient = 0\n",
    "                loss_values = 0\n",
    "                bias = 0\n",
    "\n",
    "\n",
    "                #printing the subproblem gradients\n",
    "                #print(f\"Final gradient of the subproblem {s,k} : {sub_problem_gradient[-1]}\")\n",
    "                epoch_gradient_values[iteration,s,k,:] = sub_problem_gradient\n",
    "                epoch_level_function_values[iteration,s,k,:] = loss_values\n",
    "\n",
    "                #Retrieve Original and Updated Factor Matrices\n",
    "                #Bk = lsr_ten.get_factor_matrix(s, k)\n",
    "                #Bk1 = weights\n",
    "                if intercept: b = bias\n",
    "\n",
    "                #Shape Bk1 as needed\n",
    "                #Bk1 = np.reshape(Bk1, (shape[k], ranks[k]), order = 'F') #if there is an error check here\n",
    "\n",
    "                #fixing the factor matrices to be the true value \n",
    "                Bk  = all_factormatrices[s][k]\n",
    "                Bk1 = all_factormatrices[s][k]\n",
    "\n",
    "\n",
    "                #Update Residuals and store updated factor matrix\n",
    "                factor_residuals[s][k] = np.linalg.norm(Bk1 - Bk)\n",
    "                updated_factor_matrices[s, k] = Bk1\n",
    "\n",
    "                iterate_difference[iteration,s,k] = factor_residuals[s][k]\n",
    "\n",
    "\n",
    "                #Update Factor Matrix\n",
    "                lsr_ten.update_factor_matrix(s, k, updated_factor_matrices[s, k])\n",
    "\n",
    "                #update the intercept\n",
    "                if intercept: lsr_ten.update_intercept(b)\n",
    "\n",
    "                #Calculate Objective Function Value\n",
    "                expanded_lsr = lsr_ten.expand_to_tensor()\n",
    "                expanded_lsr = np.reshape(expanded_lsr, X[0].shape, order = 'F')\n",
    "                objective_function_value = objective_function_tensor_sep(y, X, expanded_lsr,lsr_ten,lambda1)\n",
    "                objective_function_values[iteration, s, k] = objective_function_value\n",
    "\n",
    "                #Print Objective Function Values\n",
    "                print(f\"Iteration: {iteration}, Separation Rank: {s}, Factor Matrix: {k}, Objective Function Value: {objective_function_value}\")\n",
    "                \n",
    "                #Calculate Gradient Values\n",
    "                bk = np.reshape(Bk, (-1, 1), order = 'F') #Flatten Factor Matrix Column Wise\n",
    "                Omega = X_tilde\n",
    "                z = bias\n",
    "                gradient_value = (-2 * Omega.T) @ (y_tilde.reshape(-1,1) - Omega @ bk  - z) + (2 * lambda1 * bk)\n",
    "                \n",
    "                #Store Gradient Values\n",
    "                gradient_values[iteration, s, k] = np.linalg.norm(gradient_value, ord = 'fro')\n",
    "\n",
    "\n",
    "        #Absorb necessary matrices into X, aside from core tensor, to get X_tilde\n",
    "        print('---------------------------------------------Core-------------------------------------------------')\n",
    "        X, y = training_data, training_labels\n",
    "        X_tilde, y_tilde = lsr_ten.bcd_core_update_x_y(X, y)\n",
    "\n",
    "        #Solve the sub-problem pertaining to the core tensor\n",
    "        hypers = {'lambda': lambda1, 'lr': lr, 'epochs': epochs, 'batch_size': 64, 'bias': intercept, 'decay_factor':decay_factor}\n",
    "        weights, bias, loss_values, gap_to_optimality, nmse_values, corr_values, R2_values,sub_problem_gradient,loss_values = SGD(X_tilde, y_tilde.reshape(-1,1), cost_function_code = 1, hypers = hypers, optimizer_code = 0, p_star = 0)\n",
    "\n",
    "        print(f\"Final gradient of the subproblem Core : {sub_problem_gradient[-1]}\")\n",
    "        epoch_gradient_values[iteration,:,len(ranks),:] = sub_problem_gradient\n",
    "        epoch_level_function_values[iteration,:,len(ranks),:] = loss_values\n",
    "\n",
    "        #Get Original and Updated Core Tensor\n",
    "        Gk = lsr_ten.get_core_matrix()\n",
    "        Gk1 = np.reshape(weights, ranks, order = 'F')\n",
    "        b = bias\n",
    "\n",
    "        #Update Residuals and store updated Core Tensor\n",
    "        core_residual = np.linalg.norm(Gk1 - Gk)\n",
    "        updated_core_tensor = Gk1\n",
    "\n",
    "        #saving iterate differece in a list\n",
    "        iterate_difference[iteration,:,len(ranks)] = core_residual\n",
    "\n",
    "        \n",
    "        #Update Core Tensor\n",
    "        lsr_ten.update_core_matrix(updated_core_tensor)\n",
    "\n",
    "\n",
    "        #Update Intercept\n",
    "\n",
    "        if intercept: lsr_ten.update_intercept(b)\n",
    "\n",
    "        #Calculate Objective Function Value\n",
    "        if intercept: b = lsr_ten.get_intercept()\n",
    "        expanded_lsr = lsr_ten.expand_to_tensor()\n",
    "        expanded_lsr = np.reshape(expanded_lsr, X[0].shape, order = 'F')\n",
    "\n",
    "        #saving the lsr tensor \n",
    "        tensor_iteration.append(expanded_lsr)\n",
    "\n",
    "        objective_function_value = objective_function_tensor_sep(y, X, expanded_lsr,lsr_ten, lambda1, b if intercept else None)\n",
    "        objective_function_values[iteration, :, (len(ranks))] = objective_function_value\n",
    "        \n",
    "        #print('')\n",
    "        #Print Objective Function Value\n",
    "        # print(f\"BCD Regression, Iteration: {iteration}, Core Tensor, Objective Function Value: {objective_function_value}\")\n",
    "        \n",
    "        #Calculate Gradient Values\n",
    "        g = np.reshape(Gk1, (-1, 1), order = 'F') #Flatten Core Matrix Column Wise\n",
    "        Omega = X_tilde\n",
    "        z = bias\n",
    "        gradient_value = (-2 * Omega.T) @ (y_tilde.reshape(-1,1) - Omega @ g  - z) + (2 * lambda1 * g)\n",
    "        \n",
    "        #Store Gradient Value\n",
    "        gradient_values[iteration, :, (len(ranks))] = np.linalg.norm(gradient_value, ord='fro')\n",
    "\n",
    "        #storing lsr_ten\n",
    "        factor_core_iterates.append(copy.deepcopy(lsr_ten))\n",
    "\n",
    "        #Stopping Criteria\n",
    "        diff = np.sum(factor_residuals.flatten()) + core_residual  #need to change this\n",
    "        # print('------------------------------------------------------------------------------------------')\n",
    "        # print(f\"Value of Stopping Criteria: {diff}\")\n",
    "        # print(f\"Expanded Tensor: {expanded_lsr}\")\n",
    "        # print('------------------------------------------------------------------------------------------')\n",
    "        if diff < threshold: \n",
    "            print(f'The threshold activated{diff}')\n",
    "            break\n",
    "\n",
    "\n",
    "    return lsr_ten, objective_function_values, gradient_values,iterate_difference,epoch_gradient_values,epoch_level_function_values,tensor_iteration,factor_core_iterates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Contains All Helper Functions for Optimization\n",
    "import numpy as np\n",
    "\n",
    "#Calculate value of objective function(vectorized case)\n",
    "def objective_function_vectorized(y: np.ndarray, X: np.ndarray, w: np.ndarray, alpha, b = None):\n",
    "    I = (X @ w).flatten()\n",
    "    y = y.flatten()\n",
    "    w = w.flatten()\n",
    "\n",
    "    if b is not None:\n",
    "      b = b.flatten()\n",
    "      function = (np.linalg.norm(y - I - b) ** 2) + (alpha * (np.linalg.norm(w) ** 2))\n",
    "    else:\n",
    "      function = (np.linalg.norm(y - I) ** 2) + (alpha * (np.linalg.norm(w) ** 2))\n",
    "    return function\n",
    "\n",
    "#Calculate value of objective function(tensor case)\n",
    "def objective_function_tensor(y: np.ndarray, X: np.ndarray, B: np.ndarray, alpha,b = None):\n",
    "    I = inner_product(X, B).flatten()\n",
    "    y = y.flatten()\n",
    "    B = B.flatten()\n",
    "    if b is not None:\n",
    "      b = b.flatten()\n",
    "      function = (np.linalg.norm(y - I -b) ** 2) + (alpha * (np.linalg.norm(B) ** 2))\n",
    "    else:\n",
    "      function = (np.linalg.norm(y - I) ** 2) + (alpha * (np.linalg.norm(B) ** 2))\n",
    "    return function\n",
    "\n",
    "def objective_function_tensor_sep(y: np.ndarray, X: np.ndarray, B: np.ndarray,lsr_ten, alpha,b = None):\n",
    "    I = inner_product(X, B).flatten()\n",
    "    y = y.flatten()\n",
    "    B = B.flatten()\n",
    "    regularizer = 0\n",
    "\n",
    "    #developing the separable regularizing term\n",
    "   \n",
    "    separation = len(lsr_ten.factor_matrices) \n",
    "    tucker = len(lsr_ten.factor_matrices[0])\n",
    "    \n",
    "    for s in range(separation):\n",
    "       for k in range(tucker):\n",
    "          regularizer += (np.linalg.norm(lsr_ten.factor_matrices[s][k])**2)\n",
    "    regularizer = regularizer + (np.linalg.norm(lsr_ten.core_tensor)**2)\n",
    "    if b is not None:\n",
    "      b = b.flatten()\n",
    "      function = (np.linalg.norm(y - I -b) ** 2) + (alpha * regularizer)\n",
    "    else:\n",
    "      function = (np.linalg.norm(y - I) ** 2) + (alpha * regularizer)\n",
    "    return function\n",
    "\n",
    "#Calculate x* and p* for Objective Function(Tensor Case)\n",
    "#X_train is a Tensor of samples x m x n\n",
    "#Y_train is a normal vector of size samples x 1. It can also be a flattened array of size (samples, )\n",
    "def calculate_optimal_iterate_and_function_value(X_train: np.ndarray, Y_train: np.ndarray, lambda1):\n",
    "    X_train = X_train.reshape((X_train.shape[0], -1))\n",
    "    Y_train = Y_train.reshape((-1, 1))\n",
    "\n",
    "    #Calculate Optimal Weight Tensor and Optimal Objective Function Value\n",
    "    B_optimal = np.linalg.inv(X_train.T @ X_train + lambda1 * np.eye(X_train.shape[1])) @ X_train.T @ Y_train\n",
    "    I = X_train @ B_optimal\n",
    "    p_star = (np.linalg.norm(Y_train - I) ** 2) + (lambda1 * (np.linalg.norm(B_optimal) ** 2))\n",
    "\n",
    "    return B_optimal, p_star\n",
    "\n",
    "\n",
    "#Inner product of two tensors\n",
    "#tensor1: samples x m x n\n",
    "#tensor2: m x n\n",
    "def inner_product(tensor1: np.ndarray, tensor2: np.ndarray):\n",
    "    tensor1 = tensor1.reshape(tensor1.shape[0], -1)\n",
    "    tensor2 = tensor2.reshape(-1, 1)\n",
    "    return tensor1 @ tensor2\n",
    "\n",
    "#Calculate R2 Score\n",
    "def R2(y_true, y_pred):\n",
    "    #Flatten for insurance\n",
    "    y_true = y_true.flatten()\n",
    "    y_pred = y_pred.flatten()\n",
    "\n",
    "    #Calculate R2 Score\n",
    "    y_true_mean = np.mean(y_true)\n",
    "    tss = np.sum((y_true - y_true_mean) ** 2)\n",
    "    rss = np.sum((y_true - y_pred) ** 2)\n",
    "    r2 = 1 - (rss / tss)\n",
    "    return r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import Optimization Toolkits\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import r2_score\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "\n",
    "#Cost Function[Least Squares]\n",
    "#||(XW + b) - Y||_2^2\n",
    "class LeastSquares(nn.Module):\n",
    "    def __init__(self, input_dim, uses_bias = False):\n",
    "        super(LeastSquares, self).__init__() #Initialize class\n",
    "        self.linear = nn.Linear(input_dim, 1, uses_bias) #input_dim = dimension of each sample in X, output_dim = 1 = number of y values for each sample\n",
    "                \n",
    "    #Evaluate the Cost Function given X and y\n",
    "    def evaluate(self, X, Y, reduction = 'sum'):\n",
    "        mse_loss = nn.MSELoss(reduction = reduction)\n",
    "        return mse_loss(self.linear(X), Y.rehshape(-1,1))\n",
    "\n",
    "#Cost Function[Least Squares + L2 Regularization Term]\n",
    "#||(XW + b) - Y ||_2^2 + lambda * ||w||^2_2\n",
    "class RidgeRegression(nn.Module):\n",
    "    def __init__(self, input_dim, lmbd, uses_bias = False):\n",
    "        super(RidgeRegression, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, 1, uses_bias) #input_dim = dimension of each sample in X, output_dim = 1 = number of y values for each sample\n",
    "        self.lmbd = lmbd #Ridge Regression Lambda Value\n",
    "        \n",
    "    #Evaluate the Cost Function given X and y\n",
    "    def evaluate(self, X, Y, reduction = 'sum'):\n",
    "        mse_loss = nn.MSELoss(reduction = reduction)\n",
    "        return mse_loss(self.linear(X), Y) + self.l2_regularization()\n",
    "            \n",
    "    #Calculate value of lambda * ||w||^2_2\n",
    "    def l2_regularization(self):\n",
    "        return self.lmbd * (torch.norm(self.linear.weight) ** 2)\n",
    "    \n",
    "#Perform Exact Line Search for Ridge Regression\n",
    "#Ridge Regression: ||(XW + b) - Y ||_2^2 + lambda * ||w||^2_2\n",
    "def exact_line_search_RR(X: np.ndarray, Y: np.ndarray, lmbd, cost_function, uses_bias):\n",
    "    #Get Model Parameters\n",
    "    W = cost_function.linear.weight.data.numpy().reshape((-1, 1)) \n",
    "    b = cost_function.linear.bias.item() if uses_bias else 0\n",
    "    \n",
    "    #Search Direction\n",
    "    DeltaW = -1 * cost_function.linear.weight.grad.numpy().reshape((-1, 1))\n",
    "    Deltab = -1 * cost_function.linear.bias.grad if uses_bias else 0\n",
    "    \n",
    "    #Compute value of t\n",
    "    numerator = -((X@W + b - Y).T @ (X @ DeltaW + Deltab)) - (lmbd * (W.T @ DeltaW))\n",
    "    denominator = (np.linalg.norm(X @ DeltaW + Deltab) ** 2) + (lmbd * (np.linalg.norm(DeltaW) ** 2))\n",
    "    t = (numerator / denominator) [0, 0]\n",
    "    \n",
    "    return t    \n",
    "    \n",
    "#Optimize a Cost Function via Stochastic Gradient Descent\n",
    "#X: Shape n x d where n is the number of samples and d is the number of features\n",
    "#Y: Shape n x 1 where n is the number of samples\n",
    "#cost_function_code: 0 for Normal Least Squares, 1 for Ridge Regression\n",
    "#hypers: hyperparameters\n",
    "#optimizer_code: 0 for SGD, 1 for Adagrad, 2 for RMSProp\n",
    "#p_star: estimated optimal value\n",
    "#W_true: true weights\n",
    "def SGD(X: np.ndarray, Y: np.ndarray, cost_function_code = 1, hypers = {}, optimizer_code = 0, p_star = 0, W_true = None):\n",
    "    hypers = defaultdict(int, hypers) #Convert hypers to defaultdict\n",
    "\n",
    "    #Get necessary hyperparameters\n",
    "    uses_bias = hypers['bias'] #determine whether the bias term is needed\n",
    "    lmbd = hypers['lambda'] #Lambda for ridge regression\n",
    "    lr = hypers['lr'] #learning rate\n",
    "    epochs = hypers['epochs'] #number of epochs\n",
    "    batch_size = hypers['batch_size'] #batch size to use for SGD\n",
    "\n",
    "    #Get additional hyperparameters\n",
    "    momentum = hypers['momentum']\n",
    "    dampening = hypers['dampening']\n",
    "    nesterov = hypers['nesterov']\n",
    "    decay_factor = hypers['decay_factor']\n",
    "    \n",
    "    #Initialize Cost Function\n",
    "    if cost_function_code == 0:\n",
    "        cost_function = LeastSquares(X.shape[1], uses_bias)\n",
    "    elif cost_function_code == 1:\n",
    "        cost_function = RidgeRegression(X.shape[1], lmbd, uses_bias)\n",
    "    \n",
    "    #Convert X and Y to pytorch tensors\n",
    "    X = torch.tensor(X, dtype = torch.float32)\n",
    "    Y = torch.tensor(Y, dtype = torch.float32)\n",
    "    \n",
    "    #Splitting data into minibatches \n",
    "    dataset = TensorDataset(X,Y)\n",
    "    dataloader = DataLoader(dataset, batch_size = batch_size,  shuffle=True )\n",
    "    \n",
    "    #Initialize Optimizer\n",
    "    if optimizer_code == 0:\n",
    "        optimizer = optim.SGD(cost_function.parameters(), lr = lr, momentum = momentum, dampening = dampening, nesterov = nesterov)\n",
    "    elif optimizer_code == 1:\n",
    "        optimizer = optim.Adagrad(cost_function.parameters(), lr = lr)\n",
    "    elif optimizer_code == 2:\n",
    "        optimizer = optim.RMSprop(cost_function.parameters(), lr = lr, alpha = decay_factor, momentum = momentum)\n",
    "\n",
    "    #scheduler = MultiStepLR(optimizer,milestones=[80],gamma = 100)\n",
    "    \n",
    "    #Store batch loss values\n",
    "    loss_values = []\n",
    "    \n",
    "\n",
    "    #Store gap to optimality\n",
    "    gap_to_optimality = []\n",
    "\n",
    "    #saving gradient \n",
    "    sub_problem_gradient =  []\n",
    "    \n",
    "    #Store Metric Values\n",
    "\n",
    "    nmse_values = []\n",
    "    corr_values = []\n",
    "    R2_values = []\n",
    "\n",
    "    #Training Loop\n",
    "    for epoch in range(epochs):\n",
    "    \n",
    "        num_batches = 0\n",
    "        sum_gradient_norm = 0\n",
    "\n",
    "        for X_sample, Y_sample in dataloader:\n",
    "        \n",
    "            Y_sample = Y_sample.reshape(-1,1)\n",
    "\n",
    "            # Compute stochastic loss\n",
    "            stochastic_loss = cost_function.evaluate(X_sample, Y_sample, 'sum')\n",
    "            \n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Backward pass to compute stochastic gradient\n",
    "            stochastic_loss.backward()\n",
    "            \n",
    "            # Update parameters\n",
    "            optimizer.step()\n",
    "            \n",
    "            #storing the batch wise gradients \n",
    "            for param in cost_function.parameters():\n",
    "               #print(f\"Gradient Norm: {torch.norm(param.grad)}\")\n",
    "               stochastic_gradient  = torch.norm(param.grad)\n",
    "               sum_gradient_norm += stochastic_gradient\n",
    "            \n",
    "            num_batches += 1\n",
    "\n",
    "        #Print and Store batch loss values\n",
    "        batch_loss = cost_function.evaluate(X, Y, 'sum')\n",
    "        loss_values.append(batch_loss.item())\n",
    "        gap_to_optimality.append(batch_loss.item() - p_star)\n",
    "        \n",
    "        #learning rate reduction \n",
    "        #scheduler.step()\n",
    "\n",
    "        #Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Backward pass to compute stochastic gradient\n",
    "        batch_loss.backward()\n",
    "\n",
    "        for param in cost_function.parameters():\n",
    "                print(f\"Gradient Norm: {torch.norm(param.grad)}\")\n",
    "                gradient_after_epoch = torch.norm(param.grad)\n",
    "                sub_problem_gradient.append(gradient_after_epoch)\n",
    "\n",
    "        #epcoh_gradient = sum_gradient_norm/num_batches\n",
    "        #sub_problem_gradient.append(epcoh_gradient)\n",
    "        \n",
    "        #Calculate Metrics\n",
    "        weights = cost_function.linear.weight.data.numpy().reshape((-1, 1))\n",
    "        bias = cost_function.linear.bias.item() if uses_bias else 0\n",
    "        X_numpy = X.numpy()\n",
    "        Y_predicted = X_numpy @ weights + bias\n",
    "        Y_numpy = Y.numpy()\n",
    "        \n",
    "\n",
    "        nmse = np.sum(np.square((Y_predicted - Y_numpy))) / np.sum(np.square(Y_numpy))\n",
    "        correlation = np.corrcoef(Y_predicted.flatten(), Y_numpy.flatten())[0, 1]\n",
    "        R2_score = r2_score(Y_numpy, Y_predicted)\n",
    "        \n",
    "        nmse_values.append(nmse)\n",
    "        corr_values.append(correlation)\n",
    "        R2_values.append(R2_score)\n",
    "        \n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {batch_loss:.4f}, Gap to Optimality: {gap_to_optimality[-1]:.4f}, NMSE: {nmse}, Correlation: {correlation}, R2: {R2_score}')\n",
    "\n",
    "    # Create a figure with two horizontal subplots\n",
    "    #fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "    # Plotting in the first subplot (ax1)\n",
    "    #ax1.plot(range(1, len(loss_values) + 1), loss_values, marker='o')\n",
    "    #ax1.set_title('Subproblem Loss')\n",
    "    #ax1.set_yscale('log')\n",
    "    #ax1.set_xlabel('Epoch')\n",
    "    #ax1.set_ylabel('Loss')\n",
    "    #ax1.grid(True)\n",
    "\n",
    "    # Plotting in the second subplot (ax2)\n",
    "    #ax2.plot(range(1, len(sub_problem_gradient) + 1), sub_problem_gradient, marker='o')\n",
    "    #ax2.set_title('Subproblem Gradient')\n",
    "    #ax2.set_yscale('log')\n",
    "    #ax2.set_xlabel('Epoch')\n",
    "    #ax2.set_ylabel('Loss')\n",
    "    #ax2.grid(True)\n",
    "\n",
    "    # Adjust the layout to prevent overlap\n",
    "    #plt.tight_layout()\n",
    "\n",
    "    # Display the plot\n",
    "    #plt.show()\n",
    "\n",
    "    weights = cost_function.linear.weight.data.numpy().reshape((-1, 1)) #Return weights as numpy array\n",
    "\n",
    "    #return weights and bias and loss metrics\n",
    "    if uses_bias:\n",
    "        return weights, cost_function.linear.bias.item(), loss_values, gap_to_optimality, nmse_values, corr_values, R2_values,sub_problem_gradient,loss_values\n",
    "    else:\n",
    "        return weights, 0, loss_values, gap_to_optimality, nmse_values, corr_values, R2_values,sub_problem_gradient,loss_values\n",
    "    \n",
    "\n",
    "#Optimize a Cost Function via Gradient Descent with Exact Line Search\n",
    "#X: Shape n x d where n is the number of samples and d is the number of features\n",
    "#Y: Shape n x 1 where n is the number of samples\n",
    "#cost_function_code: 0 for Normal Least Squares, 1 for Ridge Regression\n",
    "#hypers: hyperparameters\n",
    "#p_star: estimated optimal value\n",
    "#W_true: true weights\n",
    "def GD2(X: np.ndarray, Y: np.ndarray, cost_function_code = 1, hypers = {}, p_star = 0, W_true = None):\n",
    "    hypers = defaultdict(int, hypers) #Convert hypers to defaultdict\n",
    "    \n",
    "    #Get necessary hyperparameters\n",
    "    uses_bias = hypers['bias'] #determine whether the bias term is needed\n",
    "    lmbd = hypers['lambda'] #Lambda for ridge regression\n",
    "    epochs = hypers['epochs'] #number of epochs\n",
    "    \n",
    "    #Initialize Cost Function\n",
    "    if cost_function_code == 0:\n",
    "        cost_function = LeastSquares(X.shape[1], uses_bias)\n",
    "    elif cost_function_code == 1:\n",
    "        cost_function = RidgeRegression(X.shape[1], lmbd, uses_bias)\n",
    "    \n",
    "    #Convert X and Y to pytorch tensors\n",
    "    X_tensor = torch.tensor(X, dtype = torch.float32)\n",
    "    Y_tensor = torch.tensor(Y, dtype = torch.float32)\n",
    "    \n",
    "    #If W_true is None, set it to a zero vector\n",
    "    #if not isinstance(W_true, np.ndarray):\n",
    "    #    W_true = np.zeros(shape = (X.shape[1], 1))\n",
    "        \n",
    "    #Store batch loss values\n",
    "    loss_values = []\n",
    "    \n",
    "    #Store gap to optimality\n",
    "    gap_to_optimality = []\n",
    "    \n",
    "    #Store Metric Values \n",
    "    #nee_values = []\n",
    "    nmse_values = []\n",
    "    corr_values = []\n",
    "    R2_values = []\n",
    "\n",
    "    #Training Loop\n",
    "    for epoch in range(epochs):\n",
    "        # Zero the gradients\n",
    "        for param in cost_function.parameters():\n",
    "            if param.grad is not None:\n",
    "                param.grad.zero_()\n",
    "            \n",
    "        # Compute loss\n",
    "        loss = cost_function.evaluate(X_tensor, Y_tensor, 'sum')\n",
    "\n",
    "        # Backward pass to compute gradient\n",
    "        loss.backward()\n",
    "\n",
    "        # Update parameters\n",
    "        t = exact_line_search_RR(X, Y, lmbd, cost_function, uses_bias)\n",
    "        print(f\"Value of t is: {t}\")\n",
    "        \n",
    "        # Manually update the weights and biases\n",
    "        with torch.no_grad():\n",
    "            for param in cost_function.parameters():\n",
    "                param -= t * param.grad\n",
    "        \n",
    "        #Print and Store loss values\n",
    "        loss_value = cost_function.evaluate(X_tensor, Y_tensor, 'sum').item()\n",
    "        loss_values.append(loss_value)\n",
    "        gap_to_optimality.append(loss_value - p_star)\n",
    "        \n",
    "        #Calculate Metrics\n",
    "        weights = cost_function.linear.weight.data.numpy().reshape((-1, 1))\n",
    "        bias = cost_function.linear.bias.item() if uses_bias else 0\n",
    "        X_numpy = X_tensor.numpy()\n",
    "        Y_predicted = X_numpy @ weights + bias\n",
    "        Y_numpy = Y_tensor.numpy()\n",
    "        \n",
    "        #nee = ((np.linalg.norm(weights - W_true)) ** 2) /  ((np.linalg.norm(W_true)) ** 2)\n",
    "        nmse = np.sum(np.square((Y_predicted - Y_numpy))) / np.sum(np.square(Y_numpy))\n",
    "        correlation = np.corrcoef(Y_predicted.flatten(), Y_numpy.flatten())[0, 1]\n",
    "        R2_score = r2_score(Y_numpy, Y_predicted)\n",
    "        \n",
    "        #nee_values.append(nee)\n",
    "        nmse_values.append(nmse)\n",
    "        corr_values.append(correlation)\n",
    "        R2_values.append(R2_score)\n",
    "                \n",
    "        #print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss_value:.4f}, Gap to Optimality: {gap_to_optimality[-1]:.4f}, NMSE: {nmse}, Correlation: {correlation}, R2: {R2_score}')\n",
    "        \n",
    "        # Stopping Criteria\n",
    "        criteria_satisfied = True\n",
    "        for name, param in cost_function.named_parameters():\n",
    "            if param.grad is not None:\n",
    "                print(f\"Gradient Norm for {name}: {torch.norm(param.grad)}\")\n",
    "                criteria_satisfied = criteria_satisfied and (torch.norm(param.grad) <= 0.001)\n",
    "            else:\n",
    "                print(f\"No gradient Norm for {name}\")\n",
    "        \n",
    "        if criteria_satisfied:\n",
    "            break\n",
    "\n",
    "    weights = cost_function.linear.weight.data.numpy().reshape((-1, 1)) #Return weights as numpy array\n",
    "\n",
    "    #return weights and bias and loss metrics\n",
    "    if uses_bias:\n",
    "        return weights, cost_function.linear.bias.item(), loss_values, gap_to_optimality, nmse_values, corr_values, R2_values\n",
    "    else:\n",
    "        return weights, 0, loss_values, gap_to_optimality, nmse_values, corr_values, R2_values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_sgd(X_train: np.ndarray, Y_train: np.ndarray, X_test: np.ndarray, Y_test: np.ndarray, lambda1, hypers,Y_train_mean,lsr_tensor_SGD,B_tensored = None,intercept = False):\n",
    "  hypers['weight_decay'] = lambda1\n",
    "\n",
    "  \n",
    "  #Define LSR Tensor Hyperparameters\n",
    "  ranks = hypers['ranks']\n",
    "  separation_rank = hypers['separation_rank']\n",
    "  LSR_tensor_dot_shape = tuple(X_train.shape)[1:]\n",
    "  need_intercept = intercept\n",
    "\n",
    "  #Construct LSR Tensor\n",
    "  lsr_tensor = lsr_tensor_SGD\n",
    "  lsr_tensor, objective_function_values, gradient_values,iterate_differences,epoch_level_gradients,epoch_level_function,tensor_iteration,factor_core_iterate = lsr_bcd_regression(lsr_tensor, X_train, Y_train, hypers,intercept = need_intercept)\n",
    "  expanded_lsr = lsr_tensor.expand_to_tensor()\n",
    "  expanded_lsr = np.reshape(expanded_lsr, X_train[0].shape, order = 'F')\n",
    "  print(expanded_lsr.shape)\n",
    "  Y_test_predicted = inner_product(np.transpose(X_test, (0, 2, 1)), expanded_lsr.flatten(order ='F')) + lsr_tensor.b + Y_train_mean\n",
    "\n",
    "  print('---------------------------Testing with Best Lambda------------------------------')\n",
    "  #print(f\"Y_test_predicted: {Y_test_predicted.flatten()}, Y_test: {Y_test.flatten()}\")\n",
    "  test_nmse_loss = np.sum(np.square((Y_test_predicted.flatten() - Y_test.flatten()))) / np.sum(np.square(Y_test.flatten()))\n",
    "  if B_tensored is not None:\n",
    "    normalized_estimation_error = ((np.linalg.norm(expanded_lsr - B_tensored)) ** 2) /  ((np.linalg.norm(B_tensored)) ** 2)\n",
    "  test_R2_loss = R2(Y_test.flatten(), Y_test_predicted.flatten())\n",
    "  test_correlation = np.corrcoef(Y_test_predicted.flatten(), Y_test.flatten())[0, 1]\n",
    "\n",
    "  print(\"Y Test Predicted: \", Y_test_predicted.flatten())\n",
    "  print(\"Y Test Actual: \", Y_test.flatten())\n",
    "\n",
    "  if B_tensored is not None:\n",
    "    return normalized_estimation_error, test_nmse_loss, test_R2_loss, test_correlation, objective_function_values,gradient_values,iterate_differences,epoch_level_gradients,epoch_level_function,tensor_iteration,factor_core_iterate\n",
    "  else:\n",
    "    normalized_estimation_error = np.inf\n",
    "    return normalized_estimation_error, test_nmse_loss, test_R2_loss, test_correlation, objective_function_values,gradient_values,iterate_differences,epoch_level_gradients,epoch_level_function,tensor_iteration,factor_core_iterate\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting the regression implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Setting up global parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_dimensions = np.array([32, 32])\n",
    "tensor_mode_ranks = np.array([4, 4])\n",
    "separation_rank = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample mean for each feature (across samples): [4.99871322 4.98420917 5.00873521 ... 5.01840176 4.98600789 5.01023623]\n",
      "Sample variance for each feature (across samples): None\n",
      "Response Average: 83.56008733093063\n"
     ]
    }
   ],
   "source": [
    "n_train = 650\n",
    "n_test = 100\n",
    "\n",
    "#Subset X_train and Y_train\n",
    "X_train = X_train_Full[0:(n_train),:,:]\n",
    "Y_train = Y_train_Full[0:(n_train)]\n",
    "\n",
    "#Subset X_test and Y_test\n",
    "X_test = X_test_Full[0:(n_test),:,:]\n",
    "Y_test = Y_test_Full[0:(n_test)]\n",
    "\n",
    "\n",
    "#Preprocessing\n",
    "\n",
    "# Reshape the 3D array to a 2D array where each row represents a sample\n",
    "# The shape of the original 3D array is (n_samples, n_features_per_sample, n_dimensions)\n",
    "# We reshape it to (n_samples, n_features_per_sample * n_dimensions)\n",
    "\n",
    "\n",
    "X_train_2D = X_train.reshape(n_train, -1)\n",
    "X_test_2D = X_test.reshape(n_test,-1)\n",
    "\n",
    "\n",
    "# Initialize StandardScaler\n",
    "scaler = StandardScaler(with_std = False) #standard scalar only\n",
    "\n",
    "# Fit scaler on train data and transform train data\n",
    "X_train_scaled = scaler.fit_transform(X_train_2D)\n",
    "# Transform test data using the scaler fitted on train data\n",
    "X_test_scaled = scaler.transform(X_test_2D)\n",
    "\n",
    "# Reshape the scaled data back to 3D\n",
    "X_train = X_train_scaled.reshape(n_train, tensor_dimensions[0],tensor_dimensions[1])\n",
    "X_test  = X_test_scaled.reshape(n_test, tensor_dimensions[0],tensor_dimensions[1])\n",
    "\n",
    "#average response value\n",
    "Y_train_mean = np.mean(Y_train)\n",
    "#Mean centering y_train and y_test\n",
    "Y_train = Y_train - Y_train_mean\n",
    "\n",
    "\n",
    "print(\"Sample mean for each feature (across samples):\",scaler.mean_)\n",
    "print(\"Sample variance for each feature (across samples):\",scaler.var_)\n",
    "print('Response Average:',Y_train_mean)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Intializing the tensor object "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intializing the tensor object \n",
    "\n",
    "hypers = {'max_iter': 50, 'threshold': 1e-8, 'ranks': tuple(tensor_mode_ranks), 'separation_rank': separation_rank}\n",
    "\n",
    "ranks = hypers['ranks']\n",
    "separation_rank = hypers['separation_rank']\n",
    "LSR_tensor_dot_shape = tuple(X_train.shape)[1:]\n",
    "need_intercept = False\n",
    "\n",
    "#initializing the tensor object\n",
    "#lsr_tensor = LSR_tensor_dot(shape = LSR_tensor_dot_shape, ranks = ranks, separation_rank = separation_rank, intercept = need_intercept)\n",
    "\n",
    "#regularization parameter\n",
    "lambda1 = 50\n",
    "\n",
    "\n",
    "#saving the initializer\n",
    "#formatted_time = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "#pkl_file = f\"/Users/lakrama/Neuro Project Codes/LSR-Tensor-Ridge-Regression_All_Data/Platforms for Experments/Data/Experiments_Isolating_Sub_Problems/Initializers_ExecutionTime_intercept_5_{formatted_time}, tensor_dimensions:{tensor_dimensions}, tensor_mode_= ranks:{tensor_mode_ranks}, separation_rank:{separation_rank}.pkl\"\n",
    "\n",
    "#with open(pkl_file, \"wb\") as file:\n",
    "#      dill.dump((lsr_tensor), file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the initializer\n",
    "\n",
    "# intializing the tensor object\n",
    "import sys\n",
    "sys.path.append('/Users/lakrama/Neuro Project Codes/LSR-Tensor-Ridge-Regression/Platforms for Experments/CodeFiles') \n",
    "from LSR_Tensor_2D_v1 import LSR_tensor_dot\n",
    "\n",
    "import pickle\n",
    "pkl_file = \"/Users/lakrama/Neuro Project Codes/LSR-Tensor-Ridge-Regression_All_Data/Platforms for Experments/Data/Experiments_Isolating_Sub_Problems/Initializers_ExecutionTime_intercept_5_2024-06-23 21:06:37, tensor_dimensions:[32 32], tensor_mode_= ranks:[4 4], separation_rank:2.pkl\"\n",
    "file= open(pkl_file, 'rb')\n",
    "lsr_tensor_initializer = pickle.load(file)\n",
    "file.close()\n",
    "\n",
    "lsr_tensor = copy.deepcopy(lsr_tensor_initializer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Trainning and Testing SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------------------Learning Rate 7e-07 -----------------------------------------\n",
      "Objective Function Value: 4831437.893446013\n",
      "--------------------------------------------------------------BCD iteration 0 --------------------------------------------------------------\n",
      "---------------------------------------------Sep 0 Factor 0 -------------------------------------------------\n",
      "Iteration: 0, Separation Rank: 0, Factor Matrix: 0, Objective Function Value: 4233888.149107747\n",
      "---------------------------------------------Sep 0 Factor 1 -------------------------------------------------\n",
      "Iteration: 0, Separation Rank: 0, Factor Matrix: 1, Objective Function Value: 3210320.131039708\n",
      "---------------------------------------------Sep 1 Factor 0 -------------------------------------------------\n",
      "Iteration: 0, Separation Rank: 1, Factor Matrix: 0, Objective Function Value: 3743681.83633678\n",
      "---------------------------------------------Sep 1 Factor 1 -------------------------------------------------\n",
      "Iteration: 0, Separation Rank: 1, Factor Matrix: 1, Objective Function Value: 3414681.423897649\n",
      "---------------------------------------------Core-------------------------------------------------\n",
      "Gradient Norm: 506784.0\n",
      "Epoch [1/100], Loss: 182439.9531, Gap to Optimality: 182439.9531, NMSE: 0.528853178024292, Correlation: 0.7397763273439009, R2: 0.47114680421522526\n",
      "Gradient Norm: 298970.1875\n",
      "Epoch [2/100], Loss: 68644.1719, Gap to Optimality: 68644.1719, NMSE: 0.19893451035022736, Correlation: 0.9397065286214765, R2: 0.8010654870368696\n",
      "Gradient Norm: 182408.734375\n",
      "Epoch [3/100], Loss: 28147.3516, Gap to Optimality: 28147.3516, NMSE: 0.08150991797447205, Correlation: 0.9769529283707986, R2: 0.9184900809651044\n",
      "Gradient Norm: 115532.8828125\n",
      "Epoch [4/100], Loss: 12637.5088, Gap to Optimality: 12637.5088, NMSE: 0.03652885928750038, Correlation: 0.9887412863390573, R2: 0.9634711411550382\n",
      "Gradient Norm: 76421.34375\n",
      "Epoch [5/100], Loss: 6200.5615, Gap to Optimality: 6200.5615, NMSE: 0.01785602793097496, Correlation: 0.9938503246581608, R2: 0.9821439720488245\n",
      "Gradient Norm: 52861.30859375\n",
      "Epoch [6/100], Loss: 3292.7578, Gap to Optimality: 3292.7578, NMSE: 0.009418376721441746, Correlation: 0.996436364590438, R2: 0.9905816230086072\n",
      "Gradient Norm: 37994.765625\n",
      "Epoch [7/100], Loss: 1859.7451, Gap to Optimality: 1859.7451, NMSE: 0.005258909892290831, Correlation: 0.9978739854411354, R2: 0.9947410896190716\n",
      "Gradient Norm: 28113.48828125\n",
      "Epoch [8/100], Loss: 1102.2271, Gap to Optimality: 1102.2271, NMSE: 0.0030594223644584417, Correlation: 0.9987050084062193, R2: 0.9969405777518316\n",
      "Gradient Norm: 21276.46875\n",
      "Epoch [9/100], Loss: 680.8919, Gap to Optimality: 680.8919, NMSE: 0.0018356257351115346, Correlation: 0.999198251703816, R2: 0.9981643744193069\n",
      "Gradient Norm: 16364.890625\n",
      "Epoch [10/100], Loss: 436.8396, Gap to Optimality: 436.8396, NMSE: 0.0011265008943155408, Correlation: 0.9994983099936415, R2: 0.9988734991025544\n",
      "Gradient Norm: 12759.7431640625\n",
      "Epoch [11/100], Loss: 291.6676, Gap to Optimality: 291.6676, NMSE: 0.0007045154925435781, Correlation: 0.9996828456165403, R2: 0.9992954845444791\n",
      "Gradient Norm: 10045.650390625\n",
      "Epoch [12/100], Loss: 203.2198, Gap to Optimality: 203.2198, NMSE: 0.00044727945351041853, Correlation: 0.9997974736955109, R2: 0.9995527205185477\n",
      "Gradient Norm: 7986.95849609375\n",
      "Epoch [13/100], Loss: 148.5713, Gap to Optimality: 148.5713, NMSE: 0.0002882579283323139, Correlation: 0.9998695560600876, R2: 0.9997117420541286\n",
      "Gradient Norm: 6403.58447265625\n",
      "Epoch [14/100], Loss: 114.2499, Gap to Optimality: 114.2499, NMSE: 0.00018831509805750102, Correlation: 0.9999152048758679, R2: 0.9998116849115591\n",
      "Gradient Norm: 5171.34912109375\n",
      "Epoch [15/100], Loss: 92.3992, Gap to Optimality: 92.3992, NMSE: 0.00012462184531614184, Correlation: 0.9999442390775201, R2: 0.9998753781533669\n",
      "Gradient Norm: 4213.48583984375\n",
      "Epoch [16/100], Loss: 78.3542, Gap to Optimality: 78.3542, NMSE: 8.364339009858668e-05, Correlation: 0.9999630296394125, R2: 0.9999163566112682\n",
      "Gradient Norm: 3471.27392578125\n",
      "Epoch [17/100], Loss: 69.2571, Gap to Optimality: 69.2571, NMSE: 5.7074346841545776e-05, Correlation: 0.999975299269678, R2: 0.9999429256568425\n",
      "Gradient Norm: 2896.41015625\n",
      "Epoch [18/100], Loss: 63.3116, Gap to Optimality: 63.3116, NMSE: 3.968858072767034e-05, Correlation: 0.9999833281150834, R2: 0.9999603114213306\n",
      "Gradient Norm: 2454.55615234375\n",
      "Epoch [19/100], Loss: 59.3931, Gap to Optimality: 59.3931, NMSE: 2.8215928978170268e-05, Correlation: 0.9999886331029589, R2: 0.9999717840705094\n",
      "Gradient Norm: 2102.343505859375\n",
      "Epoch [20/100], Loss: 56.7448, Gap to Optimality: 56.7448, NMSE: 2.04418356588576e-05, Correlation: 0.9999921387605328, R2: 0.9999795581645676\n",
      "Gradient Norm: 1837.44189453125\n",
      "Epoch [21/100], Loss: 54.9751, Gap to Optimality: 54.9751, NMSE: 1.5239730601024348e-05, Correlation: 0.9999945010792813, R2: 0.999984760270172\n",
      "Gradient Norm: 1637.6923828125\n",
      "Epoch [22/100], Loss: 53.7752, Gap to Optimality: 53.7752, NMSE: 1.1705648830684368e-05, Correlation: 0.9999961013728823, R2: 0.9999882943512902\n",
      "Gradient Norm: 1483.7947998046875\n",
      "Epoch [23/100], Loss: 52.9441, Gap to Optimality: 52.9441, NMSE: 9.250455150322523e-06, Correlation: 0.999997190660551, R2: 0.9999907495445206\n",
      "Gradient Norm: 1369.131103515625\n",
      "Epoch [24/100], Loss: 52.3694, Gap to Optimality: 52.3694, NMSE: 7.54861412133323e-06, Correlation: 0.9999979402391342, R2: 0.9999924513856977\n",
      "Gradient Norm: 1282.7789306640625\n",
      "Epoch [25/100], Loss: 51.9663, Gap to Optimality: 51.9663, NMSE: 6.350896455842303e-06, Correlation: 0.9999984578628862, R2: 0.9999936491041927\n",
      "Gradient Norm: 1215.9658203125\n",
      "Epoch [26/100], Loss: 51.6744, Gap to Optimality: 51.6744, NMSE: 5.479651463247137e-06, Correlation: 0.999998823255638, R2: 0.9999945203485903\n",
      "Gradient Norm: 1165.8544921875\n",
      "Epoch [27/100], Loss: 51.4644, Gap to Optimality: 51.4644, NMSE: 4.85046030007652e-06, Correlation: 0.999999082939857, R2: 0.9999951495396318\n",
      "Gradient Norm: 1131.0625\n",
      "Epoch [28/100], Loss: 51.3176, Gap to Optimality: 51.3176, NMSE: 4.409823304740712e-06, Correlation: 0.999999266883946, R2: 0.999995590176782\n",
      "Gradient Norm: 1096.5887451171875\n",
      "Epoch [29/100], Loss: 51.1993, Gap to Optimality: 51.1993, NMSE: 4.051466930832248e-06, Correlation: 0.9999993946850481, R2: 0.9999959485331795\n",
      "Gradient Norm: 1076.2269287109375\n",
      "Epoch [30/100], Loss: 51.1203, Gap to Optimality: 51.1203, NMSE: 3.8126379422465106e-06, Correlation: 0.9999994890352241, R2: 0.9999961873621637\n",
      "Gradient Norm: 1068.79296875\n",
      "Epoch [31/100], Loss: 51.0740, Gap to Optimality: 51.0740, NMSE: 3.673812670967891e-06, Correlation: 0.9999995579468666, R2: 0.999996326187094\n",
      "Gradient Norm: 1057.6163330078125\n",
      "Epoch [32/100], Loss: 51.0303, Gap to Optimality: 51.0303, NMSE: 3.540513716870919e-06, Correlation: 0.9999996095385763, R2: 0.9999964594868109\n",
      "Gradient Norm: 1041.2239990234375\n",
      "Epoch [33/100], Loss: 50.9877, Gap to Optimality: 50.9877, NMSE: 3.409236114748637e-06, Correlation: 0.999999643645034, R2: 0.9999965907639292\n",
      "Gradient Norm: 1032.5313720703125\n",
      "Epoch [34/100], Loss: 50.9592, Gap to Optimality: 50.9592, NMSE: 3.3211615573236486e-06, Correlation: 0.9999996725556336, R2: 0.9999966788384429\n",
      "Gradient Norm: 1025.6551513671875\n",
      "Epoch [35/100], Loss: 50.9376, Gap to Optimality: 50.9376, NMSE: 3.254300736443838e-06, Correlation: 0.9999996936902222, R2: 0.9999967456993157\n",
      "Gradient Norm: 1019.3129272460938\n",
      "Epoch [36/100], Loss: 50.9188, Gap to Optimality: 50.9188, NMSE: 3.1959025363903493e-06, Correlation: 0.9999997107195402, R2: 0.9999968040977001\n",
      "Gradient Norm: 1017.8977661132812\n",
      "Epoch [37/100], Loss: 50.9089, Gap to Optimality: 50.9089, NMSE: 3.165263024129672e-06, Correlation: 0.9999997246008362, R2: 0.9999968347371887\n",
      "Gradient Norm: 1017.4022827148438\n",
      "Epoch [38/100], Loss: 50.9019, Gap to Optimality: 50.9019, NMSE: 3.1436170502274763e-06, Correlation: 0.9999997356487518, R2: 0.9999968563829461\n",
      "Gradient Norm: 1017.6630859375\n",
      "Epoch [39/100], Loss: 50.8996, Gap to Optimality: 50.8996, NMSE: 3.136935220027226e-06, Correlation: 0.9999997406974753, R2: 0.9999968630649508\n",
      "Gradient Norm: 1016.6986694335938\n",
      "Epoch [40/100], Loss: 50.8933, Gap to Optimality: 50.8933, NMSE: 3.1172273793345084e-06, Correlation: 0.9999997486546633, R2: 0.9999968827727748\n",
      "Gradient Norm: 1019.8910522460938\n",
      "Epoch [41/100], Loss: 50.8951, Gap to Optimality: 50.8951, NMSE: 3.1232057153829373e-06, Correlation: 0.9999997537774497, R2: 0.9999968767942524\n",
      "Gradient Norm: 1018.2056274414062\n",
      "Epoch [42/100], Loss: 50.8909, Gap to Optimality: 50.8909, NMSE: 3.1098961699171923e-06, Correlation: 0.9999997569724579, R2: 0.9999968901039391\n",
      "Gradient Norm: 1016.8521728515625\n",
      "Epoch [43/100], Loss: 50.8865, Gap to Optimality: 50.8865, NMSE: 3.095882220804924e-06, Correlation: 0.9999997609814304, R2: 0.9999969041180503\n",
      "Gradient Norm: 1012.3325805664062\n",
      "Epoch [44/100], Loss: 50.8780, Gap to Optimality: 50.8780, NMSE: 3.0690500807395438e-06, Correlation: 0.9999997639973748, R2: 0.9999969309498703\n",
      "Gradient Norm: 1015.501708984375\n",
      "Epoch [45/100], Loss: 50.8821, Gap to Optimality: 50.8821, NMSE: 3.0819726362096844e-06, Correlation: 0.9999997650088704, R2: 0.9999969180274517\n",
      "Gradient Norm: 1019.25927734375\n",
      "Epoch [46/100], Loss: 50.8854, Gap to Optimality: 50.8854, NMSE: 3.0925839382689446e-06, Correlation: 0.9999997684669248, R2: 0.9999969074158904\n",
      "Gradient Norm: 1016.358642578125\n",
      "Epoch [47/100], Loss: 50.8804, Gap to Optimality: 50.8804, NMSE: 3.0768130727665266e-06, Correlation: 0.9999997696851449, R2: 0.9999969231870155\n",
      "Gradient Norm: 1017.0261840820312\n",
      "Epoch [48/100], Loss: 50.8810, Gap to Optimality: 50.8810, NMSE: 3.07871550830896e-06, Correlation: 0.999999770324575, R2: 0.9999969212846773\n",
      "Gradient Norm: 1012.976318359375\n",
      "Epoch [49/100], Loss: 50.8761, Gap to Optimality: 50.8761, NMSE: 3.063025133087649e-06, Correlation: 0.9999997686175851, R2: 0.9999969369746436\n",
      "Gradient Norm: 1010.9783325195312\n",
      "Epoch [50/100], Loss: 50.8730, Gap to Optimality: 50.8730, NMSE: 3.0531302854797104e-06, Correlation: 0.9999997690594475, R2: 0.9999969468700162\n",
      "Gradient Norm: 1012.1343994140625\n",
      "Epoch [51/100], Loss: 50.8743, Gap to Optimality: 50.8743, NMSE: 3.0572748528356897e-06, Correlation: 0.9999997696872068, R2: 0.9999969427252462\n",
      "Gradient Norm: 1013.5081176757812\n",
      "Epoch [52/100], Loss: 50.8752, Gap to Optimality: 50.8752, NMSE: 3.0601352136727655e-06, Correlation: 0.9999997712611421, R2: 0.9999969398647287\n",
      "Gradient Norm: 1014.0601196289062\n",
      "Epoch [53/100], Loss: 50.8757, Gap to Optimality: 50.8757, NMSE: 3.0618625714851078e-06, Correlation: 0.9999997718613185, R2: 0.9999969381376065\n",
      "Gradient Norm: 1012.0916137695312\n",
      "Epoch [54/100], Loss: 50.8723, Gap to Optimality: 50.8723, NMSE: 3.051040721402387e-06, Correlation: 0.9999997726479791, R2: 0.9999969489588796\n",
      "Gradient Norm: 1001.464599609375\n",
      "Epoch [55/100], Loss: 50.8598, Gap to Optimality: 50.8598, NMSE: 3.011055696333642e-06, Correlation: 0.9999997679635573, R2: 0.9999969889442071\n",
      "Gradient Norm: 1001.9930419921875\n",
      "Epoch [56/100], Loss: 50.8587, Gap to Optimality: 50.8587, NMSE: 3.007629402418388e-06, Correlation: 0.9999997710035078, R2: 0.9999969923703835\n",
      "Gradient Norm: 1008.157470703125\n",
      "Epoch [57/100], Loss: 50.8666, Gap to Optimality: 50.8666, NMSE: 3.0330022582347738e-06, Correlation: 0.9999997725246899, R2: 0.9999969669975359\n",
      "Gradient Norm: 1017.949951171875\n",
      "Epoch [58/100], Loss: 50.8809, Gap to Optimality: 50.8809, NMSE: 3.078231657127617e-06, Correlation: 0.9999997727803716, R2: 0.9999969217682626\n",
      "Gradient Norm: 1018.49560546875\n",
      "Epoch [59/100], Loss: 50.8812, Gap to Optimality: 50.8812, NMSE: 3.079355565205333e-06, Correlation: 0.9999997735160037, R2: 0.9999969206446214\n",
      "Gradient Norm: 1017.0430908203125\n",
      "Epoch [60/100], Loss: 50.8781, Gap to Optimality: 50.8781, NMSE: 3.0694136512465775e-06, Correlation: 0.9999997749979316, R2: 0.9999969305862838\n",
      "Gradient Norm: 1024.7650146484375\n",
      "Epoch [61/100], Loss: 50.8883, Gap to Optimality: 50.8883, NMSE: 3.101909214819898e-06, Correlation: 0.9999997767975767, R2: 0.9999968980906457\n",
      "Gradient Norm: 1020.3486938476562\n",
      "Epoch [62/100], Loss: 50.8823, Gap to Optimality: 50.8823, NMSE: 3.0826561214780668e-06, Correlation: 0.9999997760599513, R2: 0.9999969173438545\n",
      "Gradient Norm: 1015.6943359375\n",
      "Epoch [63/100], Loss: 50.8775, Gap to Optimality: 50.8775, NMSE: 3.067551688218373e-06, Correlation: 0.999999772849909, R2: 0.9999969324486407\n",
      "Gradient Norm: 1018.6781616210938\n",
      "Epoch [64/100], Loss: 50.8815, Gap to Optimality: 50.8815, NMSE: 3.0802957553532906e-06, Correlation: 0.999999773334471, R2: 0.999996919704196\n",
      "Gradient Norm: 1018.092529296875\n",
      "Epoch [65/100], Loss: 50.8807, Gap to Optimality: 50.8807, NMSE: 3.07761774820392e-06, Correlation: 0.999999773458039, R2: 0.9999969223821519\n",
      "Gradient Norm: 1018.5957641601562\n",
      "Epoch [66/100], Loss: 50.8806, Gap to Optimality: 50.8806, NMSE: 3.0774920105614e-06, Correlation: 0.9999997745658609, R2: 0.9999969225079344\n",
      "Gradient Norm: 1007.6707763671875\n",
      "Epoch [67/100], Loss: 50.8670, Gap to Optimality: 50.8670, NMSE: 3.0341964247782016e-06, Correlation: 0.999999770873245, R2: 0.9999969658033866\n",
      "Gradient Norm: 1012.6198120117188\n",
      "Epoch [68/100], Loss: 50.8734, Gap to Optimality: 50.8734, NMSE: 3.0546507332473993e-06, Correlation: 0.9999997721734254, R2: 0.9999969453493601\n",
      "Gradient Norm: 1019.567626953125\n",
      "Epoch [69/100], Loss: 50.8828, Gap to Optimality: 50.8828, NMSE: 3.084469199166051e-06, Correlation: 0.9999997734870201, R2: 0.9999969155309685\n",
      "Gradient Norm: 1021.1019897460938\n",
      "Epoch [70/100], Loss: 50.8847, Gap to Optimality: 50.8847, NMSE: 3.090397740379558e-06, Correlation: 0.9999997741183283, R2: 0.9999969096021601\n",
      "Gradient Norm: 1017.2953491210938\n",
      "Epoch [71/100], Loss: 50.8786, Gap to Optimality: 50.8786, NMSE: 3.07109758068691e-06, Correlation: 0.9999997747700832, R2: 0.9999969289024591\n",
      "Gradient Norm: 1015.6863403320312\n",
      "Epoch [72/100], Loss: 50.8755, Gap to Optimality: 50.8755, NMSE: 3.061130655623856e-06, Correlation: 0.9999997758100058, R2: 0.9999969388695122\n",
      "Gradient Norm: 1011.3928833007812\n",
      "Epoch [73/100], Loss: 50.8707, Gap to Optimality: 50.8707, NMSE: 3.0459455047093797e-06, Correlation: 0.9999997736125383, R2: 0.9999969540543622\n",
      "Gradient Norm: 1010.8030395507812\n",
      "Epoch [74/100], Loss: 50.8706, Gap to Optimality: 50.8706, NMSE: 3.045452558581019e-06, Correlation: 0.999999772486696, R2: 0.9999969545476464\n",
      "Gradient Norm: 1006.4929809570312\n",
      "Epoch [75/100], Loss: 50.8641, Gap to Optimality: 50.8641, NMSE: 3.0248090752138523e-06, Correlation: 0.9999997727177907, R2: 0.9999969751908436\n",
      "Gradient Norm: 1006.9908447265625\n",
      "Epoch [76/100], Loss: 50.8655, Gap to Optimality: 50.8655, NMSE: 3.029324716408155e-06, Correlation: 0.9999997716648354, R2: 0.9999969706750884\n",
      "Gradient Norm: 1019.6079711914062\n",
      "Epoch [77/100], Loss: 50.8816, Gap to Optimality: 50.8816, NMSE: 3.0806306767772185e-06, Correlation: 0.9999997751786105, R2: 0.9999969193693647\n",
      "Gradient Norm: 1014.134033203125\n",
      "Epoch [78/100], Loss: 50.8756, Gap to Optimality: 50.8756, NMSE: 3.061412826355081e-06, Correlation: 0.9999997722169001, R2: 0.9999969385871712\n",
      "Gradient Norm: 1018.102294921875\n",
      "Epoch [79/100], Loss: 50.8801, Gap to Optimality: 50.8801, NMSE: 3.0757023523619864e-06, Correlation: 0.9999997742842544, R2: 0.9999969242978988\n",
      "Gradient Norm: 1013.9423217773438\n",
      "Epoch [80/100], Loss: 50.8754, Gap to Optimality: 50.8754, NMSE: 3.0607338885602076e-06, Correlation: 0.9999997721517314, R2: 0.9999969392661898\n",
      "Gradient Norm: 1015.4973754882812\n",
      "Epoch [81/100], Loss: 50.8776, Gap to Optimality: 50.8776, NMSE: 3.067783154619974e-06, Correlation: 0.9999997722629516, R2: 0.9999969322168922\n",
      "Gradient Norm: 1015.763916015625\n",
      "Epoch [82/100], Loss: 50.8782, Gap to Optimality: 50.8782, NMSE: 3.0699347917106934e-06, Correlation: 0.999999771837368, R2: 0.9999969300651388\n",
      "Gradient Norm: 1014.2120971679688\n",
      "Epoch [83/100], Loss: 50.8761, Gap to Optimality: 50.8761, NMSE: 3.063069243580685e-06, Correlation: 0.9999997716258429, R2: 0.9999969369304438\n",
      "Gradient Norm: 1013.7515258789062\n",
      "Epoch [84/100], Loss: 50.8750, Gap to Optimality: 50.8750, NMSE: 3.059751634282293e-06, Correlation: 0.9999997722241427, R2: 0.9999969402481732\n",
      "Gradient Norm: 1015.7481079101562\n",
      "Epoch [85/100], Loss: 50.8772, Gap to Optimality: 50.8772, NMSE: 3.066744284296874e-06, Correlation: 0.999999773300297, R2: 0.9999969332556472\n",
      "Gradient Norm: 1025.5296630859375\n",
      "Epoch [86/100], Loss: 50.8898, Gap to Optimality: 50.8898, NMSE: 3.1067377221916104e-06, Correlation: 0.999999776133677, R2: 0.999996893262136\n",
      "Gradient Norm: 1015.2843017578125\n",
      "Epoch [87/100], Loss: 50.8767, Gap to Optimality: 50.8767, NMSE: 3.0650753615191206e-06, Correlation: 0.9999997731790755, R2: 0.9999969349245313\n",
      "Gradient Norm: 1013.6922607421875\n",
      "Epoch [88/100], Loss: 50.8752, Gap to Optimality: 50.8752, NMSE: 3.060084736716817e-06, Correlation: 0.9999997719184217, R2: 0.999996939915291\n",
      "Gradient Norm: 1013.2416381835938\n",
      "Epoch [89/100], Loss: 50.8741, Gap to Optimality: 50.8741, NMSE: 3.0567136946046958e-06, Correlation: 0.9999997725631731, R2: 0.9999969432863189\n",
      "Gradient Norm: 1015.7511596679688\n",
      "Epoch [90/100], Loss: 50.8772, Gap to Optimality: 50.8772, NMSE: 3.066635144932661e-06, Correlation: 0.9999997734074891, R2: 0.9999969333648177\n",
      "Gradient Norm: 1013.0724487304688\n",
      "Epoch [91/100], Loss: 50.8737, Gap to Optimality: 50.8737, NMSE: 3.0555884222849272e-06, Correlation: 0.9999997727035109, R2: 0.9999969444116096\n",
      "Gradient Norm: 1017.806640625\n",
      "Epoch [92/100], Loss: 50.8807, Gap to Optimality: 50.8807, NMSE: 3.0777637221035548e-06, Correlation: 0.9999997727257844, R2: 0.9999969222364093\n",
      "Gradient Norm: 1015.6901245117188\n",
      "Epoch [93/100], Loss: 50.8780, Gap to Optimality: 50.8780, NMSE: 3.0690114272147184e-06, Correlation: 0.9999997721105047, R2: 0.9999969309885038\n",
      "Gradient Norm: 1014.0379638671875\n",
      "Epoch [94/100], Loss: 50.8757, Gap to Optimality: 50.8757, NMSE: 3.061857569264248e-06, Correlation: 0.9999997718708846, R2: 0.9999969381427081\n",
      "Gradient Norm: 1011.0884399414062\n",
      "Epoch [95/100], Loss: 50.8717, Gap to Optimality: 50.8717, NMSE: 3.0490170956909424e-06, Correlation: 0.999999771415164, R2: 0.9999969509829155\n",
      "Gradient Norm: 1013.9824829101562\n",
      "Epoch [96/100], Loss: 50.8759, Gap to Optimality: 50.8759, NMSE: 3.0623996281065047e-06, Correlation: 0.9999997713888208, R2: 0.9999969376006252\n",
      "Gradient Norm: 1015.0396118164062\n",
      "Epoch [97/100], Loss: 50.8769, Gap to Optimality: 50.8769, NMSE: 3.0657338356832042e-06, Correlation: 0.9999997721796418, R2: 0.999996934266487\n",
      "Gradient Norm: 1011.8038940429688\n",
      "Epoch [98/100], Loss: 50.8729, Gap to Optimality: 50.8729, NMSE: 3.0531004995282274e-06, Correlation: 0.9999997710410186, R2: 0.9999969468995606\n",
      "Gradient Norm: 1008.5797729492188\n",
      "Epoch [99/100], Loss: 50.8693, Gap to Optimality: 50.8693, NMSE: 3.0416163099289406e-06, Correlation: 0.9999997692081707, R2: 0.999996958383786\n",
      "Gradient Norm: 1010.9061889648438\n",
      "Epoch [100/100], Loss: 50.8723, Gap to Optimality: 50.8723, NMSE: 3.051092562600388e-06, Correlation: 0.9999997699514184, R2: 0.9999969489072302\n",
      "Final gradient of the subproblem Core : 1010.9061889648438\n",
      "(32, 32)\n",
      "---------------------------Testing with Best Lambda------------------------------\n",
      "Y Test Predicted:  [ 67.53853502  64.20717224 100.8834273  106.76210225  66.56476499\n",
      "  96.79099956 112.72305875  87.07489369  58.93520323  60.77974494\n",
      "  65.79054058  64.1869706   82.79226974  68.58277406 142.37635943\n",
      "  97.98607471  98.31390718  55.46827419  96.73277032  87.27160898\n",
      "  48.05598235  91.13653357  73.60421991  77.47415855  47.03419407\n",
      "  79.38520152  84.71286517  45.53590536  87.56399626  73.48840086\n",
      "  86.91005077  88.25669284 124.59402845  81.43825276  87.544606\n",
      "  82.76598113  98.71633541  69.08145338  80.61150805  93.0644971\n",
      "  68.43873146  87.28068722  62.06392357  57.39445426  56.06231082\n",
      "  89.30479225 110.4928332  134.53278301  43.31807201  74.49565568\n",
      "  96.21051424  87.90107999  79.22691853  90.71814776 125.82276422\n",
      " 160.24652982  60.54957827  86.89520423  89.56402537  85.50654269\n",
      "  79.97187144  95.5332856   57.1378502   50.42355236  70.88651068\n",
      " 131.16478161  94.48837301  83.17062947  62.95122058  84.07078967\n",
      "  76.26680027  96.45277155  77.0977972   65.79581375  71.80820758\n",
      "  65.18816167  61.05324855  68.75946339  66.44818356 104.92253756\n",
      "  79.76039461  91.81167799 110.40929919  62.26642403  57.64421844\n",
      "  98.11216775  67.8600205   72.13914595 127.7951123   63.34685881\n",
      " 103.66751399 149.96002774  67.44070827  92.3879271  123.61940279\n",
      "  46.97086607  64.67980214 108.17195667 101.1085379  113.34379341]\n",
      "Y Test Actual:  [ 67.54932471  64.19367989 100.89907103 106.80033802  66.5296898\n",
      "  96.80085769 112.75716496  87.08379176  58.86645104  60.75871896\n",
      "  65.76651181  64.14603363  82.80477245  68.55912818 142.45923938\n",
      "  98.00053671  98.36489423  55.40988723  96.73410604  87.25275894\n",
      "  47.97392502  91.14302569  73.57729565  77.46144622  46.97758628\n",
      "  79.38435102  84.7040345   45.4879342   87.55672081  73.49886432\n",
      "  86.90598491  88.25175908 124.63253198  81.43265772  87.55287141\n",
      "  82.8080788   98.72768177  69.0468243   80.62856336  93.08545506\n",
      "  68.40807412  87.26847367  62.04717769  57.33990441  56.01334774\n",
      "  89.31934273 110.55219975 134.61307846  43.26935217  74.47617341\n",
      "  96.21843559  87.89937403  79.21862917  90.71786386 125.8824493\n",
      " 160.38439066  60.50387741  86.87508783  89.5739495   85.50880057\n",
      "  79.97185552  95.54965657  57.09355445  50.35928763  70.86185745\n",
      " 131.24078613  94.50837386  83.15863626  62.93422433  84.09831139\n",
      "  76.26182003  96.48126457  77.07300387  65.77838739  71.76861932\n",
      "  65.14937109  61.0440168   68.74400314  66.41967439 104.95340089\n",
      "  79.75508697  91.80406117 110.44770514  62.24491669  57.60974417\n",
      "  98.13365195  67.82489044  72.14320497 127.85744229  63.30804812\n",
      " 103.70015697 150.07489827  67.42743244  92.3906977  123.66379363\n",
      "  46.92967844  64.66629945 108.22935093 101.14612067 113.38031778]\n",
      "Error Report on Testing _ With best Lambda\n",
      "SGD_Alpha chosen for model:  50\n",
      "SGD_Test Normalized Estimation Error:  2.944392381879712e-06\n",
      "SGD_Test NMSE Loss:  1.920941174484772e-07\n",
      "SGD_Test R2 Loss:  0.9999973367328364\n",
      "SGD_Test Correlation:  0.9999998167971883\n",
      "Objective Function Values 22200.00870466773\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final gradient: 1010.9061889648438\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learning_rates = [0.0000007]\n",
    "\n",
    "\n",
    "for idx,lr in enumerate (learning_rates):   \n",
    "      \n",
    "      print('')\n",
    "      print('-------------------------------------Learning Rate',lr,'-----------------------------------------')\n",
    "      lsr_tensor_SGD = copy.deepcopy(lsr_tensor_initializer)\n",
    "      learning_rate = lr\n",
    "      epochs = 100\n",
    "      batch_size = 64\n",
    "\n",
    "      momentum = 0\n",
    "      nesterov = False\n",
    "      decay_factor = 0\n",
    "      hypers = {'max_iter': 1, 'threshold': 1e-4, 'ranks': tuple(tensor_mode_ranks), 'separation_rank': separation_rank,'learning_rate':learning_rate,'epochs':epochs,'batch_size': batch_size, 'momentum':momentum, 'nesterov': nesterov, 'decay_factor': decay_factor}\n",
    "\n",
    "      normalized_estimation_error_SGD, test_nmse_loss_SGD, test_R2_loss_SGD, test_correlation_SGD, objective_function_values_SGD,gradient_values_SGD,iterate_differences_SGD,epoch_level_gradients_SGD,epoch_level_function,tensor_iteration_SGD,factor_core_iterate_SGD = train_test_sgd(X_train, Y_train, X_test, Y_test, lambda1, hypers, Y_train_mean,lsr_tensor_SGD,B_tensored,intercept= False)\n",
    "\n",
    "    \n",
    "      #Get current time and store in variable\n",
    "      formatted_time = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "      max_iter = hypers['max_iter']\n",
    "      pkl_file = f\"/Users/lakrama/Neuro Project Codes/LSR-Tensor-Ridge-Regression_All_Data/Platforms for Experments/Data/Experiments_Isolating_Sub_Problems/Final lr search/STAGE 1/SGD_learning_rate_{learning_rate}_decay_{decay_factor}_intercept5_,ExecutionTime{formatted_time}, n_train_{n_train},n_test_{n_test}, tensor_dimensions:{tensor_dimensions}, tensor_mode_= ranks:{tensor_mode_ranks}, separation_rank:{separation_rank}, max_iter={max_iter}.pkl\"\n",
    "\n",
    "      with open(pkl_file, \"wb\") as file:\n",
    "        dill.dump((lsr_tensor_SGD,lambda1, normalized_estimation_error_SGD, test_nmse_loss_SGD, test_R2_loss_SGD, test_correlation_SGD, objective_function_values_SGD,gradient_values_SGD, iterate_differences_SGD,epoch_level_gradients_SGD,epoch_level_function,tensor_iteration_SGD,factor_core_iterate_SGD), file)\n",
    "\n",
    "\n",
    "      print(\"Error Report on Testing _ With best Lambda\")\n",
    "      print(\"SGD_Alpha chosen for model: \", lambda1)\n",
    "      print(\"SGD_Test Normalized Estimation Error: \", normalized_estimation_error_SGD)\n",
    "      print(\"SGD_Test NMSE Loss: \", test_nmse_loss_SGD)\n",
    "      print(\"SGD_Test R2 Loss: \", test_R2_loss_SGD)\n",
    "      print(\"SGD_Test Correlation: \", test_correlation_SGD)\n",
    "      print(\"Objective Function Values\", objective_function_values_SGD[0,1,2])\n",
    "\n",
    "      # Looking at the  variation of function values within a BCD iteration\n",
    "\n",
    "      import matplotlib.pyplot as plt\n",
    "\n",
    "      # Create a line plot\n",
    "      plt.plot(epoch_level_gradients_SGD[0,1,2,:], marker='o')\n",
    "\n",
    "      # Add title and labels\n",
    "      plt.title('Gradient Value')\n",
    "      plt.xlabel('X-axis')\n",
    "      plt.yscale('log')\n",
    "      plt.ylabel('Y-axis')\n",
    "\n",
    "      plt.show()\n",
    "\n",
    "      print(f'final gradient: {epoch_level_gradients_SGD[0,1,2,-1]}')\n",
    "      # fucnion value\n",
    "\n",
    "      # Create a line plot\n",
    "      plt.plot(epoch_level_function[0,1,2,:], marker='o')\n",
    "\n",
    "      # Add title and labels\n",
    "      plt.title('Function Value')\n",
    "      plt.xlabel('X-axis')\n",
    "      plt.yscale('log')\n",
    "      plt.ylabel('Y-axis')\n",
    "\n",
    "      plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchtensor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
