{"cells":[{"cell_type":"markdown","metadata":{"id":"iRQhcYsPikCF"},"source":["# HCP Vector Pipeline"]},{"cell_type":"markdown","metadata":{},"source":["## Importing Native Files"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import sys\n","sys.path.append('../Code Files')\n","sys.path.append('../Data')"]},{"cell_type":"markdown","metadata":{"id":"4SSG97QjikCI"},"source":["## Import Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":6796,"status":"ok","timestamp":1711574615316,"user":{"displayName":"Lakshitha Ramanayake Mudiyanselage","userId":"09058794058790427589"},"user_tz":240},"id":"16gRRjI8ikCI"},"outputs":[],"source":["import datetime\n","import dill\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from optimization import SGD, GD\n","import pandas as pd\n","import re\n","import scipy\n","from sklearn.metrics import r2_score\n","import torch\n","import seaborn as sns"]},{"cell_type":"markdown","metadata":{},"source":["## Prepare HCP Data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#Load fMRI Resting State Data\n","with open(\"../Data/fmri_rs.npy\", \"rb\") as f:\n","  fmri_rs = np.load(f)\n","\n","#Print Shape of Data for Sanity\n","print(fmri_rs.shape)\n","\n","#Take the Transpose so that each Sample is a Row\n","fmri_rs = fmri_rs.T\n","\n","#Get Split to divide into train + test\n","mat_file = scipy.io.loadmat(\"../Data/MMP_HCP_60_splits.mat\")\n","seed_1 = mat_file['folds']['seed_1']\n","subject_lists = seed_1[0, 0]['sub_fold'][0, 0]['subject_list']\n","test_subjects = [int(item[0]) for item in subject_lists[0,0].flatten()]\n","\n","#Get HCP test subjects\n","HCP_753_Subjects = []\n","with open('../Data/MMP_HCP_753_subs.txt', 'r') as file:\n","    HCP_753_Subjects = [int(re.sub('\\n', '', line)) for line in file.readlines()]\n","\n","#Put the HCP test subjects into a dataframe\n","df = pd.read_csv(\"../Data/MMP_HCP_componentscores.csv\")\n","df['Subject'] = pd.to_numeric(df['Subject'], errors='coerce')\n","df = df[df['Subject'].isin(HCP_753_Subjects)].reset_index(drop = True)\n","\n","#Split all our data into a Train and Test Set\n","df_train, df_test = df[~df['Subject'].isin(test_subjects)], df[df['Subject'].isin(test_subjects)]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#Create Train and Test Arrays corresponding to Training and Test Subjects\n","train_subjects = df_train.index.to_list()\n","test_subjects = df_test.index.to_list()\n","\n","#Reshape Labels into Column Vectors\n","X_train, Y_train = fmri_rs[train_subjects], df_train[\"varimax_cog\"].to_numpy().reshape((-1, 1))\n","X_test, Y_test = fmri_rs[test_subjects], df_test[\"varimax_cog\"].to_numpy().reshape((-1, 1))"]},{"cell_type":"markdown","metadata":{},"source":["## Preprocessing Step"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#Row Wise Normalization of Samples\n","def normalize_rows(matrix: np.ndarray):\n","    \"\"\"\n","    Normalize each row of the given matrix by the norm of the row.\n","    \n","    Parameters:\n","    matrix (numpy.ndarray): The input matrix to be normalized.\n","    \n","    Returns:\n","    numpy.ndarray: The row-normalized matrix.\n","    \"\"\"\n","    # Calculate the L2 norm for each row. Adding a small epsilon to avoid division by zero.\n","    row_norms = np.linalg.norm(matrix, axis=1, keepdims=True)\n","    epsilon = 1e-10  # Small value to prevent division by zero\n","    row_norms[row_norms == 0] = epsilon\n","    \n","    # Normalize each row by its norm\n","    normalized_matrix = matrix / row_norms\n","    return normalized_matrix\n","\n","#Preprocess Data\n","X_train = normalize_rows(X_train)\n","X_test = normalize_rows(X_test)"]},{"cell_type":"markdown","metadata":{},"source":["## Investigate Data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Create the histogram\n","sparse_counts = np.sum((X_train <= 1e-6), axis = 1)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sns.histplot(sparse_counts, bins=30)\n","\n","# Add titles and labels\n","plt.title('Histogram of Data')\n","plt.xlabel('Number of Values in Sample <= 1e-6')\n","plt.ylabel('Frequency')\n","\n","# Show the plot\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["## Global Hyperparameters"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["use_bias = False\n","lamb = 0.19"]},{"cell_type":"markdown","metadata":{},"source":["## Closed Form Solver"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.linear_model import Ridge\n","\n","#Create Ridge Model with specific alpha value and fit to (X_train, Y_train)\n","model = Ridge(alpha = lamb, solver = 'svd', fit_intercept = use_bias)\n","model.fit(X_train, Y_train)\n","\n","#Get Y_train predicted using model weights\n","Y_train_pred = model.predict(X_train).reshape((-1, 1))\n","weights = model.coef_.reshape((-1, 1))\n","W_true = weights\n","\n","#Calculate Optimal Value\n","p_star = (np.linalg.norm(Y_train - Y_train_pred) ** 2) + (lamb * (np.linalg.norm(weights) ** 2))\n","print(f\"Value of p^*: {p_star}\")\n","\n","#Compute Training Error Metrics\n","train_nmse_loss = np.sum(np.square((Y_train_pred - Y_train))) / np.sum(np.square(Y_train))\n","train_correlation = np.corrcoef(Y_train_pred.flatten(), Y_train.flatten())[0, 1]\n","train_R2_score = r2_score(Y_train, Y_train_pred)\n","error_metrics = [train_nmse_loss, train_correlation, train_R2_score]\n","\n","print('-----------------------------Training Error Metrics---------------------------------')\n","print(f\"NMSE: {train_nmse_loss}\")\n","print(f\"Correlation: {train_correlation}\")\n","print(f\"R2_Score: {train_R2_score}\")\n","\n","Y_test_predicted =  model.predict(X_test).reshape((-1, 1))\n","\n","#Compute Test Error Metrics\n","test_nmse_loss = np.sum(np.square((Y_test_predicted - Y_test))) / np.sum(np.square(Y_test))\n","test_correlation = np.corrcoef(Y_test_predicted.flatten(), Y_test.flatten())[0, 1]\n","test_R2_score = r2_score(Y_test, Y_test_predicted)\n","error_metrics = [test_nmse_loss,test_correlation,test_R2_score]\n","\n","print('-----------------------------Testing Error Metrics---------------------------------')\n","print(f\"NMSE: {test_nmse_loss}\")\n","print(f\"Correlation: {test_correlation}\")\n","print(f\"R2_Score: {test_R2_score}\")"]},{"cell_type":"markdown","metadata":{"id":"7hg9t1vkikCJ"},"source":["## Gradient Descent[Exact Line Search] Solver"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# max_epochs = 1000\n","# optimizer_code = 0\n","\n","# hypers = {'lambda': lamb, 'epochs': max_epochs, 'bias': use_bias}\n","# W_estimated, b_estimated, losses, gap_to_optimality, nee_values, nmse_values, corr_values, R2_values = GD(X = X_train, Y = Y_train, cost_function_code = 1, hypers = hypers, p_star = p_star, W_true = W_true)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# plt.plot(range(1, len(gap_to_optimality)+1), gap_to_optimality)\n","# plt.title('Gap to Optimality')\n","# plt.xlabel('Epoch')\n","# plt.ylabel('Gap to Optimality')\n","# plt.grid(True)\n","# plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# plt.plot(range(1, len(losses)+1), losses)\n","# plt.title('Loss')\n","# plt.xlabel('Epoch')\n","# plt.ylabel('Loss')\n","# plt.grid(True)\n","# plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["## Stochastic Gradient Descent Solver"]},{"cell_type":"markdown","metadata":{},"source":["Learning Rate = 1e-3:\n","- Noticeable difference between the Stochastic Gradients and Full Gradient\n","\n","Learning Rate = 1e-4:\n","- Smaller difference"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":447930,"status":"ok","timestamp":1711578161736,"user":{"displayName":"Lakshitha Ramanayake Mudiyanselage","userId":"09058794058790427589"},"user_tz":240},"id":"8mi-tfULikCJ","outputId":"64542465-8c60-47bf-e48b-4dc6e47d20f8"},"outputs":[],"source":["#Cost Function to Use(0 for Least Squares, 1 for Ridge Regression)\n","cost_function_code = 1\n","\n","#Hyperparameters\n","lr = 8e-4\n","epochs = 1000\n","batch_size = 256 #Set 256 as default\n","use_bias = False\n","optimizer_code = 2\n","decay = 0.99 #Set this as default learning rate\n","hypers = {'lambda': lamb, 'lr': lr, 'epochs': epochs, 'batch_size': batch_size, 'bias': use_bias, 'optimizer_code': optimizer_code, 'decay_factor': decay}\n","\n","#Initial Weights\n","W_init = None\n","\n","#Run SGD\n","W_estimated, b_estimated, losses, gap_to_optimality, fixed_point_stochastic_gradients, fixed_point_stochastic_gradient_norms, fixed_point_full_gradients, fixed_point_full_gradient_norms, iterates, iterate_norms, full_gradients, \\\n","    full_gradient_norms, stochastic_gradients, stochastic_gradient_norms, nee_values, nmse_values, corr_values, R2_values, epochs_ran = SGD(X = X_train, Y = Y_train, cost_function_code = cost_function_code, hypers = hypers, optimizer_code = optimizer_code, p_star = p_star, W_true = W_true, W_init = W_init)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#Fixed Point Stochastic Variance\n","fixed_point_stochastic_gradient_norms = fixed_point_stochastic_gradient_norms.reshape(epochs_ran, -1)\n","fixed_point_full_gradient_norms = fixed_point_full_gradient_norms.reshape(epochs_ran, -1)\n","stochastic_variance_per_epoch = np.var(fixed_point_stochastic_gradient_norms - fixed_point_full_gradient_norms, axis = 1)\n","plt.scatter(range(1, len(stochastic_variance_per_epoch)+1), stochastic_variance_per_epoch)\n","plt.title('Variance in Fixed Point Stochastic Gradient Norms')\n","plt.xlabel('Epoch')\n","plt.yscale('log')\n","plt.ylabel('Variance')\n","plt.grid(True)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["### Normalized Fixed Point Stochastic Variance\n","normalized_stochastic_variance_per_epoch = np.var((fixed_point_stochastic_gradient_norms - fixed_point_full_gradient_norms) / fixed_point_full_gradient_norms, axis = 1)\n","plt.scatter(range(1, len(normalized_stochastic_variance_per_epoch)+1), normalized_stochastic_variance_per_epoch)\n","plt.title('Normalized Varianced in Fixed Point Stochastic Gradient Norms')\n","plt.xlabel('Epoch')\n","plt.ylabel('Normalized Variance')\n","plt.grid(True)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#Cost Function Value\n","plt.plot(range(1, len(losses)+1), losses)\n","plt.title('Cost Function Values(i.e. Loss)')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.yscale('log')\n","plt.grid(True)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#Gap to Closed Form Solution\n","plt.plot(range(1, len(gap_to_optimality)+1), gap_to_optimality)\n","plt.title('Gap to Closed Form Solution')\n","plt.xlabel('Epoch')\n","plt.ylabel('Gap')\n","plt.yscale('log')\n","plt.grid(True)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#Plot Full Gradient Norms\n","plt.plot(range(1, len(full_gradient_norms)+1), full_gradient_norms)\n","plt.title('Full Gradient Norms')\n","plt.xlabel('Epoch')\n","plt.ylabel('Norm')\n","plt.yscale('log')\n","plt.grid(True)\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["### Stochastic Gradient Descent vs Full Gradients"]},{"cell_type":"markdown","metadata":{},"source":["#### Stochastic Gradients vs Full Gradients"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["## Least Squares Term: ||XW - Y||_2^2 \n","## Gradient of Least Squares Term: Summation of Gradients of (<xi, w>  - yi)^2\n","\n","## L2 Loss Term: ||W||_2^2 \n","## Gradient of L2 Loss Term: 2w\n","\n","num_batches = int(len(stochastic_gradients) / epochs_ran) #number of gradients within a stochastic group\n","\n","#Store Stuff\n","A = []\n","A_norms = []\n","A_avg = []\n","\n","differences = []\n","\n","#Iterate through Full Gradients\n","for idx in range(len(full_gradients)):\n","    full_gradient = full_gradients[idx] #Get Full Gradient\n","    \n","    #Get Stochastic Gradients\n","    start = idx * num_batches\n","    end = (idx + 1) * num_batches\n","    stochastic_slice = stochastic_gradients[start:end]\n","    \n","    #Compute Sum of Stochastic Gradients\n","    stochastic_sum = 0\n","    for idx2 in range(len(stochastic_slice)):\n","        stochastic_sum += stochastic_slice[idx2]\n","    \n","    expected_stochastic_gradient = stochastic_sum / num_batches #expected value of stochastic gradient. Ideally, we want this expected value to equal to the full gradient\n","    \n","    #Compute Normalized Difference\n","    diff = expected_stochastic_gradient - full_gradient\n","    norm_diff = torch.norm(diff).item()\n","    differences.append(norm_diff)\n","    \n","    \n","    normalized_diff = stochastic_sum - full_gradient\n","    A.append(normalized_diff)    \n","    A_norms.append(torch.norm(normalized_diff).item())\n","    A_avg.append(torch.mean(normalized_diff))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.plot(range(1, len(differences)+1), differences)\n","plt.title('Difference between Expected Value of Stochastic Gradient and Full Gradient')\n","plt.xlabel('Epoch')\n","plt.ylabel('Difference[Norm]')\n","plt.yscale('log')\n","plt.grid(True)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.plot(range(1, len(A_norms)+1), A_norms)\n","plt.title('Difference between Sum of Stochastic Gradients and Full Gradient')\n","plt.xlabel('Epoch')\n","plt.ylabel('Difference [Norm]')\n","plt.yscale('log')\n","plt.grid(True)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.plot(range(1, len(A_avg)+1), A_avg)\n","plt.title('Average Value[Differential]')\n","plt.xlabel('Epoch')\n","plt.ylabel('Average')\n","plt.yscale('log')\n","plt.grid(True)\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["#### Fixed Point Stochastic Gradients vs Fixed Point Full Gradients"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["## Least Squares Term: ||XW - Y||_2^2 \n","## Gradient of Least Squares Term: Summation of Gradients of (<xi, w>  - yi)^2\n","\n","## L2 Loss Term: ||W||_2^2 \n","## Gradient of L2 Loss Term: 2w\n","\n","num_batches = int(len(fixed_point_stochastic_gradients) / epochs_ran) #number of gradients within a stochastic group\n","\n","#Store Stuff\n","A = []\n","A_norms = []\n","A_avg = []\n","\n","differences = []\n","\n","#Iterate through Full Gradients\n","for idx in range(len(fixed_point_full_gradients)):\n","    fixed_point_full_gradient = fixed_point_full_gradients[idx] #Get Full Gradient\n","    \n","    #Get Stochastic Gradients\n","    start = idx * num_batches\n","    end = (idx + 1) * num_batches\n","    fixed_point_stochastic_slice = fixed_point_stochastic_gradients[start:end]\n","    \n","    #Compute Sum of Stochastic Gradients\n","    fixed_point_stochastic_sum = 0\n","    for idx2 in range(len(fixed_point_stochastic_slice)):\n","        fixed_point_stochastic_sum += fixed_point_stochastic_slice[idx2]\n","    \n","    expected_fixed_point_stochastic_gradient = fixed_point_stochastic_sum / num_batches #expected value of stochastic gradient. Ideally, we want this expected value to equal to the full gradient\n","    \n","    #Compute Normalized Difference\n","    diff = expected_fixed_point_stochastic_gradient - fixed_point_full_gradient\n","    norm_diff = torch.norm(diff).item()\n","    differences.append(norm_diff)\n","    \n","    \n","    normalized_diff = fixed_point_stochastic_sum - fixed_point_full_gradient\n","    A.append(normalized_diff)    \n","    A_norms.append(torch.norm(normalized_diff).item())\n","    A_avg.append(torch.mean(normalized_diff))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.plot(range(1, len(differences)+1), differences)\n","plt.title('Difference between Expected Value of Fixed Point Stochastic Gradient and Fixed Point Full Gradient')\n","plt.xlabel('Epoch')\n","plt.ylabel('Difference[Norm]')\n","plt.yscale('log')\n","plt.grid(True)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.plot(range(1, len(A_norms)+1), A_norms)\n","plt.title('Difference between Sum of Fixed Point Stochastic Gradients and Fixed Point Full Gradient')\n","plt.xlabel('Epoch')\n","plt.ylabel('Difference [Norm]')\n","plt.yscale('log')\n","plt.grid(True)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.plot(range(1, len(A_avg)+1), A_avg)\n","plt.title('Average Value[Differential]')\n","plt.xlabel('Epoch')\n","plt.ylabel('Average')\n","plt.yscale('log')\n","plt.grid(True)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["## Compute the Sum of the Gradients of the L2 Norms during each epoch(i.e. across all stochastic updates within the epoch)\n","iterate_norm_tensor_sums = []\n","for epoch in range(epochs_ran):\n","    start = epoch * (num_batches)\n","    end = (epoch + 1) * (num_batches)\n","    iterate_slice = iterates[start: end]\n","    \n","    iterate_sum = 0\n","    \n","    for idx2 in range(len(iterate_slice)):\n","        iterate_sum += iterate_slice[idx2]\n","    \n","    iterate_norm_tensor_sums.append(torch.norm(iterate_sum))\n","\n","iterate_norm_tensor_sums"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.plot(range(1, len(iterate_norm_tensor_sums)+1), iterate_norm_tensor_sums)\n","plt.title('Iterate Sum Norms')\n","plt.xlabel('Epoch')\n","plt.yscale('log')\n","plt.grid(True)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#Testing\n","weights = W_estimated\n","use_bias = b_estimated\n","Y_test_predicted = X_test @ W_estimated + use_bias\n","\n","#Compute NEE, NMSE, Correlation, and R^2 Score\n","test_normalized_estimation_error = ((np.linalg.norm(W_estimated - W_true)) ** 2) /  ((np.linalg.norm(W_true)) ** 2)\n","test_nmse_loss = np.sum(np.square((Y_test_predicted - Y_test))) / np.sum(np.square(Y_test))\n","test_correlation = np.corrcoef(Y_test_predicted.flatten(), Y_test.flatten())[0, 1]\n","test_R2_score = r2_score(Y_test, Y_test_predicted)\n","\n","error_metrics = [test_normalized_estimation_error,test_nmse_loss,test_correlation,test_R2_score]\n","\n","print('-----------------------------Testing Error Metrics---------------------------------')\n","print(f\"NEE: {test_normalized_estimation_error}\")\n","print(f\"NMSE: {test_nmse_loss}\")\n","print(f\"Correlation: {test_correlation}\")\n","print(f\"R2_Score: {test_R2_score}\")\n","print(f\"Bias: {use_bias}\")\n","\n","# Plotting both arrays\n","plt.figure(figsize=(10, 5))  # Set the figure size\n","plt.plot(Y_test_predicted, label='Predicted', color='blue', marker='o')  # Plot Y_test_predicted\n","plt.plot(Y_test, label='Actual', color='red', linestyle='--', marker='o')  # Plot Y_test with dashed line\n","plt.title(f\"Comparison of Predicted and Actual Values\")  # Title of the plot\n","plt.xlabel('Index')  # Label for the x-axis\n","plt.ylabel('Values')  # Label for the y-axis\n","plt.legend()  # Add a legend\n","plt.grid(True)  # Add gridlines for better readability\n","plt.show()  # Display the plot\n","\n","#Get current time and store in variable\n","formatted_time = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n","pkl_file = f\"../Experimental Results/\"\n","\n","for param in hypers:\n","   pkl_file += f\"{param}={hypers[param]}.\"\n","\n","pkl_file += \".pkl\"\n","\n","with open(pkl_file, \"wb\") as file:\n","   dill.dump((W_estimated, b_estimated, losses, gap_to_optimality, fixed_point_stochastic_gradients, fixed_point_stochastic_gradient_norms, fixed_point_full_gradients, fixed_point_full_gradient_norms, iterates, iterate_norms, full_gradients, \\\n","    full_gradient_norms, stochastic_gradients, stochastic_gradient_norms, nee_values, nmse_values, corr_values, R2_values, epochs_ran), file)"]},{"cell_type":"markdown","metadata":{},"source":["## Grid Search for Learning Rate"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["lrs = np.arange(1e-5, 1e-2 + 1e-5, 1e-5)\n","\n","for lr in lrs:\n","    #Cost Function to Use(0 for Least Squares, 1 for Ridge Regression)\n","    cost_function_code = 1\n","\n","    #Hyperparameters\n","    epochs = 1000\n","    batch_size = 256 #Set 256 as default\n","    use_bias = False\n","    optimizer_code = 2\n","    decay = 0.99 #Set this as default learning rate\n","    hypers = {'lambda': lamb, 'lr': lr, 'epochs': epochs, 'batch_size': batch_size, 'bias': use_bias, 'optimizer_code': optimizer_code, 'decay_factor': decay}\n","\n","    #Initial Weights\n","    W_init = None\n","\n","    #Run SGD\n","    print(f\"------------ Learning Rate = {lr} ------------\")\n","    W_estimated, b_estimated, losses, gap_to_optimality, fixed_point_stochastic_gradients, fixed_point_stochastic_gradient_norms, fixed_point_full_gradients, fixed_point_full_gradient_norms, iterates, iterate_norms, full_gradients, \\\n","        full_gradient_norms, stochastic_gradients, stochastic_gradient_norms, nee_values, nmse_values, corr_values, R2_values, epochs_ran = SGD(X = X_train, Y = Y_train, cost_function_code = cost_function_code, hypers = hypers, optimizer_code = optimizer_code, p_star = p_star, W_true = W_true, W_init = W_init)\n","        \n","    #Get current time and store in variable\n","    formatted_time = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n","    pkl_file = f\"../Experimental Results/\"\n","\n","    for param in hypers:\n","        pkl_file += f\"{param}={hypers[param]}.\"\n","\n","    pkl_file += \".pkl\"\n","\n","    with open(pkl_file, \"wb\") as file:\n","        dill.dump((W_estimated, b_estimated, losses, gap_to_optimality, fixed_point_stochastic_gradients, fixed_point_stochastic_gradient_norms, fixed_point_full_gradients, fixed_point_full_gradient_norms, iterates, iterate_norms, full_gradients, \\\n","        full_gradient_norms, stochastic_gradients, stochastic_gradient_norms, nee_values, nmse_values, corr_values, R2_values, epochs_ran), file)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}
